<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Iptables的限速模块]]></title>
    <url>%2FLinux%E5%B7%A5%E5%85%B7%2FIptables%E7%9A%84%E9%99%90%E9%80%9F%E6%A8%A1%E5%9D%97.html</url>
    <content type="text"><![CDATA[limit和hashlimit是’iptables’的扩展模块，初始是用来限制日志次数的，不过现在基本用来限制包的（传入／传出）速率。 Limit这个模块必须用-m limit或--match limit明确指定。它用于限制匹配的速率，例如用于抑制日志消息。它只会匹配每秒给定的次数（默认情况下每小时3次，突发5次）。它有两个可选的参数： --limit 后面跟一个数字，指定每秒允许的最大平均匹配数。该数字可以使用/second、/minute、/hour或/day 或其中的一部分（所以5/second与5/s相同）明确指定单位。 --limit-burst 后面跟一个数字，表示在上述限制开始之前的最大突发。 此匹配通常可以与日志目标一起使用，以执行速率限制日志记录。 1# iptables -A FORWARD -m limit -j LOG 在第一次达到此规则时，数据包将被记录；实际上，由于默认突发是5，前五个数据包将被记录。之后，20分钟（每小时3次=20分钟）之内，无论多少包到达都不在纪录日志。也就是说，每20分钟会重新获取一次“权限”（默认burst为5）。100分钟没有命中此规则，则回到初始状态。 Note：目前不可创建充能时间大于59个小时（类似前面的100分钟）的规则；如果设置limit为1/day，则突发率必须小于3。 还可以使用此模块以更快的速度避免各种拒绝服务攻击（DoS），从而提高响应速度。 Syn-flood protection 1# iptables -A FORWARD -p tcp --syn -m limit --limit 1/s -j ACCEPT Furtive port scanner 1# iptables -A FORWARD -p tcp --tcp-flags SYN,ACK,FIN,RST RST -m limit --limit 1/s -j ACCEPT Ping of death 1# iptables -A FORWARD -p icmp --icmp-type echo-request -m limit --limit 1/s -j ACCEPT limit仅是iptables的匹配模块，如果要实现限速，还需要结合iptables的其他命令。例如： 12# iptables -A OUTPUT -p icmp -m limit --limit 5/s --limit-burst -j ACCEPT# iptables -A OUTPUT -p icmp -j DROP 利用ping baidu.com -i 0.05查看是否生效。 Hashlimithashlimit使用散列桶来表示使用单个iptables规则的一组连接的速率限制匹配。可以按每个主机组（源和/或目标地址）和/或每个端口完成分组。它使您能够表达“每个时间段每组的N个数据包”或“每秒N个字节”。 hashlimit 选项（–hashlimit-upto，–hashlimit-above）和–hashlimit-name是必需的。 hashlimit的几个参数如下： –hashlimit-upto amount[/second|/minute|/hour|/day] 如果速率低于或等于此值，则匹配。它被指定为一个数字，带有可选的时间后缀（默认值是3/小时），或者指定为每秒数据量（每秒字节数）。 –hashlimit-above amount[/second|/minute|/hour|/day] 如果速率高于此值，则匹配。 –hashlimit-burst amount 允许突发的个数(其实就是令牌桶最大容量)。默认为 5。 *–hashlimit-mode {srcip|srcport|dstip|dstport},…* 一个用逗号分隔的对象列表。如果没有给出–hashlimit-mode选项，’hashlimit’ 的行为就像 ‘limit’ 一样，但是在做哈希管理的代价很高。 –hashlimit-srcmask prefix 当mode设置为srcip时, 配置相应的掩码表示一个网段。 –hashlimit-dstmask prefix 当mode设置为dstip时, 配置相应的掩码表示一个网段。 –hashlimit-name foo 定义这条hashlimit规则的名称, 所有的条目(entry)都存放在 /proc/net/ipt_hashlimit/{foo} 里。 –hashlimit-htable-size buckets 散列表的桶数（buckets）。 –hashlimit-htable-max entries 散列中的最大条目。 –hashlimit-htable-expire msec hash规则失效时间, 单位毫秒(milliseconds)。 –hashlimit-htable-gcinterval msec 垃圾回收器回收的间隔时间, 单位毫秒。 使用hashlimit 来进行限速例子(icmp)： hashlimit-upto12# iptables -A OUTPUT -p icmp -m hashlimit --hashlimit-name icmp --hashlimit-upto 5/sec --hashlimit-burst 10 -j ACCEPT# iptables -A OUTPUT -p icmp -j DROP 结果如下 hashlimit-above12# iptables -A OUTPUT -p icmp -m hashlimit --hashlimit-name icmp --hashlimit-above 5/sec --hashlimit-burst 10 -j ACCEPT# iptables -A OUTPUT -p icmp -j DROP 结果如下 above时，需达到速率才可匹配。 hashlimit-mode12# iptables -A OUTPUT -p icmp -m hashlimit --hashlimit-name icmp --hashlimit-above 5/sec --hashlimit-burst 10 --hashlimit-mode srcip --hashlimit-srcmask 16 -j ACCEPT# iptables -A OUTPUT -p icmp -j DROP 运行命令 ‘ping baidu.com -i 0.1’ 之后查看文件 ‘/proc/net/ipt_hashlimit/icmp’ 如下 说一下ipt_hashlimit文件中每列的含义 序号（列） 含义 1 显示如果没有匹配的规则数据包，垃圾收集将删除哈希限制条目的时间（以秒为单位）。 2 基于您使用–hashlimit-mode指定的模式，在这种情况下，它是srcip。它在这里显示srcip。 3 当前的“信用”（每jiffy重新递增1）。 4 信用上限（“–hashlimit-burst”的成本*设置）。 5 成本（即每次规则匹配时减少多少信用）。 如果“信用”达到0，那么哈希条目已超过限制。 其他选项12# iptables -A OUTPUT -p icmp -m hashlimit --hashlimit-name icmp --hashlimit-upto 5/sec --hashlimit-burst 10 --hashlimit-mode dstip --hashlimit-dstmask 16 --hashlimit-htable-size 1 --hashlimit-htable-max 1 --hashlimit-htable-expire 12000 -j ACCEPT# iptables -A OUTPUT -p icmp -j DROP 配置mode为 dstip，hash bucket num为 1 ，最大条目为 1，失效时间为 12 秒（12000 ms），文件如下图（环境有杂包） 运行命令 ‘ping 8.8.8.8 -i 0.02’ ，由于刚开始此表已被占用，必须等到此表老化之后才能连通。 优秀资料packet-filtering-HOWTO-7 iptables-extensions iptables的hashlimit模块 Per-IP rate limiting with iptables What do the fields in /proc/net/ipt_hashlimit/FILE mean?]]></content>
      <categories>
        <category>Linux工具</category>
      </categories>
      <tags>
        <tag>limit</tag>
        <tag>hashlimit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断Linux是否运行在物理机]]></title>
    <url>%2FLinux%E5%B7%A5%E5%85%B7%2F%E5%88%A4%E6%96%ADLinux%E6%98%AF%E5%90%A6%E8%BF%90%E8%A1%8C%E5%9C%A8%E7%89%A9%E7%90%86%E6%9C%BA.html</url>
    <content type="text"><![CDATA[dmidecode命令dmidecode命令用来查看硬件信息。 1dmidecode is a tool for dumping a computer DMI (some say SMBIOS) table contents in a human-readable format. So if your machine is a vm, you should not get any output. DMI ，即Desktop Management Interface。也有被称为SMBIOS，即System Management BIOS。 较低版本的dmidecode命令不支持参数，因此要看信息的话，要用more/less/grep来配合才能更好些。 查看服务器型号：dmidecode | grep ‘Product Name’ 查看主板的序列号：dmidecode |grep ‘Serial Number’ 查看系统序列号：dmidecode -s system-serial-number 查看内存信息：dmidecode -t memory 查看OEM信息：dmidecode -t 11 显示生产厂商物理机12# dmidecode -s system-manufacturerIBM 虚拟机12# dmidecode -s system-manufacturerOpenStack Foundation 显示产品名物理机12# dmidecode -s system-product-nameSystem x3650 M4 -[7915IA5]- 虚拟机12# dmidecode -s system-product-nameOpenStack Nova /proc/scsi/scsi文件12Try check on /proc/scsi/scsi, if it is a vm, you would not get any attached device: Attached devices: 物理机12345# cat /proc/scsi/scsiAttached devices:Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: IBM Model: ServeRAID M5110e Rev: 3.15 Type: Direct-Access ANSI SCSI revision: 05 虚拟机12# cat /proc/scsi/scsiAttached devices: ethtool命令物理机1234567891011# ethtool -i eth0driver: igbversion: 3.2.10firmware-version: 1.5-2expansion-rom-version: bus-info: 0000:06:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: yessupports-register-dump: yessupports-priv-flags: no 虚拟机1234567891011# ethtool -i eth0driver: virtio_netversion: 1.0.0firmware-version: expansion-rom-version: bus-info: 0000:00:03.0supports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no 此处简单介绍一下lspci命令。 lspcilspci 是一个用来显示系统中所有PCI总线设备或连接到该总线上的所有设备的工具。 lspci 不加任何选项 123456789Host bridge: &lt;==主板芯片VGA compatible controller &lt;==显卡Audio device &lt;==音频设备PCI bridge &lt;==接口插槽USB Controller &lt;==USB控制器ISA bridge IDE interface SMBus Ethernet controller &lt;==网卡 lspci -tv 列出所有的pci设备 lspci | grep -i &#39;eth&#39; 查看网卡型号 lscpi 和 eth*的对应关系 /sys/devices设备信息 123456789# pwd/sys/devices# find . -name '*eth*'./pci0000:00/0000:00:1c.0/0000:06:00.0/net:eth0./pci0000:00/0000:00:1c.0/0000:06:00.1/net:eth1./pci0000:00/0000:00:1c.0/0000:06:00.2/net:eth2./pci0000:00/0000:00:1c.0/0000:06:00.3/net:eth3# lspci -s 06:00.006:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) ethtool 方式 12345678910111213# ethtool -i eth1 driver: igbversion: 3.2.10firmware-version: 1.5-2expansion-rom-version: bus-info: 0000:06:00.1supports-statistics: yessupports-test: yessupports-eeprom-access: yessupports-register-dump: yessupports-priv-flags: no# lspci -s 06:00.1 // bus-info06:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) 优秀资料怎么判断你的linux系统是不是运行在虚拟机器上面 linux下eth*与lspci中对应关系]]></content>
      <categories>
        <category>Linux工具</category>
      </categories>
      <tags>
        <tag>dmidecode</tag>
        <tag>lspci</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程监控之daemontools]]></title>
    <url>%2FLinux%E5%B7%A5%E5%85%B7%2F%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7%E4%B9%8Bdaemontools.html</url>
    <content type="text"><![CDATA[简介Daemontools是svscanboot，svscan，supervise，svc，svok，svstat等一系列工具的合集。其中，supervise是其中的核心工具。 对于使用Daemontools的优点可以参考Service creation。 安装参考官网教程 及 Linux命令帮助。 1234567# mkdir -p /package# chmod 1755 /package //设置粘贴位# cd /package# wget https://cr.yp.to/daemontools/daemontools-0.76.tar.gz# tar zxvf daemontools-0.76.tar.gz# cd admin/daemontools-0.76/# package/install 若上一步出错，错误信息如下： 1234567Linking ./src/* into ./compile...Compiling everything in ./compile..../load envdir unix.a byte.a /usr/bin/ld: errno: TLS definition in /lib64/libc.so.6 section .tbss mismatches non-TLS reference in envdir.o/lib64/libc.so.6: error adding symbols: Bad valuecollect2: error: ld returned 1 exit statusmake: *** [envdir] Error 1 则编辑src/conf-cc，加gcc加上-include /usr/include/errno.h 使用标准错误即可。 验证安装： 1# cat /etc/inittab //查看是否有svscanboot，需重启以启动svscan svscanbootsvscanboot是svscan的开机启动程序。 有几种不同的方式来运行svscanboot命令，启动svscan。unix / linux发行版继续使用它们的启动系统，打破了与现有启动脚本的兼容性。 /etc/rc.local将下行加入文件/etc/rc.local。 1csh -cf '/command/svscanboot &amp;' 这是传统的方法。 /etc/inittab将下行加入文件/etc/inittab。 1SV:12345:respawn:/command/svscanboot 这是传统的System V方法，并且几乎在Linux系统中得到普遍支持多年，但被Ubuntu 6.10所破坏。 /etc/event.d version 1将以下几行加入文件/etc/event.d/svscan。 123456start on runlevel-1start on runlevel-2start on runlevel-3start on runlevel-4start on runlevel-5respawn /command/svscanboot Ubuntu 6.10以下版本可用。 /etc/event.d version 2将以下几行加入文件/etc/event.d/svscan。 1234567start on runlevel 1start on runlevel 2start on runlevel 3start on runlevel 4start on runlevel 5respawnexec /command/svscanboot Ubuntu 7.04 到 9.04 可用。 /etc/init将以下几行加入文件/etc/init/svscan.conf。 1234start on runlevel [12345]stop on runlevel [^12345]respawnexec /command/svscanboot Ubuntu 9.10 到 (至少) 11.10 之间可用。 svscansvscan启动并监视一组服务。 svscan为当前目录的每个子目录启动一个监控进程，最多可达1000个子目录。svscan跳过以点开始的子目录名称。监控必须在svscan的path上。 每个子目录下都会有一个名为run的用来启动对应服务的脚本程序。supervise会监控该服务，在服务消亡时使用run脚本来自动启动该服务。 svscan可以选择启动一对监控进程，一个用于子目录s，一个用于s/log，并在它们之间建立一个管道。如果s最长为255个字节并且s/log存在，它会执行此操作。（在版本0.70和更低版本中，如果粘滞位（sticky bit）被置位，它会执行此操作。）svscan的每个管道需要两个空闲的文件描述符。 每隔5秒钟，svscan再次检查子目录。如果它看到一个新的子目录，它会启动一个新的监控过程。如果它看到监控进程退出的旧子目录，则重新启动监控进程。在日志情况下，它重新使用相同的管道，以避免数据丢失。 svscan被设计为永远运行。如果在创建管道或运行监督时遇到困难，则会向stderr发送消息;它会在五秒钟后再试一次。 如果svscan被赋予一个命令行参数，它将在启动时切换到该目录。 测试 目录创建 1234# cd /root# mkdir services;# cd services;# mkdir test1 test2; 程序创建 1# cd test1; 创建test1.c文件，代码如下： 1234567891011#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main()&#123; while(1) &#123; printf("in process test 1\n"); sleep(1); &#125; return 0;&#125; 编译 1# gcc -o test1 test1.c 创建可执行文件run（chmod +x run） 123#!/bin/shecho "start test 1 !"exec ./test1 test2中文件类似test1中创建。 svscan运行。 1# svscan /root/services supervisesupervise是一种进程监控工具，其功能是当监控的指定进程消亡时，重启进程。 启用supervise监控服务比较容易，只需要添加一个被监控的服务的目录，在该目录中添加启动服务器的名字为run的脚本文件即可。supervise调用这个脚本，并监控管理该脚本中运行的程序。 1#supervise s supervisor是所有监控项目的父进程。 监控切换到名为s的目录并启动./run。如果./run退出，它会重新启动./run。它在启动./run后暂停一秒，以便在./run立即退出时不会太快地循环。 如果文件s/down存在，则监督不会立即开始./run。你可以使用svc来启动./run并提供其他命令来监控。 监控维护目录s/supervise中的二进制格式的状态信息，该目录必须是可写的，以便监控。状态信息可以通过svstat读取。 如果启动后监控无法找到它需要的文件，或者监控的另一个副本已经在s中运行，它可能立即退出。一旦监控成功运行，它不会退出，除非它被杀死或被特别要求退出。 可以使用svok来检查监督是否成功运行。也可以使用svscan来可靠地启动一系列监控进程。 工具原理supervise启动的时候fork一个子进程，子进程执行execvp系统调用，将自己替换成执行的模块。 模块变成supervise的子进程运行，而supervise则死循环运行，并通过waitpid或者wait3系统调用选择非阻塞的方式去侦听子进程的运行情况。同时也会读取pipe文件svcontrol的命令，然后根据命令去执行不同的动作。 如果子进程因某种原因导致退出，则supervise通过waitpid或者wait3获知，并继续启动模块，如果模块异常导致无法启动，则会使supervise陷入死循环，不断的启动模块。 测试（直接使用svscan测试程序）可直接使用svscan测试程序。 1# supervise test1 test1为目录。一般服务器运行的程序会将其挂起，如下 1# nohup supervise test/ &gt; /dev/null 2&gt;&amp;1 &amp; //nohup,no hang up 使用的文件supervise会在服务目录下创建名为supervise的目录，同时supervise目录下会有4个文件，分别为control lock ok status，对应的功能和我们可以从中获取的信息如下 control 可以理解为一个控制接口，supervise读取这个管道的信息，进而根据管道的信息去控制子进程，而通过控制子进程的方式实际上就是给子进程发信号。 可以利用的信息：直接写命令到该文件中，如echo ‘d’ &gt; svcontorl，让supervise去控制子进程，比较常用的命令如下 d: 停掉子进程，并且不启动。 u: 相对于d，启动子进程。 k: 发送一个kill信号，因kill之后supervise马上又重启，因此可以认为是一个重启的动作。 x: 标志supervise退出。 lock supervise的文件锁，通过该文件锁去控制并发，防止同一个status目录下启动多个进程，造成混乱。 可以通过/sbin/fuser这个命令去获取lock文件的使用信息，如果fuser返回值为0，表示supervise已经启动,同时fuser返回了supervise的进程pid。 ok 通过此文件判断supervise进程状态。 status supervise用来记录一些信息之用，可以简单的理解为 char status[20]，其中status[16]记录了supervise所启动的子进程的pid，status[17]-status[19] 是子进程pid别右移8位，而status[0]-status[11] 没有用，status[12]-status[14] 是标志位，一般没啥用,status[15] 直接为0。 可以直接通过od命令去读取该文件,一般有用的就是od -An -j16 -N2 -tu2 status可以直接拿到 supervise所负责的子进程的pid。 如果用 od -d status 或 od -t u2 status 读取 2 字节，无符号短整型，最大值是 65535。如果 cat /proc/sys/kernel/pid_max 大于这个数。od -d status 读出来的 pid 就有可能是错的。每次右移 8 位，是想把 pid 保存成 4 字节。即从 16 - 19 共 4 个字节。正确的读取应该是 od -t u4 status，按 4 字节无符号整形读取就不会有问题了。 svcsvc控制由supervise监控的服务。 1svc opts services opts是一系列getopt样式的选项。sercies由任意数量的参数组成，每个参数命名一个由supervise使用的目录。 svc依次将所有选项应用于每个服务。这里是选项： -u: Up。如果服务未运行，则启动。如果服务已停止，则重启。 -d: Down。服务已运行，向服务发送TERM和CONT信号。停止之后，不在重启。 -o: Once。服务未运行，启动。停止之后，不在重启。 -p: Pause。向服务发送STOP信号。 -c: Continue。向服务发送CONT信号。 -h: Hangup。向服务发送HUP信号。 -a: Alarm。向服务发送ALRM信号。 -i: Interrupt。向服务发送INT信号。 -t: Terminate。向服务发送TERM信号。 -k: Kill。向服务发送KILL信号。 -x: Exit。一旦服务关闭，supervise将立即退出。如果在一个稳定的系统上使用这个选项，监控可能永远运行。 测试接svscan测试。 -d 1# svc -d /root/services/test1 test1停止，仅test2打印程序。supervise test1未停止。 -u 1# svc -u /root/services/test1 test1重启。 -k 1# svc -k /root/services/test1 test1重启。test1被kill之后，supervise会将其重启，可查看其进程号以确定。 -dx 1# svc -dx /root/services/test1 退出test1 及 supervise test1进程。svscan监听时会将supervise test1重新启动。 svoksvok检测supervise是否在运行。 1# svok service svok检查supervise是否在名为service的目录中成功运行。如果supervise成功运行，它将以0为返回值退出。如果supervise没有成功运行，它将以100为返回值退出。 svstatsvstat打印由supervise监控的服务状态。 1# svstat services services由任意数量的参数（目录）组成。svstat为每个目录打印一条可视化的行，说明supervise是否在该目录中成功运行，并报告由supervise维护的状态信息。 测试12345# svstat services/test1services/test1: up (pid 12587) 129 seconds# svc -d services/test1 # svstat services/test1services/test1: down 8 seconds, normally up fghackfghack是一个anti-backgrounding的工具。 1fghack child fghack运行有许多额外描述符写入管道的子进程。fghack读取并丢弃任何写入管道的数据。在子进程退出且管道关闭后，fghack退出。 pgrphackpgrphack会在一个单独的进程组中运行一个程序。 1pgrphack child 优秀资料supervise进程管理利器 进程的守护神 - daemontools 用Daemontools监控Linux服务]]></content>
      <categories>
        <category>Linux工具</category>
      </categories>
      <tags>
        <tag>supervise</tag>
        <tag>svscan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程管理之supervisor]]></title>
    <url>%2FLinux%E5%B7%A5%E5%85%B7%2F%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E4%B9%8Bsupervisor.html</url>
    <content type="text"><![CDATA[Supervisor (http://supervisord.org) 是一个用 Python 写的进程管理工具，可以很方便的用来启动、重启、关闭进程（不仅仅是 Python 进程）。除了对单个进程的控制，还可以同时启动、关闭多个进程，比如很不幸的服务器出问题导致所有应用程序都被杀死，此时可以用 supervisor 同时启动所有应用程序而不是一个一个地敲命令启动。 安装Supervisor 可以运行在 Linux、Mac OS X 上。如前所述，supervisor 是 Python 编写的，所以安装起来也很方便，可以直接用 pip : 1sudo pip install supervisor 也可以使用 yum或apt-get进行安装。 supervisord 配置Supervisor 相当强大，提供了很丰富的功能，不过我们可能只需要用到其中一小部分。安装完成之后，可以编写配置文件，来满足自己的需求。为了方便，我们把配置分成两部分：supervisord（supervisor 是一个 C/S 模型的程序，这是 server 端，对应的有 client 端：supervisorctl）和应用程序（即我们要管理的程序）。 首先来看 supervisord 的配置文件。安装完 supervisor 之后，可以运行echo_supervisord_conf 命令输出默认的配置项，也可以重定向到一个配置文件里： 1echo_supervisord_conf &gt; /etc/supervisord.conf 去除里面大部分注释和“不相关”的部分，我们可以先看这些配置： 123456789101112131415161718192021222324252627282930313233[unix_http_server]file=/tmp/supervisor.sock ; UNIX socket 文件，supervisorctl 会使用;chmod=0700 ; socket 文件的 mode，默认是 0700;chown=nobody:nogroup ; socket 文件的 owner，格式： uid:gid;[inet_http_server] ; HTTP 服务器，提供 web 管理界面;port=127.0.0.1:9001 ; Web 管理后台运行的 IP 和端口，如果开放到公网，需要注意安全性;username=user ; 登录管理后台的用户名;password=123 ; 登录管理后台的密码[supervisord]logfile=/tmp/supervisord.log ; 日志文件，默认是 $CWD/supervisord.loglogfile_maxbytes=50MB ; 日志文件大小，超出会 rotate，默认 50MBlogfile_backups=10 ; 日志文件保留备份数量默认 10loglevel=info ; 日志级别，默认 info，其它: debug,warn,tracepidfile=/tmp/supervisord.pid ; pid 文件nodaemon=false ; 是否在前台启动，默认是 false，即以 daemon 的方式启动minfds=1024 ; 可以打开的文件描述符的最小值，默认 1024minprocs=200 ; 可以打开的进程数的最小值，默认 200; the below section must remain in the config file for RPC; (supervisorctl/web interface) to work, additional interfaces may be; added by defining them in separate rpcinterface: sections[rpcinterface:supervisor]supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface[supervisorctl]serverurl=unix:///tmp/supervisor.sock ; 通过 UNIX socket 连接 supervisord，路径与 unix_http_server 部分的 file 一致;serverurl=http://127.0.0.1:9001 ; 通过 HTTP 的方式连接 supervisord; 包含其他的配置文件[include]files = relative/directory/*.ini ; 可以是 *.conf 或 *.ini 我们把上面这部分配置保存到 /etc/supervisord.conf（或其他任意有权限访问的文件），然后启动 supervisord（通过 -c 选项指定配置文件路径）： 1supervisord -c /etc/supervisord.conf 查看 supervisord 是否在运行： 1ps aux | grep supervisord program 配置上面我们已经把 supervisrod 运行起来了，现在可以添加我们要管理的进程的配置文件。这些配置可以都写到 supervisord.conf 文件里，如果应用程序很多，最好通过 include 的方式把不同的程序（组）写到不同的配置文件里。 为了举例，我们新建一个目录 /etc/supervisor/ 用于存放这些配置文件，相应的，把 /etc/supervisord.conf 里 include 部分的的配置修改一下： 12[include]files = /etc/supervisor/*.conf 假设有个用 Flask 开发的用户系统 usercenter, 生产环境使用 gunicorn 运行。项目代码位于 /home/leon/projects/usercenter，WSGI 对象位于 wsgi.py。在命令行启动的方式是这样的： 12cd /home/leon/projects/usercentergunicorn -w 8 -b 0.0.0.0:17510 wsgi:app 对应的配置文件可能是： 12345678910111213[program:usercenter]directory = /home/leon/projects/usercenter ; 程序的启动目录command = gunicorn -w 8 -b 0.0.0.0:17510 wsgi:app ; 启动命令autostart = true ; 在 supervisord 启动的时候也自动启动startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了autorestart = true ; 程序异常退出后自动重启startretries = 3 ; 启动失败自动重试次数，默认是 3user = leon ; 用哪个用户启动redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 falsestdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MBstdout_logfile_backups = 20 ; stdout 日志文件备份数; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）stdout_logfile = /data/logs/usercenter_stdout.log 其中 [program:usercenter] 中的 usercenter 是应用程序的唯一标识，不能重复。对该程序的所有操作（start, restart 等）都通过名字来实现。 Tips 1: Python 环境有两种方式指定程序使用的 Python 环境： command 使用绝对路径。假设使用 pyenv 来管理 Python 环境，上面例子中的 gunicorn 路径可以替换为 /home/leon/.pyenv/versions/usercenter/bin/gunicorn. 这种方式一目了然，推荐。 通过 environment 配置 PYTHONPATH. environment=PYTHONPATH=$PYTHONPATH:/home/leon/.pyenv/versions/usercenter/bin/. environment 这个配置项非常有用，可以用来给程序传入环境变量。 Tips 2: 后台进程Supervisor 只能管理在前台运行的程序，所以如果应用程序有后台运行的选项，需要关闭。 Tips 3: 子进程有时候用 Supervisor 托管的程序还会有子进程（如 Tornado），如果只杀死主进程，子进程就可能变成孤儿进程。通过这两项配置来确保所有子进程都能正确停止： 12stopasgroup=truekillasgroup=true 使用 supervisorctlSupervisorctl 是 supervisord 的一个命令行客户端工具，启动时需要指定与 supervisord 使用同一份配置文件，否则与 supervisord 一样按照顺序查找配置文件。 1supervisorctl -c /etc/supervisord.conf 上面这个命令会进入 supervisorctl 的 shell 界面，然后可以执行不同的命令了： 123456&gt; status # 查看程序状态&gt; stop usercenter # 关闭 usercenter 程序&gt; start usercenter # 启动 usercenter 程序&gt; restart usercenter # 重启 usercenter 程序&gt; reread ＃ 读取有更新（增加）的配置文件，不会启动新添加的程序&gt; update ＃ 重启配置文件修改过的程序 上面这些命令都有相应的输出，除了进入 supervisorctl 的 shell 界面，也可以直接在 bash 终端运行： 123456$ supervisorctl status$ supervisorctl stop usercenter$ supervisorctl start usercenter$ supervisorctl restart usercenter$ supervisorctl reread$ supervisorctl update 其它除了 supervisorctl 之外，还可以配置 supervisrod 启动 web 管理界面，这个 web 后台使用 Basic Auth 的方式进行身份认证。 除了单个进程的控制，还可以配置 group，进行分组管理。 经常查看日志文件，包括 supervisord 的日志和各个 pragram 的日志文件，程序 crash 或抛出异常的信息一半会输出到 stderr，可以查看相应的日志文件来查找问题。 Supervisor 有很丰富的功能，还有其他很多项配置，可以在官方文档获取更多信息。 优秀资料使用 supervisor 管理进程]]></content>
      <categories>
        <category>Linux工具</category>
      </categories>
      <tags>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协议报文解析]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2F%E5%8D%8F%E8%AE%AE%E6%8A%A5%E6%96%87%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[以太网帧格式以太网帧因为历史原因存在多个版本，这里最常用的IEEE802.3以太网帧格式。 参数 含义 Preamble 前导码，7个字节，用于数据传输过程中的双方发送、接收的速率的同步 SFD 帧开始符，1个字节，表明下一个字节开始是真实数据（目的MAC地址） dst MAC 目的MAC地址，6个字节，指明帧的接受者 src MAC 源MAC地址，6个字节，指明帧的发送者 Length 长度，2个字节，指明该帧数据字段的长度，但不代表数据字段长度能够达到（2^16）字节 Type 类型，2个字节，指明帧中数据的协议类型 Data and Pad 数据与填充，46~1500个字节，包含了上层协议传递下来的数据，如果加入数据字段后帧长度不够64字节，会在数据字段加入“填充”至达到64字节 FCS 帧校验序列，4个字节，对接收网卡（主要是检测Data and Pad字段）提供判断是否传输错误的一种方法，如果发现错误，丢弃此帧。目前最为流行的用于FCS的算法是循环冗余校验（cyclic redundancy check –CRC） ARP报文格式ARP协议号为0x0806。 参数 含义 硬件类型 硬件地址的类型，值为1表示以太网地址 协议类型 要映射的协议地址类型。它的值为0x0800表示IP地址类型 硬件地址长度 以字节为单位，对于以太网上的IP地址的ARP请求或应答来说，值为6 协议地址长度 以字节为单位，对于以太网上的IP地址的ARP请求或应答来说，值为4 操作类型(OP) 1表示ARP请求，2表示ARP应答 发送者硬件地址 发送端MAC地址 发送者IP地址 发送端IP地址 目标硬件地址 目标MAC地址 目标IP地址 目标IP地址 ARP状态机如下 IPV4报文格式IP数据包由首部和数据两部分组成。首部的前一部分是固定长度，共 20 字节,是所有IP数据报必须具有的。在首部的固定部分的后面是一些可选字段，其长度是可变的。 参数 含义 版本 占4位，IP协议的版本，目前的IP协议版本号为4，下一代IP协议版本号为6。 首部长度 占 4 位，IP报头的长度。固定部分的长度（20字节）和可变部分的长度之和。共占4位。最大为1111，即10进制的15，代表IP报头的最大长度可以为15个32bits（4字节），也就是最长可为15*4=60字节，除去固定部分的长度20字节，可变部分的长度最大为40字节。 服务类型 Type Of Service。 占 8 位,用来获得更好的服务. 总长度 总长度指首都及数据之和的长度，单位为字节。因为总长度字段为 16位，所以数据报的最大长度为 216-1=65 535字节。在IP层下面的每一种数据链路层都有自己的帧格式，其中包括帧格式中的数据字段的最大长度，即最大传送单元 MTU (Maximum Transfer Unit)。当一个数据报封装成链路层的帧时，此数据报的总长度 (即首部加上数据部分)一定不能超过下面的数据链路层的MTU值，否则要分片。 标识 占 16位。IP软件在存储器中维持一个计数器，每产生一个数据报,计数器就加 1，并将此值赋给标识字段。但这个”标识”并不是序号，因为 IP是无连接的服务。数据报不存在按序接收的问题。当数据报由于长度超过网络的 MTU 而必须分片时，这个标识字段的值就被复制到所有的数据报的标识字段中。相同的标识字段的值使分片后的各数据报片最后能正确地重装成为原来的数据报. 标志 占3 位，但目前只有2位有意义。标志字段中的最低位记为 MF(More Fragment)。MF=1即表示后面”还有分片”的数据报。MF=0表示这已是若干数据报片中的最后一个。标志字段中间的一位记为DF(Don’t Fragment)，意思是”不能分片”，只有当 DF=0时才允许分片。 片偏移 占 13位。较长的分组在分片后，某片在原分组中的相对位置。也就是说，相对用户数据字段的起点，该片从何处开始。片偏移以 8个字节为偏移单位，这就是说，每个分片的长度一定是 8字节(64位)的整数倍。 生存时间 占 8位，生存时间字段常用的英文缩写是TTL(Time To Live)，其表明数据报在网络中的寿命。由发出数据报的源点设置这个字段。其目的是防止无法交付的数据报无限制地在因特网中兜圈子，因而白白消耗网络资源。最初的设计是以秒作为 TTL的单位。每经过一个路由器时，就把TTL减去数据报在路由器消耗掉的一段时间。若数据报在路由器消耗的时间小于 1 秒，就把TTL值减 1。当 TTL值为 0时，就丢弃这个数据报。 协议 占 8 位。协议字段指出此数据报携带的数据是使用何种协议，以便使目的主机的IP层知道应将数据部分上交给哪个处理过程。 首部校验和 占 16位。这个字段只检验数据报的首部,但不包括数据部分。这是因为数据报每经过一个路由器，都要重新计算一下首都检验和。 源地址 占32位。 目的地址 占32位。 Linux 内核相关结构体如下： 12345678910111213141516171819202122//ip 头部结构体struct iphdr &#123;#if defined(__LITTLE_ENDIAN_BITFIELD) __u8 ihl:4, version:4;#elif defined (__BIG_ENDIAN_BITFIELD) __u8 version:4, ihl:4;#else#error "Please fix &lt;asm/byteorder.h&gt;"#endif __u8 tos; __be16 tot_len; __be16 id; __be16 frag_off; __u8 ttl; __u8 protocol; __sum16 check; __be32 saddr; __be32 daddr; /*The options start here. */&#125;; IP协议号 十进制 十六进制 关键字 协议 引用 0 0x00 HOPOPT IPv6逐跳选项 RFC 2460 1 0x01 ICMP 互联网控制消息协议（ICMP） RFC 792 2 0x02 IGMP 因特网组管理协议（IGMP） RFC 1112 3 0x03 GGP 网关对网关协议 RFC 823 4 0x04 IPv4 IPv4 (封装) RFC 791 5 0x05 ST 因特网流协议 RFC 1190, RFC 1819 6 0x06 TCP 传输控制协议（TCP） RFC 793 7 0x07 CBT 有核树组播路由协议 RFC 2189 8 0x08 EGP 外部网关协议 RFC 888 9 0x09 IGP 内部网关协议（任意私有内部网关（用于思科的IGRP）） 10 0x0A BBN-RCC-MON BBN RCC 监视 11 0x0B NVP-II 网络语音协议 RFC 741 12 0x0C PUP Xerox PUP 13 0x0D ARGUS ARGUS 14 0x0E EMCON EMCON 15 0x0F XNET Cross Net Debugger IEN 158 16 0x10 CHAOS Chaos 17 0x11 UDP 用户数据报协议（UDP） RFC 768 18 0x12 MUX Multiplexing IEN 90 19 0x13 DCN-MEAS DCN Measurement Subsystems 20 0x14 HMP Host Monitoring Protocol RFC 869 21 0x15 PRM Packet Radio Measurement 22 0x16 XNS-IDP XEROX NS IDP 23 0x17 TRUNK-1 Trunk-1 24 0x18 TRUNK-2 Trunk-2 25 0x19 LEAF-1 Leaf-1 26 0x1A LEAF-2 Leaf-2 27 0x1B RDP Reliable Datagram Protocol RFC 908 28 0x1C IRTP Internet Reliable Transaction Protocol RFC 938 29 0x1D ISO-TP4 ISO Transport Protocol Class 4 RFC 905 30 0x1E NETBLT Bulk Data Transfer Protocol RFC 998 31 0x1F MFE-NSP MFE Network Services Protocol 32 0x20 MERIT-INP MERIT Internodal Protocol 33 0x21 DCCP Datagram Congestion Control Protocol RFC 4340 34 0x22 3PC Third Party Connect Protocol 35 0x23 IDPR Inter-Domain Policy Routing Protocol RFC 1479 36 0x24 XTP Xpress Transport Protocol 37 0x25 DDP Datagram Delivery Protocol 38 0x26 IDPR-CMTP IDPR Control Message Transport Protocol 39 0x27 TP++ TP++ Transport Protocol 40 0x28 IL IL Transport Protocol&amp;action=edit&amp;redlink=1) 41 0x29 IPv6 IPv6 Encapsulation RFC 2473 42 0x2A SDRP Source Demand Routing Protocol RFC 1940 43 0x2B IPv6-Route Routing Header for IPv6 RFC 2460 44 0x2C IPv6-Frag Fragment Header for IPv6 RFC 2460 45 0x2D IDRP Inter-Domain Routing Protocol 46 0x2E RSVP Resource Reservation Protocol RFC 2205 47 0x2F GRE 通用路由封装（GRE） RFC 2784, RFC 2890 48 0x30 MHRP Mobile Host Routing Protocol 49 0x31 BNA BNA 50 0x32 ESP 封装安全协议（ESP） RFC 4303 51 0x33 AH 认证头协议（AH） RFC 4302 52 0x34 I-NLSP Integrated Net Layer Security Protocol TUBA 53 0x35 SWIPE SwIPe&amp;action=edit&amp;redlink=1) IP with Encryption 54 0x36 NARP NBMA Address Resolution Protocol RFC 1735 55 0x37 MOBILE IP Mobility (Min Encap) RFC 2004 56 0x38 TLSP Transport Layer Security Protocol (using Kryptonet key management) 57 0x39 SKIP Simple Key-Management for Internet Protocol RFC 2356 58 0x3A IPv6-ICMP 互联网控制消息协议第六版（ICMPv6） RFC 4443, RFC 4884 59 0x3B IPv6-NoNxt No Next Header for IPv6 RFC 2460 60 0x3C IPv6-Opts Destination Options for IPv6 RFC 2460 61 0x3D Any host internal protocol 62 0x3E CFTP CFTP 63 0x3F Any local network 64 0x40 SAT-EXPAK SATNET and Backroom EXPAK 65 0x41 KRYPTOLAN Kryptolan 66 0x42 RVD MIT Remote Virtual Disk Protocol 67 0x43 IPPC Internet Pluribus Packet Core 68 0x44 Any distributed file system 69 0x45 SAT-MON SATNET Monitoring 70 0x46 VISA VISA Protocol 71 0x47 IPCV Internet Packet Core Utility 72 0x48 CPNX Computer Protocol Network Executive 73 0x49 CPHB Computer Protocol Heart Beat 74 0x4A WSN Wang Span Network 75 0x4B PVP Packet Video Protocol 76 0x4C BR-SAT-MON Backroom SATNET Monitoring 77 0x4D SUN-ND SUN ND PROTOCOL-Temporary 78 0x4E WB-MON WIDEBAND Monitoring 79 0x4F WB-EXPAK WIDEBAND EXPAK 80 0x50 ISO-IP International Organization for Standardization Internet Protocol 81 0x51 VMTP Versatile Message Transaction Protocol&amp;action=edit&amp;redlink=1) RFC 1045 82 0x52 SECURE-VMTP Secure Versatile Message Transaction Protocol RFC 1045 83 0x53 VINES VINES 84 0x54 TTP TTP 84 0x54 IPTM Internet Protocol Traffic Manager 85 0x55 NSFNET-IGP NSFNET-IGP 86 0x56 DGP Dissimilar Gateway Protocol 87 0x57 TCF TCF 88 0x58 EIGRP 增强型内部网关路由协议（EIGRP） 89 0x59 OSPF 开放式最短路径优先（OSPF） RFC 1583 90 0x5A Sprite-RPC Sprite RPC Protocol 91 0x5B LARP Locus Address Resolution Protocol 92 0x5C MTP Multicast Transport Protocol 93 0x5D AX.25 AX.25 94 0x5E IPIP IP-within-IP Encapsulation Protocol RFC 2003 95 0x5F MICP Mobile Internetworking Control Protocol 96 0x60 SCC-SP Semaphore Communications Sec. Pro 97 0x61 ETHERIP Ethernet-within-IP Encapsulation RFC 3378 98 0x62 ENCAP Encapsulation Header RFC 1241 99 0x63 Any private encryption scheme 100 0x64 GMTP GMTP 101 0x65 IFMP Ipsilon Flow Management Protocol 102 0x66 PNNI PNNI over IP 103 0x67 PIM Protocol Independent Multicast 104 0x68 ARIS IBM’s ARIS (Aggregate Route IP Switching) Protocol 105 0x69 SCPS SCPS (Space Communications Protocol Standards) SCPS-TP[1] 106 0x6A QNX QNX 107 0x6B A/N Active Networks 108 0x6C IPComp IP Payload Compression Protocol RFC 3173 109 0x6D SNP Sitara Networks Protocol 110 0x6E Compaq-Peer Compaq Peer Protocol 111 0x6F IPX-in-IP IPX in IP 112 0x70 VRRP Virtual Router Redundancy Protocol, Common Address Redundancy Protocol (not IANA assigned) VRRP:RFC 3768 113 0x71 PGM PGM Reliable Transport Protocol RFC 3208 114 0x72 Any 0-hop protocol 115 0x73 L2TP Layer Two Tunneling Protocol Version 3 RFC 3931 116 0x74 DDX D-II Data Exchange (DDX) 117 0x75 IATP Interactive Agent Transfer Protocol 118 0x76 STP Schedule Transfer Protocol 119 0x77 SRP SpectraLink Radio Protocol 120 0x78 UTI Universal Transport Interface Protocol 121 0x79 SMP Simple Message Protocol 122 0x7A SM Simple Multicast Protocol draft-perlman-simple-multicast-03 123 0x7B PTP Performance Transparency Protocol 124 0x7C IS-IS over IPv4 Intermediate System to Intermediate System (IS-IS) Protocol over IPv4 RFC 1142 and RFC 1195 125 0x7D FIRE Flexible Intra-AS Routing Environment 126 0x7E CRTP Combat Radio Transport Protocol 127 0x7F CRUDP Combat Radio User Datagram 128 0x80 SSCOPMCE Service-Specific Connection-Oriented Protocol in a Multilink and Connectionless Environment ITU-T Q.2111 (1999) 129 0x81 IPLT 130 0x82 SPS Secure Packet Shield 131 0x83 PIPE Private IP Encapsulation within IP Expired I-D draft-petri-mobileip-pipe-00.txt 132 0x84 SCTP Stream Control Transmission Protocol 133 0x85 FC Fibre Channel 134 0x86 RSVP-E2E-IGNORE Reservation Protocol (RSVP) End-to-End Ignore RFC 3175 135 0x87 Mobility Header Mobility Extension Header for IPv6 RFC 6275 136 0x88 UDPLite Lightweight User Datagram Protocol RFC 3828 137 0x89 MPLS-in-IP Multiprotocol Label Switching Encapsulated in IP RFC 4023 138 0x8A manet MANET Protocols RFC 5498 139 0x8B HIP Host Identity Protocol RFC 5201 140 0x8C Shim6 Site Multihoming by IPv6 Intermediation RFC 5533 141 0x8D WESP Wrapped Encapsulating Security Payload RFC 5840 142 0x8E ROHC Robust Header Compression RFC 5856 143-252 0x8F-0xFC UNASSIGNED 253-254 0xFD-0xFE Use for experimentation and testing RFC 3692 255 0xFF Reserved. IP选项Linux内核IP选项如下 常量 类型 含义 IPOPT_END 0 选型表结尾 IPOPT_NOOP 1 无操作 IPOPT_SEC 130 基础安全 IPOPT_LSRR 131 宽松源路径选站 IPOPT_TIMESTAMP 68 时间戳 IPOPT_CIPSO 134 IPOPT_RR 7 路由记录 IPOPT_SID 136 流标识符 IPOPT_SSRR 137 严格源路径选站 IPOPT_RA 148 IP选项内核结构体如下 123456789101112131415161718struct ip_options &#123; __be32 faddr; __be32 nexthop; unsigned char optlen; unsigned char srr; unsigned char rr; unsigned char ts; unsigned char is_strictroute:1, srr_is_hit:1, is_changed:1, rr_needaddr:1, ts_needtime:1, ts_needaddr:1; unsigned char router_alert; unsigned char cipso; unsigned char __pad2; unsigned char __data[0];&#125;; ICMP报文格式TCP报文格式 TCP报头中的源端口号和目的端口号同IP数据报中的源IP与目的IP唯一确定一条TCP连接。 参数 含义 源端口 源端口和IP地址的作用是标识报文的返回地址。 目的端口 端口指明接收方计算机上的应用程序接口。 序号 本报文段发送的数据组的第一个字节的序号。在TCP传送的流中，每一个字节一个序号。 确认号 ACK，指明下一个期待收到的字节序号，表明该序号之前的所有数据已经正确无误的收到。确认号只有当ACK标志为1时才有效。 数据偏移 4bits。由于首部可能含有可选项内容，因此TCP报头的长度是不确定的，报头不包含任何可选字段则长度为20字节，4位首部长度字段所能表示的最大值为1111，转化为10进制为15，15*32/8 = 60，故报头最大长度为60字节。首部长度也叫数据偏移，是因为首部长度实际上指示了数据区在报文段中的起始偏移值。 保留 为将来定义新的用途保留，现在一般置0。 控制位 URG ACK PSH RST SYN FIN，共6个，每一个标志位表示一个控制功能。 窗口 滑动窗口大小，用来告知发送端接受端的缓存大小，以此控制发送端发送数据的速率，从而达到流量控制。窗口大小时一个16bit字段，因而窗口大小最大为65535。 校验和 奇偶校验，此校验和是对整个的 TCP 报文段，包括 TCP 头部和 TCP 数据，以 16 位字进行计算所得。由发送端计算和存储，并由接收端进行验证。 紧急指针 只有当 URG 标志置 1 时紧急指针才有效。紧急指针是一个正的偏移量，和顺序号字段中的值相加表示紧急数据最后一个字节的序号。 TCP 的紧急方式是发送端向另一端发送紧急数据的一种方式。 选项和填充 最常见的可选字段是最长报文大小，又称为MSS（Maximum Segment Size），每个连接方通常都在通信的第一个报文段（为建立连接而设置SYN标志为1的那个段）中指明这个选项，它表示本端所能接受的最大报文段的长度。选项长度不一定是32位的整数倍，所以要加填充位，即在这个字段中加入额外的零，以保证TCP头是32的整数倍。 数据部分 TCP 报文段中的数据部分是可选的。在一个连接建立和一个连接终止时，双方交换的报文段仅有 TCP 首部。如果一方没有数据要发送，也使用没有任何数据的首部来确认收到的数据。在处理超时的许多情况中，也会发送不带任何数据的报文段。 12345678910111213141516171819202122232425262728293031323334struct tcphdr &#123; __be16 source; __be16 dest; __be32 seq; __be32 ack_seq;#if defined(__LITTLE_ENDIAN_BITFIELD) __u16 res1:4, doff:4, fin:1, syn:1, rst:1, psh:1, ack:1, urg:1, ece:1, cwr:1;#elif defined(__BIG_ENDIAN_BITFIELD) __u16 doff:4, res1:4, cwr:1, ece:1, urg:1, ack:1, psh:1, rst:1, syn:1, fin:1;#else#error "Adjust your &lt;asm/byteorder.h&gt; defines"#endif __be16 window; __sum16 check; __be16 urg_ptr;&#125;; UDP报文格式 123456struct udphdr &#123; __be16 source; __be16 dest; __be16 len; __sum16 check;&#125;; 优秀资料学习整理——以太帧、ip帧、udp/tcp帧、http报文结构 ARP 协议详解 ARP报文结构 IP数据包格式]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ospenvswitch 编译及升级]]></title>
    <url>%2FOpenvswitch%2FOpenvswitch-%E7%BC%96%E8%AF%91%E5%8F%8A%E5%8D%87%E7%BA%A7.html</url>
    <content type="text"><![CDATA[编译安装参照官网，也查了些资料，编译期间出现些问题，先将正确的流程及问题解决方法记录如下。 Kernel 版本为 3.10.0-693.11.1.el7.x86_64 。内核与ovs关系可参考 release。 有序列表（1、2），顺序执行；无序列表（·），多选一。 正式开始。 获取代码（2.9为例，其他版本可参考官网） wget 下载 123# wget http://openvswitch.org/releases/openvswitch-2.9.0.tar.gz# tar zxvf openvswitch-2.9.0.tar.gz# cd openvswitch-2.9.0 Git 下载 1234# git clone https://github.com/openvswitch/ovs.git# cd ovs// git tag 或 git branch -a 查看所有分支，切换为稳定分支# git checkout origin/branch-2.9 编译环境配置 GUN make C 编译器 GCC 4.6 or later. Clang 3.4 or later. MSVC 2013. （windows平台） libssl，openssl相关，连接的安全机密性，可选。 libcap-ng，可选。 Python 2.7. 用到 six version 1.4.0或之后的库。 除此之外，如果需要Linux kernel支持的话，还需要打开以下开关（无需内核支持可参考Open vSwitch without Kernel Support ）。 受支持的内核版本。 Ingress policing的支持，需要以嵌入或模块形式打开 NET_CLS_BASIC、 NET_SCH_INGRESS和 NET_ACT_POLICE。 3.11之前的内核，不得加载ip_gre模块（NET_IPGRE）。 开启CONFIG_TUN以使能ovs对TAP设备的支持。 与内核相对应的GCC版本。 一个内核构建目录对应于模块要运行的linux内核镜像（linux-header相关文件）。 如果使用git树或者修改了打开的vswitch构建系统或数据库架构，则还需要以下软件： Autoconf version 2.63 or later. Automake version 1.10 or later. libtool version 2.4 or later. (Older versions might work too.) 还有一些 datapath test 相关包，此处不在赘述。对于一个系统，基本下面的命令搞定 1# yum install -y autoconf automake libtool kernel-devel-$(uname -r) 为了支持TAP 和 随机，需要/dev/urandom和/dev/net/tun这两个文件存在。 构建”configure”脚本 1# ./boot.sh 配置 默认情况下，所有的文件都安装在/usr/local目录下，即下面的命令 1# ./configure 此时，openvswitch会在／usr/local/etc/openvswitch下查找数据库。 但是如果想按照传统安装方式（/usr/local -&gt; /usr, /usr/local/var -&gt; /var, /etc/openvswitch作为默认数据库目录），则 1# ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc 构建Linux内核模块，以便您可以运行基于内核的交换机，在–with-linux上传递内核构建目录的位置。 1# ./configure --with-linux=/lib/modules/$(uname -r)/build 以上总结，直接运行 1# ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc --with-linux=/lib/modules/$(uname -r)/build 除此之前，还可以编译不同架构的ovs，例如 1# ./configure --with-linux=/path/to/linux KARCH=mips 还有其他的选项，可参考官网。 Building make install 默认将程序及帮助文件装入/usr/local 1# make ; make install 安装内核模块 1# make modules_install 如果运行时遇到类似如下错误 1234567891011121314# make modules_installcd datapath/linux &amp;&amp; make modules_installmake[1]: Entering directory `/root/openvswitch-2.9.0/datapath/linux'make -C /lib/modules/3.10.0-693.11.1.el7.x86_64/build M=/root/openvswitch-2.9.0/datapath/linux modules_installmake[2]: Entering directory `/usr/src/kernels/3.10.0-693.11.1.el7.x86_64' INSTALL /root/openvswitch-2.9.0/datapath/linux/openvswitch.koCan't read private key ……………… INSTALL /root/openvswitch-2.9.0/datapath/linux/vport-vxlan.koCan't read private key DEPMOD 3.10.0-693.11.1.el7.x86_64make[2]: Leaving directory `/usr/src/kernels/3.10.0-693.11.1.el7.x86_64'depmod `sed -n 's/#define UTS_RELEASE "\([^"]*\)"/\1/p' /lib/modules/3.10.0-693.11.1.el7.x86_64/build/include/generated/utsrelease.h`make[1]: Leaving directory `/root/openvswitch-2.9.0/datapath/linux' 恭喜你，和我一样，中招了。 此问题是安装内核模块是遇错，那么手动安装即可，并保持重启已经安装，所以需要以下操作 12345678910111213141516171819202122// 查看这几个模块的依赖关系是否建立，一般在最后几行# vim /lib/modules/$(uname -r)/modules.dep// 查看相对目录的ko文件是否是刚编译过的# cd /lib/modules/$(uname -r)/extra# ls -al# date// 如果不是最新，从源码目录`datapath/linux`中把几个ko文件重新移入过去。// 之后改写文件，让其可以开机加载就可以了（不想重启就手动加载）# cat /etc/sysconfig/modules/ovs.modules #!/bin/shmodprobe gremodprobe libcrc32cmodprobe openvswitchmodprobe vport-genevemodprobe vport-gremodprobe vport-lispmodprobe vport-sttmodprobe vport-vxlan# chmod +x /etc/sysconfig/modules/ovs.modules (文件加入可执行权限)# reboot 开始 全自动执行 12345// 加入环境变量，或者直接绝对路径执行// 会启动 ovs-vswitchd 和 ovsdb-server//不在此目录，则'find /usr/ -name 'ovs-ctl''找一下# export PATH=$PATH:/usr/local/share/openvswitch/scripts# ovs-ctl start 半自动（指定启动项） 1234//只启动 ovsdb-server# ovs-ctl --no-ovs-vswitchd start//只启动 ovs-vswitchd# ovs-ctl --no-ovsdb-server start 手动执行 1234//不用 ovs-ctl工具//创建数据库目录（根据configure），此处为默认# mkdir -p /usr/local/etc/openvswitch# ovsdb-tool create /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema 配置ovsdb-serve以使用数据库 1234567# mkdir -p /usr/local/var/run/openvswitch# ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \ --remote=db:Open_vSwitch,Open_vSwitch,manager_options \ --private-key=db:Open_vSwitch,SSL,private_key \ --certificate=db:Open_vSwitch,SSL,certificate \ --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \ --pidfile --detach --log-file Note：如果openvswitch 不支持ssl,则忽–private-key,–certificate,–bootstrap-ca-cert`。 初始化数据库 1# ovs-vsctl --no-wait init 开启openvswitch主要进程 1# ovs-vswitchd --pidfile --detach --log-file 验证 12# ovs-vsctl add-br br0# ovs-vsctl add-port br0 eth0 升级 终止进程 本例为默认目录 1# kill `cd /usr/local/var/run/openvswitch &amp;&amp; cat ovsdb-server.pid ovs-vswitchd.pid` 安装新的openvswitch版本 不改变配置直接编译即可； 更改配置（目录及编译选项）。 升级数据库 库内无重要数据则直接删除重建； 有重要数据则先备份数据库，然后利用ovsdb-toolconvert来进行升级 1# ovsdb-tool convert /usr/local/etc/openvswitch/conf.db vswitchd/vswitch.ovsschema 启动进程。 1# ovs-ctl start 热升级一句话概括：可利用 ovs-ctl restart一次性搞定，如果有内核修改则利用ovs-ctl force-reload-kmod。 升级仅涉及用户空间程序（实用程序、守护进程），确保新的版本与之前加载的内核模块兼容。 用户空间守护进程的升级意味着它们必须重新启动。重新启动守护进程意味着ovs-vswitchd守护进程中的openflow流将丢失。 恢复流量的一种方法是让控制器重新填充它。 另一种方法是使用像ovs-ofctl这样的工具保存以前的流程，然后在重新启动后重新添加它们。只有当新的开放vswitch接口保留旧的“端口”值时，恢复旧流才是准确的。 123456// ovs-save 可以保存每个桥的流表。ovs-save COMMAND &#123;bridge1|bridge2&#125;// ovs-ofctl 封装而成。具体路径查找方式前面已写。# /usr/share/openvswitch/scripts/ovs-save save-flows br-int //进程重启之后可通过保存文件进行恢复# ovs-ofctl replace-flows br-int /tmp/ovs-save.WvZfM1zEhH/br-int.flows.dump -O OpenFlow14 当新用户空间守护进程重新启动时，它们会自动刷新内核中的旧流设置。如果有数百个进入内核的新流程，但用户空间守护进程正在忙于从控制器或ovs-ofctl等实用程序中设置新的用户空间流量，则这可能会很耗时（冲突）。 打开vswitch数据库提供了一个通过open_vswitch表的other_config:flow-restore-wait列解决此问题的选项。有关详细信息，请参阅ovs-vswitchd.conf.db（5）手册页。 12345678此选项热升级的过程如下：1、终止ovs-vswitchd2、设置`other_config:flow-restore-wait`为true3、开启ovs-vswitchd4、利用ovs-ofctl恢复流表5、设置`other_config:flow-restore-wait`为falseovs-ctl 选项 `restart` 和 `force-reload-kmod`利用了此过程。 如果升级还涉及升级内核模块，则需要卸载旧的内核模块，并且应该加载新的内核模块。 这意味着属于开放vswitch的内核网络设备被重新创建，并且内核流程丢失。如果用户空间守护程序立即重新启动并且用户空间流尽快恢复，则可以减少流量的停机时间。 1`force-reload-kmod`卸载 vport* 和 openvswitch模块，重装 openvswitch 模块。 ovs-ctl实用程序的重新启动功能仅重新启动用户空间守护程序，确保’ofport’值在重新启动时保持一致，使用ovs-ofctl实用程序还原用户空间流，并使用other_config:flow-restore-wait列保留交通宕机时间降至最低。 ovs-ctl实用程序的force-reload-kmod函数完成了上述所有操作，但也用新的内核模块替换了旧的内核模块。 打开debian，xenserver和rhel的vswitch启动脚本使用ovs-ctl的功能，并且建议这些功能也可用于其他软件平台。 升级实例验证升级构建的拓扑如下 过程如下 br-tun、br-int、 qbr、netns创建 12345# ovs-vsctl add-br br-tun # ovs-vsctl add-br br-int# brctl addbr qbr0 # ip netns add ns0# ip netns exec ns0 ip link set dev lo up br-tun、br-int 连接 12# ovs-vsctl add-port br-tun patch-int -- set Interface patch-int type=patch options:peer=patch-tun# ovs-vsctl add-port br-int patch-tun -- set Interface patch-tun type=patch options:peer=patch-int br-int、qbr连接 1234# ip link add qvo type veth peer name qvb# brctl addif qbr0 qvb # ovs-vsctl add-port br-int qvo # ifconfig qvb up;ifconfig qvo up; ifconfig qbr0 up; qbr、ns连接 12345# ip link add qn0 type veth peer name qn1# brctl addif qbr0 qn0# ip link set qn1 netns ns0# ip netns exec ns0 ifconfig qn1 10.1.1.1/24 up# ifconfig qn0 up br-tun对端连接 12# ovs-vsctl add-port br-tun vxlan1 -- set interface vxlan1 type=vxlan options:remote_ip=192.168.32.5 options:local_ip=192.168.32.4 //对端将remote_ip local_ip换一下位置 进入对端netns ，ping本端netns内部ip地址，且在本端eth0接口抓包验证。 br-int 和 br-tun流表默认都是全通，所以修改一下，为了升级之后进行确认。 升级！（升级之前的准备见上文，升级过程中一直ping）。 不升级内核 12345678910# /usr/share/openvswitch/scripts/ovs-ctl restart Saving flows [ OK ]Exiting ovsdb-server (119795) [ OK ]Starting ovsdb-server [ OK ]system ID not configured, please use --system-id ... failed!Configuring Open vSwitch system IDs [ OK ]Exiting ovs-vswitchd (119865) [ OK ]Starting ovs-vswitchd [ OK ]Restoring saved flows [ OK ]Enabling remote OVSDB managers [ OK ] 流量不中断。 升级内核 1234567891011121314151617# /usr/share/openvswitch/scripts/ovs-ctl force-reload-kmodDetected internal interfaces: br-int br-tun [ OK ]Saving flows [ OK ]Exiting ovsdb-server (1278) [ OK ]Starting ovsdb-server [ OK ]system ID not configured, please use --system-id ... failed!Configuring Open vSwitch system IDs [ OK ]Exiting ovs-vswitchd (1304) [ OK ]Saving interface configuration [ OK ]Removing datapath: system@ovs-system [ OK ]Removing vport_vxlan module [ OK ]Removing openvswitch module [ OK ]Inserting openvswitch module [ OK ]Starting ovs-vswitchd [ OK ]Restoring saved flows [ OK ]Enabling remote OVSDB managers [ OK ]Restoring interface configuration [ OK ] 流量不中断。 Notes：升级的version &gt;= 2.8.2 时，不会重新加载vport*内核模块，这是因为 在RHEL 7.x上，遇到了一个由iptables启动脚本引起的错误，该脚本尝试删除与linux conntrack相关的所有内核模块。它无法卸载openvswitch内核模块，因为它有一个引用计数。但它成功地卸载了vport-geneve，并转而用上游的“geneve”内核模块。这会导致隧道断开。通过不加载基于vport的内核模块来避免上述情况。 ovs-vswitchd启动时将加载上游模块。(参考此处)。 验证接口及流表（流量不断基本就没什么问题）。 Notes：ovs在openstack环境中升级时，需要将openvswitch-agent先停止，以防止ovs相关进程终止时其自动拉起。]]></content>
      <categories>
        <category>openvswitch</category>
      </categories>
      <tags>
        <tag>ovs编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Netpoll浅析]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-Netpoll%E6%B5%85%E6%9E%90.html</url>
    <content type="text"><![CDATA[本文基于Linux kernel v3.10.105。 简介网卡处理流量都几种方式： 中断处理：每次接受报文都会发生中断。此种方式在高速网络中会使系统性能全面下降。 poll方式：不依靠中断，完全依靠轮询。读取特定的寄存器，条件合适时进行收发数据。 NAPI，上述两者的结合，具体在之前的文章中已经讲解过。 由于netpoll不依靠中断，因此可以在以下场合使用： 系统panic之后。此时中断控制器将可能被disable掉，无论如何，此时的机器已经和外界失联了，此时可以通过netpoll对外界通告自己的死因。 协议栈故障。如果使用中断或者NAPI的方式，由于它上接的就是协议栈，netif_receive_skb中又没有什么HOOK点，此时使用netpoll可以改变数据包的处理路径，通过一个agent可以实现远程debug。 中断是通知机制（或被动）查询网卡状态，netpoll是轮询（或主动）方式查询网卡状态。 主动调用网卡的中断处理函数，获取当前该发送数据包还是接收到一个数据包； 直接hard_xmit数据包或者使用NAPI的接口去poll网卡的数据。 Linux netpoll总体图如下： netpoll是Linux内核中的一种在协议栈不可用或者中断机制异常的情况下与外界通讯的手段，当然它也是一种绕开协议栈的方法。Netfilter是在协议栈的特殊点捕获数据包的，而netpoll却可以在网卡之上直接捕获数据包，它们甚至连协议栈的最底端都到不了。 代码分析netpoll 和 netpoll_infonetpoll结构用来描述接收和发送数据包的必要信息。 123456789101112131415struct netpoll &#123; struct net_device *dev; /*绑定的设备*/ char dev_name[IFNAMSIZ]; /*设备名*/ const char *name; /*netpoll实例的名称*/ /*接收数据包的接口*/ void (*rx_hook)(struct netpoll *, int, char *, int); /*本地、远端IP地址*/ union inet_addr local_ip, remote_ip; bool ipv6; /*ipv6支持情况*/ u16 local_port, remote_port; /*本地、远端port*/ u8 remote_mac[ETH_ALEN]; /*远端 mac地址*/ struct list_head rx; /* rx_np list element */ struct work_struct cleanup_work;&#125;; 网络设备中，当支持netpoll时，必须实现变量npinfo： 1234567struct net_device &#123;……#ifdef CONFIG_NETPOLL struct netpoll_info __rcu *npinfo;#endif……&#125;； 123456789101112131415struct netpoll_info &#123; atomic_t refcnt; /*引用计数*/ /*接收标志，NETPOLL_RX_ENABLED 或 NETPOLL_RX_DROP*/ unsigned long rx_flags; spinlock_t rx_lock; struct semaphore dev_lock; struct list_head rx_np; /*注册的rx_hook的netpolls*/ struct sk_buff_head neigh_tx; /*请求回复的邻居请求列表*/ struct sk_buff_head txq; struct delayed_work tx_work; struct netpoll *netpoll; struct rcu_head rcu;&#125;; 初始化模块初始化1234567static struct sk_buff_head skb_pool;……static int __init netpoll_init(void)&#123; skb_queue_head_init(&amp;skb_pool); return 0;&#125; 接口结构体初始化npinfo在函数__netpoll_setup中进行分配初始化，查询代码可知仅有vlan、bond和bridge类型的接口注册函数调用__netpoll_setup。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768int __netpoll_setup(struct netpoll *np, struct net_device *ndev, gfp_t gfp)&#123; struct netpoll_info *npinfo; const struct net_device_ops *ops; unsigned long flags; int err; /*设备netpoll信息初始化，dev指针、dev_name和工作队列*/ np-&gt;dev = ndev; strlcpy(np-&gt;dev_name, ndev-&gt;name, IFNAMSIZ); INIT_WORK(&amp;np-&gt;cleanup_work, netpoll_async_cleanup); /*设备禁用*/ if ((ndev-&gt;priv_flags &amp; IFF_DISABLE_NETPOLL) || !ndev-&gt;netdev_ops-&gt;ndo_poll_controller) &#123; np_err(np, "%s doesn't support polling, aborting\n", np-&gt;dev_name); err = -ENOTSUPP; goto out; &#125; if (!ndev-&gt;npinfo) &#123; npinfo = kmalloc(sizeof(*npinfo), gfp); if (!npinfo) &#123; err = -ENOMEM; goto out; &#125; npinfo-&gt;rx_flags = 0; INIT_LIST_HEAD(&amp;npinfo-&gt;rx_np); spin_lock_init(&amp;npinfo-&gt;rx_lock); sema_init(&amp;npinfo-&gt;dev_lock, 1); skb_queue_head_init(&amp;npinfo-&gt;neigh_tx); skb_queue_head_init(&amp;npinfo-&gt;txq); INIT_DELAYED_WORK(&amp;npinfo-&gt;tx_work, queue_process); atomic_set(&amp;npinfo-&gt;refcnt, 1); ops = np-&gt;dev-&gt;netdev_ops; if (ops-&gt;ndo_netpoll_setup) &#123; err = ops-&gt;ndo_netpoll_setup(ndev, npinfo, gfp); if (err) goto free_npinfo; &#125; &#125; else &#123; npinfo = rtnl_dereference(ndev-&gt;npinfo); atomic_inc(&amp;npinfo-&gt;refcnt); &#125; npinfo-&gt;netpoll = np; if (np-&gt;rx_hook) &#123; spin_lock_irqsave(&amp;npinfo-&gt;rx_lock, flags); npinfo-&gt;rx_flags |= NETPOLL_RX_ENABLED; list_add_tail(&amp;np-&gt;rx, &amp;npinfo-&gt;rx_np); spin_unlock_irqrestore(&amp;npinfo-&gt;rx_lock, flags); &#125; /* last thing to do is link it to the net device structure */ rcu_assign_pointer(ndev-&gt;npinfo, npinfo); return 0;free_npinfo: kfree(npinfo);out: return err;&#125; 收包-rx12345678static inline int netpoll_recßeive_skb(struct sk_buff *skb)&#123; //NAPI部分之前的文章已经写过，不在分析。 //现在NETPOLL 基本在 NAPI中调用。 if (!list_empty(&amp;skb-&gt;dev-&gt;napi_list)) return netpoll_rx(skb); return 0;&#125; 此函数仅在__netif_receive_skb_core开头被调用。 123456789101112131415161718192021222324252627282930static inline bool netpoll_rx_on(struct sk_buff *skb)&#123; struct netpoll_info *npinfo = rcu_dereference_bh(skb-&gt;dev-&gt;npinfo); // 或 rx_flags == NETPOLL_RX_ENABLED return npinfo &amp;&amp; (!list_empty(&amp;npinfo-&gt;rx_np) || npinfo-&gt;rx_flags);&#125;static inline bool netpoll_rx(struct sk_buff *skb)&#123; struct netpoll_info *npinfo; unsigned long flags; bool ret = false; local_irq_save(flags); //判断netpoll rx是否开启，未开启则退出。 if (!netpoll_rx_on(skb)) goto out; //接口的netpoll 信息 npinfo = rcu_dereference_bh(skb-&gt;dev-&gt;npinfo); spin_lock(&amp;npinfo-&gt;rx_lock); /* check rx_flags again with the lock held */ // 进入 netpoll 主要收包函数 __netpoll_rx if (npinfo-&gt;rx_flags &amp;&amp; __netpoll_rx(skb, npinfo)) ret = true; spin_unlock(&amp;npinfo-&gt;rx_lock);out: local_irq_restore(flags); return ret;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109int __netpoll_rx(struct sk_buff *skb, struct netpoll_info *npinfo)&#123; int proto, len, ulen; int hits = 0; const struct iphdr *iph; struct udphdr *uh; struct netpoll *np, *tmp; if (list_empty(&amp;npinfo-&gt;rx_np)) goto out; if (skb-&gt;dev-&gt;type != ARPHRD_ETHER) goto out; /* 检查是否为邻居表项。ipv4 为 ARP，v6 在pkt_is_ns 处理 */ if (skb-&gt;protocol == htons(ETH_P_ARP) &amp;&amp; atomic_read(&amp;trapped)) &#123; skb_queue_tail(&amp;npinfo-&gt;neigh_tx, skb); return 1; &#125; else if (pkt_is_ns(skb) &amp;&amp; atomic_read(&amp;trapped)) &#123; skb_queue_tail(&amp;npinfo-&gt;neigh_tx, skb); return 1; &#125; // vlan 则 去掉vlan tag if (skb-&gt;protocol == cpu_to_be16(ETH_P_8021Q)) &#123; skb = vlan_untag(skb); if (unlikely(!skb)) goto out; &#125; //收包既不为IPV4 也不为 IPV6 ，丢包 //其他主机包，丢包 //已被拷贝，丢包 proto = ntohs(eth_hdr(skb)-&gt;h_proto); if (proto != ETH_P_IP &amp;&amp; proto != ETH_P_IPV6) goto out; if (skb-&gt;pkt_type == PACKET_OTHERHOST) goto out; if (skb_shared(skb)) goto out; if (proto == ETH_P_IP) &#123; // 进入 IPV4 处理流程 if (!pskb_may_pull(skb, sizeof(struct iphdr))) goto out; iph = (struct iphdr *)skb-&gt;data; // IP 头部最小长度为20字节，20 * 8bit = 160 bit // 160 / 32 = 5 if (iph-&gt;ihl &lt; 5 || iph-&gt;version != 4) goto out; if (!pskb_may_pull(skb, iph-&gt;ihl*4)) goto out; iph = (struct iphdr *)skb-&gt;data; //校验和 if (ip_fast_csum((u8 *)iph, iph-&gt;ihl) != 0) goto out; len = ntohs(iph-&gt;tot_len);//总长度字段 if (skb-&gt;len &lt; len || len &lt; iph-&gt;ihl*4) goto out; /* 传输层可能有缓冲区，进行裁剪 */ if (pskb_trim_rcsum(skb, len)) goto out; //仅处理 UDP ？ iph = (struct iphdr *)skb-&gt;data; if (iph-&gt;protocol != IPPROTO_UDP) goto out; len -= iph-&gt;ihl*4; uh = (struct udphdr *)(((char *)iph) + iph-&gt;ihl*4); ulen = ntohs(uh-&gt;len); if (ulen != len) goto out; if (checksum_udp(skb, uh, ulen, iph-&gt;saddr, iph-&gt;daddr)) goto out; list_for_each_entry_safe(np, tmp, &amp;npinfo-&gt;rx_np, rx) &#123; if (np-&gt;local_ip.ip &amp;&amp; np-&gt;local_ip.ip != iph-&gt;daddr) continue; if (np-&gt;remote_ip.ip &amp;&amp; np-&gt;remote_ip.ip != iph-&gt;saddr) continue; if (np-&gt;local_port &amp;&amp; np-&gt;local_port != ntohs(uh-&gt;dest)) continue; np-&gt;rx_hook(np, ntohs(uh-&gt;source), (char *)(uh+1), ulen - sizeof(struct udphdr)); hits++; &#125; &#125; else &#123;#if IS_ENABLED(CONFIG_IPV6) //IPV6 略过 …………#endif &#125; if (!hits) goto out; kfree_skb(skb); return 1;out: if (atomic_read(&amp;trapped)) &#123; kfree_skb(skb); return 1; &#125; return 0;&#125; 发包-tx1234567static inline void netpoll_send_skb(struct netpoll *np, struct sk_buff *skb)&#123; unsigned long flags; local_irq_save(flags); netpoll_send_skb_on_dev(np, skb, np-&gt;dev); local_irq_restore(flags);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/* call with IRQ disabled */void netpoll_send_skb_on_dev(struct netpoll *np, struct sk_buff *skb, struct net_device *dev)&#123; int status = NETDEV_TX_BUSY; unsigned long tries; //接口网络操作函数集 const struct net_device_ops *ops = dev-&gt;netdev_ops; /* It is up to the caller to keep npinfo alive. */ struct netpoll_info *npinfo; WARN_ON_ONCE(!irqs_disabled()); npinfo = rcu_dereference_bh(np-&gt;dev-&gt;npinfo); if (!npinfo || !netif_running(dev) || !netif_device_present(dev)) &#123; __kfree_skb(skb); return; &#125; /* 不要按顺序发送消息，也不要递归 */ if (skb_queue_len(&amp;npinfo-&gt;txq) == 0 &amp;&amp; !netpoll_owner_active(dev)) &#123; struct netdev_queue *txq; //多个发包队列的话，选择一个合适的队列 txq = netdev_pick_tx(dev, skb); /* try until next clock tick */ for (tries = jiffies_to_usecs(1)/USEC_PER_POLL; tries &gt; 0; --tries) &#123; //锁队列，并将owner置为本cpu if (__netif_tx_trylock(txq)) &#123; if (!netif_xmit_stopped(txq)) &#123; //vlan 处理部分 if (vlan_tx_tag_present(skb) &amp;&amp; !vlan_hw_offload_capable(netif_skb_features(skb), skb-&gt;vlan_proto)) &#123; skb = __vlan_put_tag(skb, skb-&gt;vlan_proto, vlan_tx_tag_get(skb)); if (unlikely(!skb)) &#123; status = NETDEV_TX_OK; goto unlock_txq; &#125; skb-&gt;vlan_tci = 0; &#125; //接口实际发包函数发包 status = ops-&gt;ndo_start_xmit(skb, dev); if (status == NETDEV_TX_OK) txq_trans_update(txq); &#125; unlock_txq: __netif_tx_unlock(txq); if (status == NETDEV_TX_OK) break; &#125; /* tickle device maybe there is some cleanup */ netpoll_poll_dev(np-&gt;dev); udelay(USEC_PER_POLL); &#125; WARN_ONCE(!irqs_disabled(), "netpoll_send_skb_on_dev(): %s enabled interrupts in poll (%pF)\n", dev-&gt;name, ops-&gt;ndo_start_xmit); &#125; if (status != NETDEV_TX_OK) &#123; skb_queue_tail(&amp;npinfo-&gt;txq, skb); schedule_delayed_work(&amp;npinfo-&gt;tx_work,0); &#125;&#125; 优秀资料netpoll浅析 Linux内核的netpoll框架与netconsole]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>Netpoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAPI/非NAPI收包分析]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FNAPI-%E9%9D%9ENAPI%E6%94%B6%E5%8C%85%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[基于 kernel 3.10.105 分析。 softnet_data每个CPU都有队列，用来接收进来的帧。因为每个CPU都有其数据结构用来处理入口和出口流量，因此，不同CPU之间没必要使用上锁机制。此队列的数据结构softnet_data定义在include/linux/netdevice.h中，如下所示： 1234567891011121314151617181920212223242526272829struct softnet_data &#123; struct Qdisc *output_queue; /*出口规则队列*/ struct Qdisc **output_queue_tailp; struct list_head poll_list; /*双向列表，其中是设备有数据要传输*/ struct sk_buff *completion_queue; /*缓冲区列表，其中的缓冲已成功传输，因此可以释放掉*/ struct sk_buff_head process_queue; /*要处理的包(skb)*/ /* stats */ unsigned int processed; /*已处理的包(skb)*/ /*如果`ksoftirq`进程在cpu-time启动之前无法处理网络设备环形缓冲区中所有可用的数据包，则会更新`time_squeeze`*/ unsigned int time_squeeze; unsigned int cpu_collision; /*发送数据时，发包队列被其他CPU占用则更新此字段*/ unsigned int received_rps; /*RPS收到的包*/#ifdef CONFIG_RPS struct softnet_data *rps_ipi_list; /*本地RPS队列*/ struct call_single_data csd ____cacheline_aligned_in_smp; /*下一个要处理的 data，和rps_ipi_list 主要应用于rps_ipi_queued函数 和 net_rps_action_and_irq_enable 函数。*/ struct softnet_data *rps_ipi_next; unsigned int cpu; /*字段所属cpu*/ /*RPS队列头尾计数*/ unsigned int input_queue_head; unsigned int input_queue_tail;#endif unsigned int dropped; /*丢包计数*/ struct sk_buff_head input_pkt_queue; /*保存进来的帧*/ struct napi_struct backlog; /*虚拟的NAPI设备*/&#125;; 初始化初始化在文件net/core/dev.c的函数net_dev_init中： 123456789101112131415161718192021222324252627static int __init net_dev_init(void)&#123; …… for_each_possible_cpu(i) &#123; struct softnet_data *sd = &amp;per_cpu(softnet_data, i); memset(sd, 0, sizeof(*sd)); skb_queue_head_init(&amp;sd-&gt;input_pkt_queue); skb_queue_head_init(&amp;sd-&gt;process_queue); sd-&gt;completion_queue = NULL; INIT_LIST_HEAD(&amp;sd-&gt;poll_list); sd-&gt;output_queue = NULL; sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue; /*RPS相关*/#ifdef CONFIG_RPS sd-&gt;csd.func = rps_trigger_softirq; sd-&gt;csd.info = sd; sd-&gt;csd.flags = 0; sd-&gt;cpu = i;#endif sd-&gt;backlog.poll = process_backlog; sd-&gt;backlog.weight = weight_p; sd-&gt;backlog.gro_list = NULL; sd-&gt;backlog.gro_count = 0; &#125; ……&#125; 非NAPI以vortex_rx为例。 vortex_interrupt为中断处理函数，收包调用vortex_rx。 vortex_rx12345678910111213141516171819static int vortex_rx(struct net_device *dev)&#123; struct vortex_private *vp = netdev_priv(dev); void __iomem *ioaddr = vp-&gt;ioaddr; int i; short rx_status; …… /* The packet length: up to 4.5K!. */ int pkt_len = rx_status &amp; 0x1fff; struct sk_buff *skb; /*skb空间分配*/ skb = netdev_alloc_skb(dev, pkt_len + 5); …… …… skb-&gt;protocol = eth_type_trans(skb, dev); netif_rx(skb); dev-&gt;stats.rx_packets++; ……&#125; netif_rx非NAPI处理报文上半部函数为netif_rx，代码如下： 12345678910111213141516171819202122232425262728293031323334int netif_rx(struct sk_buff *skb)&#123; int ret; if (netpoll_rx(skb)) return NET_RX_DROP; /*检查时间戳*/ net_timestamp_check(netdev_tstamp_prequeue, skb); trace_netif_rx(skb);#ifdef CONFIG_RPS if (static_key_false(&amp;rps_needed)) &#123; struct rps_dev_flow voidflow, *rflow = &amp;voidflow; int cpu; preempt_disable(); rcu_read_lock(); cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow); if (cpu &lt; 0) cpu = smp_processor_id(); ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail); rcu_read_unlock(); preempt_enable(); &#125; else#endif &#123; unsigned int qtail; /*入队*/ /*get_cpu 禁止抢占--返回当前cpu id*/ ret = enqueue_to_backlog(skb, get_cpu(), &amp;qtail); put_cpu(); &#125; return ret;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940static int enqueue_to_backlog(struct sk_buff *skb, int cpu, unsigned int *qtail)&#123; struct softnet_data *sd; unsigned long flags; sd = &amp;per_cpu(softnet_data, cpu); local_irq_save(flags); rps_lock(sd); /*空间充足*/ if (skb_queue_len(&amp;sd-&gt;input_pkt_queue) &lt;= netdev_max_backlog) &#123; /*不为0，说明设备已得到调度，skb入队*/ if (skb_queue_len(&amp;sd-&gt;input_pkt_queue)) &#123;enqueue: __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb); input_queue_tail_incr_save(sd, qtail); rps_unlock(sd); local_irq_restore(flags); return NET_RX_SUCCESS; &#125; /*本CPU默认的NAPI加入要处理的poll_list队列； 之后触发软中断，但是由于处于硬中断中，所以软中断暂时失效，所以继续 enqueue */ if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;sd-&gt;backlog.state)) &#123; if (!rps_ipi_queued(sd)) ____napi_schedule(sd, &amp;sd-&gt;backlog); &#125; goto enqueue; &#125; sd-&gt;dropped++; rps_unlock(sd); local_irq_restore(flags); atomic_long_inc(&amp;skb-&gt;dev-&gt;rx_dropped); kfree_skb(skb); return NET_RX_DROP;&#125; 在下半部处理函数 net_rx_action中， 1234if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) &#123; work = n-&gt;poll(n, weight); trace_napi_poll(n); &#125; 会调用CPU默认处理函数process_backlog。 process_backlog12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*quota为本次要处理的包个数*/static int process_backlog(struct napi_struct *napi, int quota)&#123; int work = 0; struct softnet_data *sd = container_of(napi, struct softnet_data, backlog);#ifdef CONFIG_RPS /* Check if we have pending ipi, its better to send them now, * not waiting net_rx_action() end. */ if (sd-&gt;rps_ipi_list) &#123; local_irq_disable(); net_rps_action_and_irq_enable(sd); &#125;#endif /*weight_p 为全局变量，默认为64*/ napi-&gt;weight = weight_p; local_irq_disable(); /*已处理的小于请求处理，则继续循环*/ while (work &lt; quota) &#123; struct sk_buff *skb; unsigned int qlen; /*处理队列不为空，则继续；第一次为空*/ while ((skb = __skb_dequeue(&amp;sd-&gt;process_queue))) &#123; rcu_read_lock(); local_irq_enable(); /*每个包循环上送协议栈*/ __netif_receive_skb(skb); rcu_read_unlock(); local_irq_disable(); /*增加处理head计数*/ input_queue_head_incr(sd); if (++work &gt;= quota) &#123; local_irq_enable(); return work; &#125; &#125; /*获取接收队列包数量并将其插入process_queue队列*/ rps_lock(sd); qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue); if (qlen) skb_queue_splice_tail_init(&amp;sd-&gt;input_pkt_queue, &amp;sd-&gt;process_queue); /*为真说明本次处理肯定能处理完成，因此直接del队列即可*/ if (qlen &lt; quota - work) &#123; list_del(&amp;napi-&gt;poll_list); napi-&gt;state = 0; quota = work + qlen; &#125; rps_unlock(sd); &#125; local_irq_enable(); return work;&#125; 小结非NAPI是一次中断一次上送，流量突增时，则中断增多，CPU处理时间变少。 NAPINAPI混合了中断事件和轮询，而不使用纯粹的中断事件驱动模型。如果接收到新帧时，内核还没完成处理前几个帧的工作，驱动程序就没必要产生其他中断事件：让内核一直处理设备输入队列中的数据会比较简单（该设备中断功能关闭），然后当队列为空时，再重新开启中断功能。 net_device内相关结构1struct list_head napi_list; 初始化函数为 12345678struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name, void (*setup)(struct net_device *), unsigned int txqs, unsigned int rxqs)&#123; …… INIT_LIST_HEAD(&amp;dev-&gt;napi_list); ……&#125; 在netif_napi_add中更改 123456789101112131415161718192021222324void netif_napi_add(struct net_device *dev, struct napi_struct *napi, int (*poll)(struct napi_struct *, int), int weight)&#123; /*初始化napi信息*/ INIT_LIST_HEAD(&amp;napi-&gt;poll_list); napi-&gt;gro_count = 0; napi-&gt;gro_list = NULL; napi-&gt;skb = NULL; /*poll为napi处理函数，一般由驱动而定*/ napi-&gt;poll = poll; if (weight &gt; NAPI_POLL_WEIGHT) pr_err_once("netif_napi_add() called with weight %d on device %s\n", weight, dev-&gt;name); /*处理包数量*/ napi-&gt;weight = weight; /*dev_list 插入 dev-&gt;napi_list链表*/ list_add(&amp;napi-&gt;dev_list, &amp;dev-&gt;napi_list); napi-&gt;dev = dev;#ifdef CONFIG_NETPOLL spin_lock_init(&amp;napi-&gt;poll_lock); napi-&gt;poll_owner = -1;#endif set_bit(NAPI_STATE_SCHED, &amp;napi-&gt;state);&#125; 而netif_napi_add一般在驱动函数中调用，这里以ixgb为例： 123456789101112131415161718/*设备初始化例程*/static intixgb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)&#123; …… /*net_device结构分配*/ netdev = alloc_etherdev(sizeof(struct ixgb_adapter)); /*adapter为设备私有数据（适配器）*/ netdev-&gt;netdev_ops = &amp;ixgb_netdev_ops; ixgb_set_ethtool_ops(netdev); netdev-&gt;watchdog_timeo = 5 * HZ; adapter = netdev_priv(netdev); …… /*adapter-&gt;napi 初始化并加入设备napi_list链表*/ /*weight 为 64*/ netif_napi_add(netdev, &amp;adapter-&gt;napi, ixgb_clean, 64); ……&#125; ixgb_netdev_ops123456789101112131415161718static const struct net_device_ops ixgb_netdev_ops = &#123; .ndo_open = ixgb_open, .ndo_stop = ixgb_close, .ndo_start_xmit = ixgb_xmit_frame, .ndo_get_stats = ixgb_get_stats, .ndo_set_rx_mode = ixgb_set_multi, .ndo_validate_addr = eth_validate_addr, .ndo_set_mac_address = ixgb_set_mac, .ndo_change_mtu = ixgb_change_mtu, .ndo_tx_timeout = ixgb_tx_timeout, .ndo_vlan_rx_add_vid = ixgb_vlan_rx_add_vid, .ndo_vlan_rx_kill_vid = ixgb_vlan_rx_kill_vid,#ifdef CONFIG_NET_POLL_CONTROLLER .ndo_poll_controller = ixgb_netpoll,#endif .ndo_fix_features = ixgb_fix_features, .ndo_set_features = ixgb_set_features,&#125;; 其中，驱动open函数为 ixgb_open。 ixgb_open1234567891011121314151617181920212223242526/*此函数在设备active时调用(系统 IFF_UP标志)；分配发送和接收资源；分配中断号；设置watchdog timer等*/static intixgb_open(struct net_device *netdev)&#123; struct ixgb_adapter *adapter = netdev_priv(netdev); int err; err = ixgb_setup_tx_resources(adapter); if (err) goto err_setup_tx; netif_carrier_off(netdev); err = ixgb_setup_rx_resources(adapter); if (err) goto err_setup_rx; err = ixgb_up(adapter); if (err) goto err_up; netif_start_queue(netdev); return 0; ……&#125; ixgb_setup_rx_resources函数ixgb_setup_rx_resources是为收包分配资源的函数。 主要数据结构为 1234567891011121314151617181920212223242526272829/*封装一个指向套接字缓冲区的指针，所以一个DMA句柄可以和缓冲区一起存储*/struct ixgb_buffer &#123; struct sk_buff *skb; dma_addr_t dma; unsigned long time_stamp; u16 length; u16 next_to_watch; u16 mapped_as_page;&#125;;struct ixgb_desc_ring &#123; void *desc; /* 指向描述符ring的指针 */ dma_addr_t dma; /* 描述符环的物理地址；dna_addr_t 32位系统为u32 */ unsigned int size; /* 描述符ring的长度，以字节为单位 */ unsigned int count; /* ring之后描述符的数量 */ unsigned int next_to_use; /* 下一个关联缓冲区的描述符 */ unsigned int next_to_clean; /* 下一个要处理的描述符，需要检查DD状态位 */ struct ixgb_buffer *buffer_info; /* 缓冲区信息结构数组 */&#125;;/*单个接收描述符的布局。 控制器假定这个结构被打包成16个字节，对大多数编译器来说这是一个安全的假设。 但是，一些编译器可能会在字段之间插入填充，在这种情况下，必须以某种特定于编译器的方式打包结构。*/struct ixgb_rx_desc &#123; __le64 buff_addr; __le16 length; __le16 reserved; u8 status; u8 errors; __le16 special;&#125;; 1234567891011121314151617181920212223242526272829intixgb_setup_rx_resources(struct ixgb_adapter *adapter)&#123; struct ixgb_desc_ring *rxdr = &amp;adapter-&gt;rx_ring; struct pci_dev *pdev = adapter-&gt;pdev; int size; /*一次缓存的buffer个数*/ size = sizeof(struct ixgb_buffer) * rxdr-&gt;count; /*vzalloc 虚拟连续，物理可以不连续*/ rxdr-&gt;buffer_info = vzalloc(size); if (!rxdr-&gt;buffer_info) return -ENOMEM; /*ixgb_rx_desc 描述符分配，4K对齐*/ rxdr-&gt;size = rxdr-&gt;count * sizeof(struct ixgb_rx_desc); rxdr-&gt;size = ALIGN(rxdr-&gt;size, 4096); /*desc为分配的虚拟地址，rxdr-&gt;size为大小，而rxdr-&gt;dma为其物理地址！*/ rxdr-&gt;desc = dma_alloc_coherent(&amp;pdev-&gt;dev, rxdr-&gt;size, &amp;rxdr-&gt;dma, GFP_KERNEL); if (!rxdr-&gt;desc) &#123; vfree(rxdr-&gt;buffer_info); return -ENOMEM; &#125; /*初始化*/ memset(rxdr-&gt;desc, 0, rxdr-&gt;size); rxdr-&gt;next_to_clean = 0; rxdr-&gt;next_to_use = 0; return 0;&#125; 完成之后的内存如下： 在ixgb_alloc_rx_buffers中完成skb到DMA的流式映射。 ixgb_setup_tx_resources与ixgb_setup_tx_resources类似。 ixgb_up1234567891011121314151617181920212223intixgb_up(struct ixgb_adapter *adapter)&#123; struct net_device *netdev = adapter-&gt;netdev; int err, irq_flags = IRQF_SHARED; /*驱动层最大MTU为 netdev-&gt;mut(一般为1500) + 14(二层头)+ 4 (vlan)*/ int max_frame = netdev-&gt;mtu + ENET_HEADER_SIZE + ENET_FCS_LENGTH; struct ixgb_hw *hw = &amp;adapter-&gt;hw; …… /*分配中断号，flags为SHARED*/ err = request_irq(adapter-&gt;pdev-&gt;irq, ixgb_intr, irq_flags, netdev-&gt;name, netdev); …… /*开启设备； 使能NAPI； 使能中断*/ clear_bit(__IXGB_DOWN, &amp;adapter-&gt;flags); napi_enable(&amp;adapter-&gt;napi); ixgb_irq_enable(adapter); netif_wake_queue(netdev); mod_timer(&amp;adapter-&gt;watchdog_timer, jiffies); return 0;&#125; ixgb_intr12345678910111213141516static irqreturn_tixgb_intr(int irq, void *data)&#123; struct net_device *netdev = data; struct ixgb_adapter *adapter = netdev_priv(netdev); struct ixgb_hw *hw = &amp;adapter-&gt;hw; u32 icr = IXGB_READ_REG(hw, ICR); …… /*判断NAPI是否使能*/ if (napi_schedule_prep(&amp;adapter-&gt;napi)) &#123; /*禁中断并处理NAPI*/ IXGB_WRITE_REG(&amp;adapter-&gt;hw, IMC, ~0); __napi_schedule(&amp;adapter-&gt;napi); &#125; return IRQ_HANDLED;&#125; __napi_schedule1234567void __napi_schedule(struct napi_struct *n)&#123; unsigned long flags; local_irq_save(flags); ____napi_schedule(&amp;__get_cpu_var(softnet_data), n); local_irq_restore(flags);&#125; 1234567static inline void ____napi_schedule(struct softnet_data *sd, struct napi_struct *napi)&#123; /*设备的poll_list加入到CPU的链表中并触发软中断*/ list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list); __raise_softirq_irqoff(NET_RX_SOFTIRQ);&#125; net_rx_action12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576static void net_rx_action(struct softirq_action *h)&#123; struct softnet_data *sd = &amp;__get_cpu_var(softnet_data); unsigned long time_limit = jiffies + 2; /*netdev_budget 默认值为 300*/ int budget = netdev_budget; void *have; local_irq_disable(); while (!list_empty(&amp;sd-&gt;poll_list)) &#123; struct napi_struct *n; int work, weight; /* 窗口耗尽 或 时间过长则重新触发软中断 */ if (unlikely(budget &lt;= 0 || time_after_eq(jiffies, time_limit))) goto softnet_break; local_irq_enable(); /*中断启用，此访问依然安全， 因为中断只能新条目加入此列表尾部，且只有在 -&gt;poll中才能删除条目 */ /*获取第一个poll_list*/ n = list_first_entry(&amp;sd-&gt;poll_list, struct napi_struct, poll_list); have = netpoll_poll_lock(n); weight = n-&gt;weight; /* NAPI_STATE_SCHED测试是为了避免与netpoll的poll_napi()竞争。*/ work = 0; if (test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)) &#123; /*调用处理函数，继续以ixgb为例分析*/ work = n-&gt;poll(n, weight); trace_napi_poll(n); &#125; WARN_ON_ONCE(work &gt; weight); budget -= work; local_irq_disable(); /*已处理的与预期相符则表示包太多； */ if (unlikely(work == weight)) &#123; if (unlikely(napi_disable_pending(n))) &#123; local_irq_enable(); napi_complete(n); local_irq_disable(); &#125; else &#123; /*有gro则开始flush gro的包*/ if (n-&gt;gro_list) &#123; /*If HZ &lt; 1000, flush all packets. */ local_irq_enable(); napi_gro_flush(n, HZ &gt;= 1000); local_irq_disable(); &#125; list_move_tail(&amp;n-&gt;poll_list, &amp;sd-&gt;poll_list); &#125; &#125; netpoll_poll_unlock(have); &#125;out: net_rps_action_and_irq_enable(sd);#ifdef CONFIG_NET_DMA /* * There may not be any more sk_buffs coming right now, so push * any pending DMA copies to hardware */ dma_issue_pending_all();#endif return;softnet_break: sd-&gt;time_squeeze++; __raise_softirq_irqoff(NET_RX_SOFTIRQ); goto out;&#125; ixgb_clean123456789101112131415161718static intixgb_clean(struct napi_struct *napi, int budget)&#123; struct ixgb_adapter *adapter = container_of(napi, struct ixgb_adapter, napi); int work_done = 0; /*传输完成后回收资源*/ ixgb_clean_tx_irq(adapter); /*向网络堆栈发送收到的数据*/ ixgb_clean_rx_irq(adapter, &amp;work_done, budget); /* 说明包已经处理完，则退出napi模式 */ if (work_done &lt; budget) &#123; napi_complete(napi); if (!test_bit(__IXGB_DOWN, &amp;adapter-&gt;flags)) /*开启中断，驱动继续收包*/ ixgb_irq_enable(adapter); &#125; return work_done;&#125; ixgb_clean_rx_irq1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980static boolixgb_clean_rx_irq(struct ixgb_adapter *adapter, int *work_done, int work_to_do)&#123; struct ixgb_desc_ring *rx_ring = &amp;adapter-&gt;rx_ring; struct net_device *netdev = adapter-&gt;netdev; struct pci_dev *pdev = adapter-&gt;pdev; struct ixgb_rx_desc *rx_desc, *next_rxd; struct ixgb_buffer *buffer_info, *next_buffer, *next2_buffer; u32 length; unsigned int i, j; int cleaned_count = 0; bool cleaned = false; /*从上次处理的点开始处理，默认为0*/ i = rx_ring-&gt;next_to_clean; rx_desc = IXGB_RX_DESC(*rx_ring, i); buffer_info = &amp;rx_ring-&gt;buffer_info[i]; /*描述符状态为IXGB_RX_DESC_STATUS_DD时处理，此状态含义还不清楚*/ while (rx_desc-&gt;status &amp; IXGB_RX_DESC_STATUS_DD) &#123; struct sk_buff *skb; u8 status; /*已处理的包大于指定处理的包则break*/ if (*work_done &gt;= work_to_do) break; (*work_done)++; rmb(); /* read descriptor and rx_buffer_info after status DD */ /*status、skb取到临时变量*/ status = rx_desc-&gt;status; skb = buffer_info-&gt;skb; buffer_info-&gt;skb = NULL; /*prefetch预热内存，下次读取时比较容易命中*/ prefetch(skb-&gt;data - NET_IP_ALIGN); /*下次要处理的包到ring的尾则reset*/ if (++i == rx_ring-&gt;count) i = 0; next_rxd = IXGB_RX_DESC(*rx_ring, i); prefetch(next_rxd); /*多预热一次？*/ j = i + 1; if (j == rx_ring-&gt;count) j = 0; next2_buffer = &amp;rx_ring-&gt;buffer_info[j]; prefetch(next2_buffer); next_buffer = &amp;rx_ring-&gt;buffer_info[i]; cleaned = true; cleaned_count++; /*skb已取出，则unmap DMA*/ dma_unmap_single(&amp;pdev-&gt;dev, buffer_info-&gt;dma, buffer_info-&gt;length, DMA_FROM_DEVICE); buffer_info-&gt;dma = 0; length = le16_to_cpu(rx_desc-&gt;length); rx_desc-&gt;length = 0; /*skb消耗了多个缓冲区*/ if (unlikely(!(status &amp; IXGB_RX_DESC_STATUS_EOP))) &#123; pr_debug("Receive packet consumed multiple buffers length&lt;%x&gt;\n", length); dev_kfree_skb_irq(skb); goto rxdesc_done; &#125; …… /*函数作用不太理解--组包？TBD*/ ixgb_check_copybreak(netdev, buffer_info, length, &amp;skb); /* Good Receive */ skb_put(skb, length); /* Receive Checksum Offload */ ixgb_rx_checksum(adapter, rx_desc, skb); /*更新协议、vlan然后上送协议栈*/ skb-&gt;protocol = eth_type_trans(skb, netdev); if (status &amp; IXGB_RX_DESC_STATUS_VP) __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), le16_to_cpu(rx_desc-&gt;special)); netif_receive_skb(skb); ……&#125; 小结NAPI 是中断时利用__napi_schedule 将设备poll_list加到cpu的处理链表，之后唤醒下半部，下半部继续调用驱动层的处理函数poll，其中一次处理多个skb，而非传统的一个skb进行一次中断。达到了网络性能的提升。]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>NAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络RPS/RFS/GSO/GRO等功能释义]]></title>
    <url>%2F%E7%BD%91%E7%BB%9CRPS-RFS-GSO-GRO%E7%AD%89%E5%8A%9F%E8%83%BD%E9%87%8A%E4%B9%89.html</url>
    <content type="text"><![CDATA[内核代码版本号为 3.10.105。 释义与代码分析RSSRSS(Receive Side Scaling)是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。 网卡对接收到的报文进行解析，获取IP地址、协议和端口五元组信息。 网卡通过配置的HASH函数根据五元组信息计算出HASH值,也可以根据二、三或四元组进行计算。 取HASH值的低几位(这个具体网卡可能不同)作为RETA(redirection table)的索引。 根据RETA中存储的值分发到对应的CPU。 基于RSS技术程序可以通过硬件在多个CPU之间来分发数据流，并且可以通过对RETA的修改来实现动态的负载均衡。 RSS需要硬件支持。网卡接收到网络数据包后，要发送一个硬件中断，通知CPU取数据包。默认配置，都是由CPU0去做。 具体可查看某个driver的函数，例如 ‘drivers/net/ethernet/intel/e1000e/netdev.c’中的函数 ‘e1000e_setup_rss_hash’。 12345当CPU可以平行收包时，就会出现不同的核收取了同一个queue的报文，这就会产生报文乱序的问题。解决方法是将一个queue的中断绑定到唯一的一个核上去，从而避免了乱序问题。同时如果网络流量大的时候，可以将软中断均匀的分散到各个核上，避免CPU成为瓶颈。利用合理的中断绑定脚本 set_irq_affinity.sh(网上很多资源)。 如果硬件不支持RSS的话，那就可能需要下面的技术。 RPSRPS，即Receive Package Steering，其原理是单纯地以软件方式实现接收的报文在cpu之间平均分配，即利用报文的hash值找到匹配的cpu，然后将报文送至该cpu对应的backlog队列中进行下一步的处理。于 kernel 2.6.35 添加此特性。 报文hash值，可以是由网卡计算得到，也可以是由软件计算得到，具体的计算也因报文协议不同而有所差异，以tcp报文为例，tcp报文的hash值是根据四元组信息，即源ip、源端口、目的ip和目的端口进行hash计算得到的。 Linux通过配置文件的方式指定哪些cpu核参与到报文的分发处理，配置文件存放的路径是：’/sys/class/net/(dev)/queues/rx-(n)/rps_cpus’。例如： 12# 1010101# echo 85 &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus 当设置好该配置文件之后，内核就会去获取该配置文件的内容，然后根据解析的结果生成一个用于参与报文分发处理的cpu列表（实际实现是一个柔性数组），这样当收到报文之后，就可以建立起hash-cpu的映射关系了。 内核接口结构体中存在如下代码(仅列出重要部分)： 123456789101112struct net_device &#123;……#ifdef CONFIG_RPS struct netdev_rx_queue *_rx; /* Number of RX queues allocated at register_netdev() time */ unsigned int num_rx_queues; /* Number of RX queues currently active in device */ unsigned int real_num_rx_queues;#endif……&#125;; 结构’struct netdev_rx_queue’即为RPS的主要结构体，其定义如下 1234567891011121314struct rps_map &#123; unsigned int len; struct rcu_head rcu; u16 cpus[0]; //弹性数组，记录配置文件中配置的参与报文分发处理的cpu id&#125;;#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))struct netdev_rx_queue &#123; struct rps_map __rcu *rps_map; /*设备流表*/ struct rps_dev_flow_table __rcu *rps_flow_table; struct kobject kobj; struct net_device *dev;&#125; ____cacheline_aligned_in_smp; 如何进行存放？注册接口函数’register_netdevice’中会调用’netdev_register_kobject’，在此函数中设置内核文件相关配置，之后调用的函数如下 1234567netdev_register_kobject -&gt; register_queue_kobjects -&gt; net_rx_queue_update_kobjects -&gt; rx_queue_add_kobject -&gt; kobject_init_and_add -&gt; error = kobject_init_and_add(kobj, &amp;rx_queue_ktype, NULL, "rx-%u", index); rx_queue_ktype 定义如下 1234567891011121314static struct kobj_type rx_queue_ktype = &#123; .sysfs_ops = &amp;rx_queue_sysfs_ops, .release = rx_queue_release, .default_attrs = rx_queue_default_attrs, // 接口默认属性&#125;;--&gt;&gt;static struct attribute *rx_queue_default_attrs[] = &#123; &amp;rps_cpus_attribute.attr, &amp;rps_dev_flow_table_cnt_attribute.attr, NULL&#125;;--&gt;&gt;static struct rx_queue_attribute rps_cpus_attribute = __ATTR(rps_cpus, S_IRUGO | S_IWUSR, show_rps_map, store_rps_map); 因此，函数 rps_cpus的读方法为’show_rps_map’，写方法为’store_rps_map’。 123456789101112131415161718192021222324252627282930313233/*仅列出主要部分，详细函数请查看源码*/static ssize_t store_rps_map(struct netdev_rx_queue *queue, struct rx_queue_attribute *attribute, const char *buf, size_t len)&#123; …… map = kzalloc(max_t(unsigned int, RPS_MAP_SIZE(cpumask_weight(mask)), L1_CACHE_BYTES), GFP_KERNEL); if (!map) &#123; free_cpumask_var(mask); return -ENOMEM; &#125; i = 0; /*mask为配置的cpu bit,cpu为临时变量，cpu_online_mask为所有的cpu*/ for_each_cpu_and(cpu, mask, cpu_online_mask) map-&gt;cpus[i++] = cpu; if (i) map-&gt;len = i; else &#123; kfree(map); map = NULL; &#125; spin_lock(&amp;rps_map_lock); old_map = rcu_dereference_protected(queue-&gt;rps_map, lockdep_is_held(&amp;rps_map_lock)); rcu_assign_pointer(queue-&gt;rps_map, map); spin_unlock(&amp;rps_map_lock);……&#125; 配置已完成，其使用如下 12345678910111213141516171819202122int netif_rx(struct sk_buff *skb)&#123; ……#ifdef CONFIG_RPS if (static_key_false(&amp;rps_needed)) &#123; struct rps_dev_flow voidflow, *rflow = &amp;voidflow; int cpu; preempt_disable(); rcu_read_lock(); /*获取dev的cpu，算法可进入函数查看，其中调用skb_get_rxhash 得到 skb的 rxhash*/ cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow); if (cpu &lt; 0) cpu = smp_processor_id(); ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail); rcu_read_unlock(); preempt_enable(); &#125; else#endif……&#125; RPS是接收报文的时候处理，而XPS是发送报文的时候处理器优化。 XPSXPS，全称为Transmit Packet Steering，是软件支持的发包时的多队列，于 kernel 2.6.38 添加此特性。 通常 RPS 和 XPS 同id的队列选择的CPU相同，这也是防止不同CPU切换时性能消耗。 Linux通过配置文件的方式指定哪些cpu核参与到报文的分发处理，配置文件存放的路径是：’/sys/class/net/(dev)/queues/tx-(n)/rps_cpus’。例如： 12# 1010101# echo 85 &gt; /sys/class/net/eth0/queues/rx-0/xps_cpus 内核中有关xps最主要的函数就是 ‘get_xps_queue’ (关于配置如何映射到内核可参考RPS)。 12345678910111213141516171819202122232425u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)&#123; struct sock *sk = skb-&gt;sk; int queue_index = sk_tx_queue_get(sk); /*发送队列的index不合法 或者 ooo_okay 不为0时重新获取发送队列 */ /*ooo是 out of order， ooo_okay 标志表示流中没有未完成的数据包，所以发送队列可以改变而没有产生乱序数据包的风险。 传输层负责适当地设置ooo_okay。 例如，TCP在连接的所有数据已被确认时设置标志。 */ if (queue_index &lt; 0 || skb-&gt;ooo_okay || queue_index &gt;= dev-&gt;real_num_tx_queues) &#123; int new_index = get_xps_queue(dev, skb); if (new_index &lt; 0) new_index = skb_tx_hash(dev, skb); if (queue_index != new_index &amp;&amp; sk &amp;&amp; rcu_access_pointer(sk-&gt;sk_dst_cache)) sk_tx_queue_set(sk, new_index); queue_index = new_index; &#125; return queue_index;&#125; RFSRPS只是根据报文的hash值从分发处理报文的cpu列表中选取一个目标cpu，这样虽然负载均衡的效果很好，但是当用户态处理报文的cpu和内核处理报文软中断的cpu不同的时候，就会导致cpu的缓存不命中，影响性能。而RFS(Receive Flow Steering)就是用来处理这种情况的，RFS的目标是通过指派处理报文的应用程序所在的cpu来在内核态处理报文，以此来增加cpu的缓存命中率。所以RFS相比于RPS，主要差别就是在选取分发处理报文的目标cpu上，而RFS还需要依靠RPS提供的机制进行报文的后续处理。于 kernel 2.6.35 添加此特性。 RFS实现指派处理报文的应用程序所在的cpu来在内核态处理报文这一目标主要是依靠两个流表来实现的，其中一个是设备流表，记录的是上次在内核态处理该流中报文的cpu；另外一个是全局的socket流表，记录的是流中的报文渴望被处理的目标cpu。 设备流表12345678910111213141516171819struct netdev_rx_queue &#123; struct rps_map __rcu *rps_map; /*设备流表*/ struct rps_dev_flow_table __rcu *rps_flow_table; struct kobject kobj; struct net_device *dev;&#125; ____cacheline_aligned_in_smp;--&gt;&gt;struct rps_dev_flow_table &#123; unsigned int mask; struct rcu_head rcu; struct rps_dev_flow flows[0]; //弹性数组&#125;;--&gt;&gt;struct rps_dev_flow &#123; u16 cpu; /* 处理该流的cpu */ u16 filter; unsigned int last_qtail; /* sd-&gt;input_pkt_queue队列的尾部索引，即该队列长度 */&#125;; ‘struct rps_dev_flow’类型弹性数组大小由配置文件’ /sys/class/net/(dev)/queues/rx-(n)/rps_flow_cnt’进行指定的。指定方式可参考RPS一节。 全局socket流表rps_sock_flow_table是一个全局的数据流表，这个表中包含了数据流渴望被处理的CPU。这个CPU是当前处理流中报文的应用程序所在的CPU。全局socket流表会在调recvmsg，sendmsg (特别是inet_accept(), inet_recvmsg(), inet_sendmsg(), inet_sendpage() and tcp_splice_read())，被设置或者更新。全局socket流表rps_sock_flow_table的定义如下： 1234struct rps_sock_flow_table &#123; unsigned int mask; u16 ents[0];&#125;; mask成员存放的就是ents这个柔性数组的大小，该值也是通过配置文件的方式指定的，相关的配置文件为 ‘/proc/sys/net/core/rps_sock_flow_entries’。 全局socket流表会在调用recvmsg()等函数时被更新，而在这些函数中是通过调用函数sock_rps_record_flow()来更新或者记录流表项信息的，而sock_rps_record_flow()中最终又是调用函数rps_record_sock_flow()来更新ents柔性数组的，该函数实现如下： 1234567891011121314static inline void rps_record_sock_flow(struct rps_sock_flow_table *table, u32 hash)&#123; if (table &amp;&amp; hash) &#123; unsigned int cpu, index = hash &amp; table-&gt;mask; /* We only give a hint, preemption can change cpu under us */ /*当前CPU*/ cpu = raw_smp_processor_id(); /*ents存放当前cpu*/ if (table-&gt;ents[index] != cpu) table-&gt;ents[index] = cpu; &#125;&#125; 此时，再次分析函数 get_rps_cpu： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb, struct rps_dev_flow **rflowp)&#123; struct netdev_rx_queue *rxqueue; struct rps_map *map; struct rps_dev_flow_table *flow_table; struct rps_sock_flow_table *sock_flow_table; int cpu = -1; u16 tcpu; /*queue_mapping 表示是哪个 queue发送的skb，0为默认*/ if (skb_rx_queue_recorded(skb)) &#123; u16 index = skb_get_rx_queue(skb); if (unlikely(index &gt;= dev-&gt;real_num_rx_queues)) &#123; WARN_ONCE(dev-&gt;real_num_rx_queues &gt; 1, "%s received packet on queue %u, but number " "of RX queues is %u\n", dev-&gt;name, index, dev-&gt;real_num_rx_queues); goto done; &#125; /*找到对应的RPS收包队列指针*/ rxqueue = dev-&gt;_rx + index; &#125; else rxqueue = dev-&gt;_rx; /*没有配置 map 并且也没有 flow table,直接退出； map只有一个CPU且此CPU在线，则目标CPU即是此CPU*/ map = rcu_dereference(rxqueue-&gt;rps_map); if (map) &#123; if (map-&gt;len == 1 &amp;&amp; !rcu_access_pointer(rxqueue-&gt;rps_flow_table)) &#123; tcpu = map-&gt;cpus[0]; if (cpu_online(tcpu)) cpu = tcpu; goto done; &#125; &#125; else if (!rcu_access_pointer(rxqueue-&gt;rps_flow_table)) &#123; goto done; &#125; /*获取 skb-&gt;rxhash*/ skb_reset_network_header(skb); if (!skb_get_rxhash(skb)) goto done; flow_table = rcu_dereference(rxqueue-&gt;rps_flow_table); sock_flow_table = rcu_dereference(rps_sock_flow_table); if (flow_table &amp;&amp; sock_flow_table) &#123; u16 next_cpu; struct rps_dev_flow *rflow; /*此条流希望所在的CPU*/ rflow = &amp;flow_table-&gt;flows[skb-&gt;rxhash &amp; flow_table-&gt;mask]; tcpu = rflow-&gt;cpu; /*上次处理报文所在流的cpu*/ next_cpu = sock_flow_table-&gt;ents[skb-&gt;rxhash &amp; sock_flow_table-&gt;mask]; /* 两个cpu不一样时，在一下情况下进行切换： - 当前CPU未设置(RPS_NO_CPU) - 当前CPU处于脱机状态 - 当前CPU处理的数据包超出了最后一个使用这个表项入队的数据包。 这保证了流的所有以前的数据包已经被出队，从而保持了交付的顺序，不会乱序。 */ if (unlikely(tcpu != next_cpu) &amp;&amp; (tcpu == RPS_NO_CPU || !cpu_online(tcpu) || ((int)(per_cpu(softnet_data, tcpu).input_queue_head - rflow-&gt;last_qtail)) &gt;= 0)) &#123; tcpu = next_cpu; /*更改rflow CPU*/ rflow = set_rps_cpu(dev, skb, rflow, next_cpu); &#125; if (tcpu != RPS_NO_CPU &amp;&amp; cpu_online(tcpu)) &#123; *rflowp = rflow; cpu = tcpu; goto done; &#125; &#125; if (map) &#123; tcpu = map-&gt;cpus[((u64) skb-&gt;rxhash * map-&gt;len) &gt;&gt; 32]; if (cpu_online(tcpu)) &#123; cpu = tcpu; goto done; &#125; &#125;done: return cpu;&#125; LSO、TSO 和 GSO计算机网络上传输的数据基本单位是离散的网包，既然是网包，就有大小限制，这个限制就是 MTU（Maximum Transmission Unit）的大小，一般是1500字节。比如我们想发送很多数据出去，经过os协议栈的时候，会自动帮你拆分成几个不超过MTU的网包。然而，这个拆分是比较费计算资源的（比如很多时候还要计算分别的checksum），由 CPU 来做的话，往往会造成使用率过高。那可不可以把这些简单重复的操作 offload 到网卡上呢？ 于是就有了 LSO(Large Segment Offload )，在发送数据超过 MTU 限制的时候（太容易发生了），OS 只需要提交一次传输请求给网卡，网卡会自动的把数据拿过来，然后进行切，并封包发出，发出的网包不超过 MTU 限制。 而且现在基本上用不到 LSO，已经有更好的替代。 TSO(TCP Segmentation Offload): 是一种利用网卡来对大数据包进行自动分段，降低CPU负载的技术。 其主要是延迟分段。 GSO(Generic Segmentation Offload): GSO是协议栈是否推迟分段，在发送到网卡之前判断网卡是否支持TSO，如果网卡支持TSO则让网卡分段，否则协议栈分完段再交给驱动。 如果TSO开启，GSO会自动开启。 以下是TSO和GSO的组合关系： GSO开启， TSO开启：协议栈推迟分段，并直接传递大数据包到网卡，让网卡自动分段。 GSO开启， TSO关闭：协议栈推迟分段，在最后发送到网卡前才执行分段。 GSO关闭， TSO开启：同GSO开启， TSO开启。 GSO关闭， TSO关闭：不推迟分段，在tcp_sendmsg中直接发送MSS大小的数据包。 开启GSO/TSO驱动程序在注册网卡设备的时候默认开启GSO: NETIF_F_GSO。 1234567891011int register_netdevice(struct net_device *dev)&#123;…… /* #define NETIF_F_SOFT_FEATURES (NETIF_F_GSO | NETIF_F_GRO) */ dev-&gt;hw_features |= NETIF_F_SOFT_FEATURES; dev-&gt;features |= NETIF_F_SOFT_FEATURES; dev-&gt;wanted_features = dev-&gt;features &amp; dev-&gt;hw_features;……&#125; 驱动程序会根据网卡硬件是否支持来设置TSO: NETIF_F_TSO。 123456789// intel e1000 网卡static int e1000_probe(struct pci_dev *pdev, const struct pci_device_id *ent)&#123; …… if ((hw-&gt;mac_type &gt;= e1000_82544) &amp;&amp; (hw-&gt;mac_type != e1000_82547)) netdev-&gt;hw_features |= NETIF_F_TSO; ……&#125; 是否推迟分段GSO/TSO是否开启是保存在dev-&gt;features中，而设备和路由关联，当我们查询到路由后就可以把配置保存在sock中。 比如在tcp_v4_connect和tcp_v4_syn_recv_sock都会调用sk_setup_caps来设置GSO/TSO配置。 需要注意的是，只要开启了GSO，即使硬件不支持TSO，也会设置NETIF_F_TSO，使得sk_can_gso(sk)在GSO开启或者TSO开启的时候都返回true。 12345678910/* This will initiate an outgoing connection. */int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)&#123; …… /* OK, now commit destination to socket. */ sk-&gt;sk_gso_type = SKB_GSO_TCPV4; sk_setup_caps(sk, &amp;rt-&gt;dst); ……&#125; sk_setup_caps1234567891011121314151617void sk_setup_caps(struct sock *sk, struct dst_entry *dst)&#123; __sk_dst_set(sk, dst); sk-&gt;sk_route_caps = dst-&gt;dev-&gt;features; if (sk-&gt;sk_route_caps &amp; NETIF_F_GSO) sk-&gt;sk_route_caps |= NETIF_F_GSO_SOFTWARE; sk-&gt;sk_route_caps &amp;= ~sk-&gt;sk_route_nocaps; if (sk_can_gso(sk)) &#123; if (dst-&gt;header_len) &#123; sk-&gt;sk_route_caps &amp;= ~NETIF_F_GSO_MASK; &#125; else &#123; sk-&gt;sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM; sk-&gt;sk_gso_max_size = dst-&gt;dev-&gt;gso_max_size; sk-&gt;sk_gso_max_segs = dst-&gt;dev-&gt;gso_max_segs; &#125; &#125;&#125; 从上面可以看出，如果设备开启了GSO，sock都会将TSO标志打开，但是注意这和硬件是否开启TSO无关，硬件的TSO取决于硬件自身特性的支持。 sk_can_gso12345static inline bool sk_can_gso(const struct sock *sk)&#123; /*对于tcp，在tcp_v4_connect中被设置：sk-&gt;sk_gso_type = SKB_GSO_TCPV4*/ return net_gso_ok(sk-&gt;sk_route_caps, sk-&gt;sk_gso_type);&#125; net_gso_ok1234567static inline bool net_gso_ok(netdev_features_t features, int gso_type)&#123; netdev_features_t feature = gso_type &lt;&lt; NETIF_F_GSO_SHIFT; …… return (features &amp; feature) == feature;&#125; 由于tcp 在sk_setup_caps中sk-&gt;sk_route_caps也被设置有SKB_GSO_TCPV4，所以整个sk_can_gso成立。 GSO的数据包长度对紧急数据包或GSO/TSO都不开启的情况，才不会推迟发送， 默认使用当前MSS。开启GSO后，tcp_send_mss返回mss和单个skb的GSO大小，为mss的整数倍。 tcp_send_mss1234567891011static int tcp_send_mss(struct sock *sk, int *size_goal, int flags)&#123; int mss_now; /*通过ip option，SACKs及pmtu确定当前的mss*/ mss_now = tcp_current_mss(sk); /*tcp_xmit_size_goal获取发送数据报到达网络设备时数据段的最大长度，该长度用来分割数据，TCP发送报文时， *每个SKB的大小不能超过该值。 *在此传入是否标识MSG_OOB(out-of-band,比普通数据更高的优先级传送的带外数据)位，这是因为MSG_OOB是判断 *是否支持GSO的条件之一，而紧急数据不支持GSO。 *在不支持GSO的情况下，size_goal就等于mss_now，而如果支持GSO，则size_goal会是MSS的整数倍。数据报发送 *到网络设备后再由网络设备根据MSS进行分割。*/ *size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags &amp; MSG_OOB)); return mss_now;&#125; tcp_xmit_size_goal12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now, int large_allowed)&#123; struct tcp_sock *tp = tcp_sk(sk); u32 xmit_size_goal, old_size_goal; xmit_size_goal = mss_now; /*这里large_allowed表示是否是紧急数据； large_allowed为真表示无带外数据，可以大包发送*/ if (large_allowed &amp;&amp; sk_can_gso(sk)) &#123; u32 gso_size, hlen; /* Maybe we should/could use sk-&gt;sk_prot-&gt;max_header here ? */ hlen = inet_csk(sk)-&gt;icsk_af_ops-&gt;net_header_len + inet_csk(sk)-&gt;icsk_ext_hdr_len + tp-&gt;tcp_header_len; /* 目标是每ms发送至少一个数据包，而不是每100 ms发送一个大的TSO数据包。 sk_pacing_rate为 每秒的bytes。 这保留了ACK时钟，并且与tcp_tso_should_defer（）启发式一致。 sysctl_tcp_min_tso_segs 为 sysctl控制的系统变量。我的系统环境中值为2。 */ gso_size = sk-&gt;sk_pacing_rate / (2 * MSEC_PER_SEC); gso_size = max_t(u32, gso_size, sysctl_tcp_min_tso_segs * mss_now); /*xmit_size_goal为 gso最大分段大小减去tcp和ip头部长度 与 gso_size中比较小的值 */ xmit_size_goal = min_t(u32, gso_size, sk-&gt;sk_gso_max_size - 1 - hlen); /*最多达到收到的最大rwnd窗口通告的一半*/ xmit_size_goal = tcp_bound_to_half_wnd(tp, xmit_size_goal); /* We try hard to avoid divides here */ old_size_goal = tp-&gt;xmit_size_goal_segs * mss_now; if (likely(old_size_goal &lt;= xmit_size_goal &amp;&amp; old_size_goal + mss_now &gt; xmit_size_goal)) &#123; xmit_size_goal = old_size_goal; &#125; else &#123; tp-&gt;xmit_size_goal_segs = min_t(u16, xmit_size_goal / mss_now, sk-&gt;sk_gso_max_segs); xmit_size_goal = tp-&gt;xmit_size_goal_segs * mss_now; &#125; &#125; return max(xmit_size_goal, mss_now);&#125; tcp_sendmsg应用程序send()数据后，会在tcp_sendmsg中尝试在同一个skb，保存size_goal大小的数据，然后再通过tcp_push把这些包通过tcp_write_xmit发出去。 (代码涉及较多，以后进行分析，TBD) 最终会调用tcp_push发送skb，而tcp_push又会调用tcp_write_xmit。tcp_sendmsg已经把数据按照GSO最大的size，放到一个个的skb中， 最终调用tcp_write_xmit发送这些GSO包。tcp_write_xmit会检查当前的拥塞窗口，还有nagle测试，tsq检查来决定是否能发送整个或者部分的skb， 如果只能发送一部分，则需要调用tso_fragment做切分。最后通过tcp_transmit_skb发送， 如果发送窗口没有达到限制，skb中存放的数据将达到GSO最大值。 tcp_write_xmit123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle, int push_one, gfp_t gfp)&#123; struct tcp_sock *tp = tcp_sk(sk); struct sk_buff *skb; unsigned int tso_segs, sent_pkts; int cwnd_quota; int result; sent_pkts = 0; if (!push_one) &#123; /* Do MTU probing. */ result = tcp_mtu_probe(sk); if (!result) &#123; return false; &#125; else if (result &gt; 0) &#123; sent_pkts = 1; &#125; &#125; /*遍历发送队列*/ while ((skb = tcp_send_head(sk))) &#123; unsigned int limit; tso_segs = tcp_init_tso_segs(sk, skb, mss_now); BUG_ON(!tso_segs); if (unlikely(tp-&gt;repair) &amp;&amp; tp-&gt;repair_queue == TCP_SEND_QUEUE) goto repair; /* Skip network transmission */ cwnd_quota = tcp_cwnd_test(tp, skb); if (!cwnd_quota) &#123; if (push_one == 2) /* Force out a loss probe pkt. */ cwnd_quota = 1; else break; &#125; if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) break; /*tso_segs=1表示无需tso分段*/ if (tso_segs == 1 || !sk-&gt;sk_gso_max_segs) &#123; /* 根据nagle算法，计算是否需要推迟发送数据 */ if (unlikely(!tcp_nagle_test(tp, skb, mss_now, (tcp_skb_is_last(sk, skb) ? nonagle : TCP_NAGLE_PUSH)))) break; &#125; else &#123; /*有多个tso分段*/ /*push所有skb*/ /*如果发送窗口剩余不多，并且预计下一个ack将很快到来(意味着可用窗口会增加)，则推迟发送*/ if (!push_one &amp;&amp; tcp_tso_should_defer(sk, skb)) break; &#125; limit = max_t(unsigned int, sysctl_tcp_limit_output_bytes, sk-&gt;sk_pacing_rate &gt;&gt; 10); if (atomic_read(&amp;sk-&gt;sk_wmem_alloc) &gt; limit) &#123; set_bit(TSQ_THROTTLED, &amp;tp-&gt;tsq_flags); smp_mb__after_clear_bit(); if (atomic_read(&amp;sk-&gt;sk_wmem_alloc) &gt; limit) break; &#125; /*下面的逻辑是：不用推迟发送，马上发送的情况*/ limit = mss_now; /*由于tso_segs被设置为skb-&gt;len/mss_now，所以开启gso时一定大于1*/ /*tso分段大于1且非urg模式*/ if (tso_segs &gt; 1 &amp;&amp; sk-&gt;sk_gso_max_segs &amp;&amp; !tcp_urg_mode(tp)) /*返回当前skb中可以发送的数据大小，通过mss和cwnd*/ limit = tcp_mss_split_point(sk, skb, mss_now, min_t(unsigned int, cwnd_quota, sk-&gt;sk_gso_max_segs)); /* 当skb的长度大于限制时，需要调用tso_fragment分片,如果分段失败则暂不发送 */ /*按limit切割成多个skb*/ if (skb-&gt;len &gt; limit &amp;&amp; unlikely(tso_fragment(sk, skb, limit, mss_now, gfp))) break; TCP_SKB_CB(skb)-&gt;when = tcp_time_stamp; /*发送，如果包被qdisc丢了，则退出循环，不继续发送了*/ if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) break;repair: /*更新sk_send_head和packets_out*/ tcp_event_new_data_sent(sk, skb); tcp_minshall_update(tp, mss_now, skb); sent_pkts += tcp_skb_pcount(skb); if (push_one) break; &#125; if (likely(sent_pkts)) &#123; if (tcp_in_cwnd_reduction(sk)) tp-&gt;prr_out += sent_pkts; /* Send one loss probe per tail loss episode. */ if (push_one != 2) tcp_schedule_loss_probe(sk); tcp_cwnd_validate(sk); return false; &#125; return (push_one == 2) || (!tp-&gt;packets_out &amp;&amp; tcp_send_head(sk));&#125; 其中tcp_init_tso_segs会设置skb的gso信息后文分析。我们看到tcp_write_xmit 会调用tso_fragment进行“tcp分段”。而分段的条件是skb-&gt;len &gt; limit。这里的关键就是limit的值，我们看到在tso_segs &gt; 1时，也就是开启gso的时候，limit的值是由tcp_mss_split_point得到的，也就是min(skb-&gt;len, window)，即发送窗口允许的最大值。在没有开启gso时limit就是当前的mss。 tcp_init_tso_segs123456789101112131415/* Initialize TSO state of a skb. * This must be invoked the first time we consider transmitting * SKB onto the wire. */static int tcp_init_tso_segs(const struct sock *sk, struct sk_buff *skb, unsigned int mss_now)&#123; int tso_segs = tcp_skb_pcount(skb); if (!tso_segs || (tso_segs &gt; 1 &amp;&amp; tcp_skb_mss(skb) != mss_now)) &#123; tcp_set_skb_tso_segs(sk, skb, mss_now); tso_segs = tcp_skb_pcount(skb); &#125; return tso_segs;&#125; tcp_write_xmit最后会调用ip_queue_xmit发送skb，进入ip层。 流程图如下： UFOUFO(UDP fragmentation offload)，UPD的offload。 GRE 及 VXLAN接口初始化的时候，会置此位。 1234567/* Initialize the device structure. */static void vxlan_setup(struct net_device *dev)&#123; …… dev-&gt;features |= NETIF_F_GSO_SOFTWARE; ……&#125; 还有其他driver也支持，例如 macvlan、tun、virtnet等。 LRO和GRO当网卡收到很多碎片包的时候，LRO (Large Receive Offload)可以辅助自动组合成一段较大的数据，一次性提交给 OS处理。 GRO(Generic Receive Offload)，比 LSO更通用，自动检测网卡支持特性，支持分包则直接发给网卡，否则先分包后发给网卡。 driver macvlan支持GRO。 以上功能大多可以通过 ethtool -K 开启。查看网卡 offload功能： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# ethtool -k em1 Features for em1:rx-checksumming: ontx-checksumming: on tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: on tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: on [fixed] tx-checksum-sctp: onscatter-gather: on tx-scatter-gather: on tx-scatter-gather-fraglist: off [fixed]tcp-segmentation-offload: on tx-tcp-segmentation: on tx-tcp-ecn-segmentation: off [fixed] tx-tcp6-segmentation: on tx-tcp-mangleid-segmentation: offudp-fragmentation-offload: off [fixed]generic-segmentation-offload: ongeneric-receive-offload: onlarge-receive-offload: offrx-vlan-offload: ontx-vlan-offload: onntuple-filters: offreceive-hashing: onhighdma: on [fixed]rx-vlan-filter: onvlan-challenged: off [fixed]tx-lockless: off [fixed]netns-local: off [fixed]tx-gso-robust: off [fixed]tx-fcoe-segmentation: on [fixed]tx-gre-segmentation: ontx-ipip-segmentation: ontx-sit-segmentation: ontx-udp_tnl-segmentation: ontx-mpls-segmentation: off [fixed]fcoe-mtu: off [fixed]tx-nocache-copy: offloopback: off [fixed]rx-fcs: off [fixed]rx-all: offtx-vlan-stag-hw-insert: off [fixed]rx-vlan-stag-hw-parse: off [fixed]rx-vlan-stag-filter: off [fixed]busy-poll: on [fixed]tx-gre-csum-segmentation: ontx-udp_tnl-csum-segmentation: ontx-gso-partial: ontx-sctp-segmentation: off [fixed]l2-fwd-offload: offhw-tc-offload: off [fixed] 网卡支持特性比较多，值得继续研究。 总结接收侧： RSS是网卡驱动支持的多队列属性，队列通过中断绑定到不同的CPU，以实现流量负载。 RPS是以软件形式实现流量在不同CPU之间的分发。 RFS是报文需要在用户态处理时，保证处理的CPU与内核相同，防止缓存miss而导致的消耗。 LRO 和 GRO，多个报文组成一个大包上送协议栈。 发送侧： XPS 软件多队列发送。 TSO是利用网卡来对大数据包进行自动分段，降低CPU负载的技术。 GSO是协议栈分段功能。分段之前判断是否支持TSO，支持则推迟到网卡分段。 如果TSO开启，GSO会自动开启。 UFO类似TSO，不过只针对UDP报文。 优秀资料Linux多队列网卡的硬件的实现详解 Linux系统中RPS/RFS介绍 Linux中rps/rfs的原理及实现 网卡TSO/GSO/LRO/GRO简要介绍 Linux TCP GSO 和 TSO 实现 TCP发送源码学习(1)–tcp_sendmsg]]></content>
      <categories>
        <category>网络优化</category>
      </categories>
      <tags>
        <tag>RSS</tag>
        <tag>RPS</tag>
        <tag>RFS</tag>
        <tag>XPS</tag>
        <tag>LSO</tag>
        <tag>TSO</tag>
        <tag>GSO</tag>
        <tag>UFO</tag>
        <tag>LRO</tag>
        <tag>GRO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OVS数据收发流程解析]]></title>
    <url>%2FOpenvswitch%2FOVS%E6%95%B0%E6%8D%AE%E6%94%B6%E5%8F%91%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[ovs版本为 2.8.2。 OVS整体架构OVS架构图如下，具体每个部件功能不具体分析，本文主要涉及内核部分。 OVS接口类型执行命令ovs-vsctl show 或者 ovs-dpctl show(显示默认datapath)，来查看ovs接口信息时，常会看到接口类型，以下对OVS中有哪些接口类型及不同接口类型的接口之间的区别进行分析。 在源码中有这么一个函数： 123456789101112131415161718192021222324252627282930313233343536373839static const char *get_vport_type(const struct dpif_netlink_vport *vport)&#123; static struct vlog_rate_limit rl = VLOG_RATE_LIMIT_INIT(5, 20); switch (vport-&gt;type) &#123; case OVS_VPORT_TYPE_NETDEV: &#123; const char *type = netdev_get_type_from_name(vport-&gt;name); return type ? type : "system"; &#125; case OVS_VPORT_TYPE_INTERNAL: return "internal"; case OVS_VPORT_TYPE_GENEVE: return "geneve"; case OVS_VPORT_TYPE_GRE: return "gre"; case OVS_VPORT_TYPE_VXLAN: return "vxlan"; case OVS_VPORT_TYPE_LISP: return "lisp"; case OVS_VPORT_TYPE_STT: return "stt"; case OVS_VPORT_TYPE_UNSPEC: case __OVS_VPORT_TYPE_MAX: break; &#125; VLOG_WARN_RL(&amp;rl, "dp%d: port `%s' has unsupported type %u", vport-&gt;dp_ifindex, vport-&gt;name, (unsigned int) vport-&gt;type); return "unknown";&#125; 可看出，ovs-vsctl show时的类型显示在内核中都有对应关系。宏定义如下： 1234567891011enum ovs_vport_type &#123; OVS_VPORT_TYPE_UNSPEC, OVS_VPORT_TYPE_NETDEV, /* network device */ OVS_VPORT_TYPE_INTERNAL, /* network device implemented by datapath */ OVS_VPORT_TYPE_GRE, /* GRE tunnel. */ OVS_VPORT_TYPE_VXLAN, /* VXLAN tunnel. */ OVS_VPORT_TYPE_GENEVE, /* Geneve tunnel. */ OVS_VPORT_TYPE_LISP = 105, /* LISP tunnel */ OVS_VPORT_TYPE_STT = 106, /* STT tunnel */ __OVS_VPORT_TYPE_MAX&#125;; 下面，选取具有代表性的几个类型进行分析。 system此类vport（ovs-dpctl show未显示类型的接口）是对设备原有接口的封装，内核类型为’OVS_VPORT_TYPE_NETDEV’。定义的vport操作变量为’ovs_netdev_vport_ops’。 123456static struct vport_ops ovs_netdev_vport_ops = &#123; .type = OVS_VPORT_TYPE_NETDEV, .create = netdev_create, .destroy = netdev_destroy, .send = dev_queue_xmit,&#125;; 此种接口创建内部比较特殊，因此需要特殊强调。在’netdev_create’中有一段如下代码 12345678910static struct vport *netdev_create(const struct vport_parms *parms)&#123; struct vport *vport; vport = ovs_vport_alloc(0, &amp;ovs_netdev_vport_ops, parms); if (IS_ERR(vport)) return vport; return ovs_netdev_link(vport, parms-&gt;name);&#125; 12345678910111213141516171819202122232425262728struct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops, const struct vport_parms *parms)&#123; struct vport *vport; size_t alloc_size; /*vport分配空间*/ alloc_size = sizeof(struct vport); if (priv_size) &#123; alloc_size = ALIGN(alloc_size, VPORT_ALIGN); alloc_size += priv_size; &#125; vport = kzalloc(alloc_size, GFP_KERNEL); if (!vport) return ERR_PTR(-ENOMEM); /*初始化*/ vport-&gt;dp = parms-&gt;dp; vport-&gt;port_no = parms-&gt;port_no; vport-&gt;ops = ops; INIT_HLIST_NODE(&amp;vport-&gt;dp_hash_node); if (ovs_vport_set_upcall_portids(vport, parms-&gt;upcall_portids)) &#123; kfree(vport); return ERR_PTR(-EINVAL); &#125; return vport;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445struct vport *ovs_netdev_link(struct vport *vport, const char *name)&#123; int err; /*此种类型接口是对系统原有口的映射，因此dev赋值为系统原有接口的dev*/ vport-&gt;dev = dev_get_by_name(ovs_dp_get_net(vport-&gt;dp), name); if (!vport-&gt;dev) &#123; err = -ENODEV; goto error_free_vport; &#125; /*系统接口为以下情况是报错退出： - loopback口 - 非ARPHRD_ETHER接口 - OVS_VPORT_TYPE_INTERNAL类型接口 */ if (vport-&gt;dev-&gt;flags &amp; IFF_LOOPBACK || vport-&gt;dev-&gt;type != ARPHRD_ETHER || ovs_is_internal_dev(vport-&gt;dev)) &#123; err = -EINVAL; goto error_put; &#125; /*所有system都是datapatch 接口的slave接口； 默认只要一个datapath接口(ovs-system)，所有system类型的master都为此接口。 */ rtnl_lock(); err = netdev_master_upper_dev_link(vport-&gt;dev, get_dpdev(vport-&gt;dp), NULL, NULL); if (err) goto error_unlock; err = netdev_rx_handler_register(vport-&gt;dev, netdev_frame_hook, vport); if (err) goto error_master_upper_dev_unlink; /*禁用接口的lro功能；*/ dev_disable_lro(vport-&gt;dev); /*开启混杂模式*/ dev_set_promiscuity(vport-&gt;dev, 1); /*设置接口私有类型*/ vport-&gt;dev-&gt;priv_flags |= IFF_OVS_DATAPATH; rtnl_unlock(); return vport;…………………… &#125; ‘netdev_rx_handler_register’实现如下 123456789101112131415int netdev_rx_handler_register(struct net_device *dev, rx_handler_func_t *rx_handler, void *rx_handler_data)&#123; ASSERT_RTNL(); if (dev-&gt;rx_handler) return -EBUSY; /* Note: rx_handler_data must be set before rx_handler */ /*定义dev收包处理私有数据，即 vport指针，此处完成系统dev到vport的对应。 定义接口收包处理函数。*/ rcu_assign_pointer(dev-&gt;rx_handler_data, rx_handler_data); rcu_assign_pointer(dev-&gt;rx_handler, rx_handler); return 0;&#125; 此类接口定义了 ‘rx_handler’，因此，在CPU报文处理函数’__netif_receive_skb_core’中 1234567891011121314……rx_handler = rcu_dereference(skb-&gt;dev-&gt;rx_handler);……type = skb-&gt;protocol; list_for_each_entry_rcu(ptype, &amp;ptype_base[ntohs(type) &amp; PTYPE_HASH_MASK], list) &#123; if (ptype-&gt;type == type &amp;&amp; (ptype-&gt;dev == null_or_dev || ptype-&gt;dev == skb-&gt;dev || ptype-&gt;dev == orig_dev)) &#123; if (pt_prev) ret = deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; &#125; &#125; 也就是说此类接口处理报文在协议栈之前，因此netfilter对此类接口不起作用，所以在云环境（openstack）中，需要在虚拟机tap口与虚拟交换机之间增加Linux bridge设备来使报文经过协议栈（netfilter起作用）来实现security group。 internalOVS内部创建的虚拟网卡接口。每创建一个ovs bridge，OVS会自动创建一个同名接口(Interface)挂载到新创建的bridge上。或者也可以通过type=internal把已经挂载的接口设置为‘internal’类型。 1234# ovs-vsctl add-br ovs-switch/*OpenFlow 端口编号为 100*/# ovs-vsctl add-port ovs-switch p0 -- set Interface p0 ofport_request=100# ovs-vsctl set Interface p0 type=internal 对于 internal 类型的的网络接口，OVS 会同时在 Linux 系统中创建一个可以用来收发数据的模拟网络设备。我们可以为这个网络设备配置 IP 地址、进行数据监听等等。 内核中对’internal’接口的类型定义为’OVS_VPORT_TYPE_INTERNAL’（network device implemented by datapath，datapath实现的网络设备）。定义的vport操作变量为’ovs_internal_vport_ops’。 123456static struct vport_ops ovs_internal_vport_ops = &#123; .type = OVS_VPORT_TYPE_INTERNAL, .create = internal_dev_create, .destroy = internal_dev_destroy, .send = internal_dev_recv,&#125;; 接口创建时，调用’internal_dev_create’进行接口初始化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static struct vport *internal_dev_create(const struct vport_parms *parms)&#123; struct vport *vport; struct internal_dev *internal_dev; int err; vport = ovs_vport_alloc(0, &amp;ovs_internal_vport_ops, parms); if (IS_ERR(vport)) &#123; err = PTR_ERR(vport); goto error; &#125; /*内核创建接口，初始之后调用do_setup 初始化其他变量*/ vport-&gt;dev = alloc_netdev(sizeof(struct internal_dev), parms-&gt;name, NET_NAME_USER, do_setup); if (!vport-&gt;dev) &#123; err = -ENOMEM; goto error_free_vport; &#125; vport-&gt;dev-&gt;tstats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats); if (!vport-&gt;dev-&gt;tstats) &#123; err = -ENOMEM; goto error_free_netdev; &#125;#ifdef HAVE_IFF_PHONY_HEADROOM vport-&gt;dev-&gt;needed_headroom = vport-&gt;dp-&gt;max_headroom;#endif /*设置接口nd_net属性*/ dev_net_set(vport-&gt;dev, ovs_dp_get_net(vport-&gt;dp)); /*dev私有数据 映射*/ internal_dev = internal_dev_priv(vport-&gt;dev); internal_dev-&gt;vport = vport; /* Restrict bridge port to current netns. */ if (vport-&gt;port_no == OVSP_LOCAL) vport-&gt;dev-&gt;features |= NETIF_F_NETNS_LOCAL; rtnl_lock(); err = register_netdevice(vport-&gt;dev); if (err) goto error_unlock; /*设置混杂模式*/ dev_set_promiscuity(vport-&gt;dev, 1); rtnl_unlock(); netif_start_queue(vport-&gt;dev); return vport;…………………… &#125; vxlan ‘ovs-vsctl show’显示的type 为’vxlan’类型，此种接口为ovs虚拟接口。 ‘ovs-dpctl show’显示的type 为’vxlan’类型，此种接口是对系统的封装，可看做系统口。 内核中对vxlan类型的接口定义为OVS_VPORT_TYPE_VXLAN。ovs vxlan创建在文件’vport-vxlan.c’中，定义 操作如下 12345678910static struct vport_ops ovs_vxlan_netdev_vport_ops = &#123; .type = OVS_VPORT_TYPE_VXLAN, .create = vxlan_create, .destroy = ovs_netdev_tunnel_destroy, .get_options = vxlan_get_options,#ifndef USE_UPSTREAM_TUNNEL .fill_metadata_dst = vxlan_fill_metadata_dst,#endif .send = vxlan_xmit,&#125;; vxlan_create定义如下 12345678910static struct vport *vxlan_create(const struct vport_parms *parms)&#123; struct vport *vport; /*alloc 创建接口*/ vport = vxlan_tnl_create(parms); if (IS_ERR(vport)) return vport; /*link设置*/ return ovs_netdev_link(vport, parms-&gt;name);&#125; ‘ovs_netdev_link’ 函数上面已经分析过，值得注意的是，vxlan类型的接口收包函数也是 ‘netdev_frame_hook’ 。 基本上系统口都有master，而master为’ovs-system’。 对于ovs-system作用，还没搞清楚。 patchpatch 类型的接口是ovs中比较特殊的类型，其官方定义为“A pair of virtual devices that act as a patch cable”，在系统中运行man 5 ovs-vswitchd.conf.db可看到。 patch port类似于Linux系统中的veth，总是成对出现，分别连接在两个网桥上，从一个patch port收到的数据包会被转发到另一个patch port。 OVS接口报文处理system接口虚拟机利用TUN/TAP端口来与宿主机通信，此种端口是通过命令’ip tuntap add’来创建。ovs对原有接口的封装，也包括这类接口。 收包处理 netdev_frame_hook 123456789101112131415/* Called with rcu_read_lock and bottom-halves disabled. */static rx_handler_result_t netdev_frame_hook(struct sk_buff **pskb)&#123; struct sk_buff *skb = *pskb; if (unlikely(skb-&gt;pkt_type == PACKET_LOOPBACK)) return RX_HANDLER_PASS;/*USE_UPSTREAM_TUNNEL管理不同的目的处理*/#ifndef USE_UPSTREAM_TUNNEL netdev_port_receive(skb, NULL);#else netdev_port_receive(skb, skb_tunnel_info(skb));#endif return RX_HANDLER_CONSUMED;&#125; netdev_port_receive 1234567891011121314151617181920212223void netdev_port_receive(struct sk_buff *skb, struct ip_tunnel_info *tun_info)&#123; struct vport *vport; /*获取vport数据*/ vport = ovs_netdev_get_vport(skb-&gt;dev); if (unlikely(!vport)) goto error; /*接口禁止了lro相关，因此skb需要lro相关则报错退出*/ if (unlikely(skb_warn_if_lro(skb))) goto error; /*user不唯一则进行clone*/ skb = skb_share_check(skb, GFP_ATOMIC); if (unlikely(!skb)) return; /*恢复二层头，下面用得到*/ skb_push(skb, ETH_HLEN); /*重新计算校验和*/ skb_postpush_rcsum(skb, skb-&gt;data, ETH_HLEN); ovs_vport_receive(vport, skb, tun_info); return;error: kfree_skb(skb);&#125; ovs_vport_receive 1234567891011121314151617181920212223242526272829303132int ovs_vport_receive(struct vport *vport, struct sk_buff *skb, const struct ip_tunnel_info *tun_info)&#123; struct sw_flow_key key; int error; /*设置ovs私有数据*/ OVS_CB(skb)-&gt;input_vport = vport; OVS_CB(skb)-&gt;mru = 0; OVS_CB(skb)-&gt;cutlen = 0; /*判断是否属于同一个网络空间；可参考 openstack 网络架构 */ if (unlikely(dev_net(skb-&gt;dev) != ovs_dp_get_net(vport-&gt;dp))) &#123; u32 mark; mark = skb-&gt;mark; skb_scrub_packet(skb, true); skb-&gt;mark = mark; tun_info = NULL; &#125; /*初始化ovs内部协议号*/ ovs_skb_init_inner_protocol(skb); skb_clear_ovs_gso_cb(skb); /*此函数会解析skb内容，并给key中字段赋值*/ /*注意 input_vport-&gt;port_no 为`ovs-dpctl show`显示的port number*/ error = ovs_flow_key_extract(tun_info, skb, &amp;key); if (unlikely(error)) &#123; kfree_skb(skb); return error; &#125; //内核匹配流表路径，没有则上送。 ovs_dp_process_packet(skb, &amp;key); return 0;&#125; ovs_dp_process_packet 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void ovs_dp_process_packet(struct sk_buff *skb, struct sw_flow_key *key)&#123; const struct vport *p = OVS_CB(skb)-&gt;input_vport; struct datapath *dp = p-&gt;dp; struct sw_flow *flow; struct sw_flow_actions *sf_acts; struct dp_stats_percpu *stats; u64 *stats_counter; u32 n_mask_hit; stats = this_cpu_ptr(dp-&gt;stats_percpu); /*根据key找flow表，没有的话进行upcall； 此处找的是内核流表，可用`ovs-dpctl dump-flows [dp]`查看。 本文暂不对这些功能函数进行具体分析*/ flow = ovs_flow_tbl_lookup_stats(&amp;dp-&gt;table, key, skb_get_hash(skb), &amp;n_mask_hit); if (unlikely(!flow)) &#123; struct dp_upcall_info upcall; int error; memset(&amp;upcall, 0, sizeof(upcall)); upcall.cmd = OVS_PACKET_CMD_MISS; upcall.portid = ovs_vport_find_upcall_portid(p, skb); upcall.mru = OVS_CB(skb)-&gt;mru; error = ovs_dp_upcall(dp, skb, key, &amp;upcall, 0); if (unlikely(error)) kfree_skb(skb); else consume_skb(skb); stats_counter = &amp;stats-&gt;n_missed; goto out; &#125; /*flow填充到skb私有数据，并执行action*/ ovs_flow_stats_update(flow, key-&gt;tp.flags, skb); sf_acts = rcu_dereference(flow-&gt;sf_acts); ovs_execute_actions(dp, skb, sf_acts, key); stats_counter = &amp;stats-&gt;n_hit;out: /* Update datapath statistics. */ u64_stats_update_begin(&amp;stats-&gt;syncp); (*stats_counter)++; stats-&gt;n_mask_hit += n_mask_hit; u64_stats_update_end(&amp;stats-&gt;syncp);&#125; ovs_execute_actions 1234567891011121314151617181920/* Execute a list of actions against 'skb'. */int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb, const struct sw_flow_actions *acts, struct sw_flow_key *key)&#123; int err, level; /*单个CPU同时处理（准确的说，应该是排队，可能进程调度）4条报文*/ level = __this_cpu_inc_return(exec_actions_level); if (unlikely(level &gt; OVS_RECURSION_LIMIT)) &#123; net_crit_ratelimited("ovs: recursion limit reached on datapath %s, probable configuration error\n", ovs_dp_name(dp)); kfree_skb(skb); err = -ENETDOWN; goto out; &#125; //执行action err = do_execute_actions(dp, skb, key, acts-&gt;actions, acts-&gt;actions_len); …………&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* Execute a list of actions against 'skb'. */static int do_execute_actions(struct datapath *dp, struct sk_buff *skb, struct sw_flow_key *key, const struct nlattr *attr, int len)&#123; const struct nlattr *a; int rem; for (a = attr, rem = len; rem &gt; 0; a = nla_next(a, &amp;rem)) &#123; int err = 0; /*获取action type；nla*定义在内核 netlink.h文件中*/ switch (nla_type(a)) &#123; /*从某接口转发*/ case OVS_ACTION_ATTR_OUTPUT: &#123; //获取 out_port int port = nla_get_u32(a); struct sk_buff *clone; /* 每个输出操作都需要一个单独的'skb'克隆，如果输出操作是最后一个操作，则可以避免克隆。 */ if (nla_is_last(a, rem)) &#123; do_output(dp, skb, port, key); return 0; &#125; clone = skb_clone(skb, GFP_ATOMIC); if (clone) do_output(dp, clone, port, key); OVS_CB(skb)-&gt;cutlen = 0; break; &#125; //其他actions 这里不详细介绍 ………… &#125; if (unlikely(err)) &#123; kfree_skb(skb); return err; &#125; &#125; consume_skb(skb); return 0;&#125; do_output 12345678910111213141516static int do_output(struct datapath *dp, struct sk_buff *skb, int out_port)&#123; struct vport *vport; if (unlikely(!skb)) return -ENOMEM; vport = ovs_vport_rcu(dp, out_port); if (unlikely(!vport)) &#123; kfree_skb(skb); return -ENODEV; &#125; /*vport是flow找到的out_port的vport,因此，此处会调用vport的send函数*/ ovs_vport_send(vport, skb); return 0;&#125; 发包处理接口初始化注册的发包函数为’dev_queue_xmit’。 internal接口收包处理internal接口报文一般会从system类型接口传入，从system接口收包处理过程继续，internal类型接口定义的send函数为’internal_dev_recv’。 1234567891011121314151617181920212223242526272829static netdev_tx_t internal_dev_recv(struct sk_buff *skb)&#123; struct net_device *netdev = skb-&gt;dev; struct pcpu_sw_netstats *stats; /*接口没有管理UP，直接丢包*/ if (unlikely(!(netdev-&gt;flags &amp; IFF_UP))) &#123; kfree_skb(skb); netdev-&gt;stats.rx_dropped++; return NETDEV_TX_OK; &#125; /*初始化dst计数*/ skb_dst_drop(skb); /*更改某些数据（接口或会话）使用计数*/ nf_reset(skb); secpath_reset(skb); /*更改skb接入类型、协议*/ skb-&gt;pkt_type = PACKET_HOST; skb-&gt;protocol = eth_type_trans(skb, netdev); skb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN); stats = this_cpu_ptr(netdev-&gt;tstats); u64_stats_update_begin(&amp;stats-&gt;syncp); stats-&gt;rx_packets++; stats-&gt;rx_bytes += skb-&gt;len; u64_stats_update_end(&amp;stats-&gt;syncp); /*此函数以前的文章分析过，此处不在重复*/ netif_rx(skb); return NETDEV_TX_OK;&#125; 发包处理netif_rx函数重新进入了本机协议栈的处理，而internal类型的接口没有设置’rx_handler’，因此进入正常协议栈流程，最后会进入正常转发流程。 1dev_queue_xmit -&gt; dev_hard_start_xmit -&gt; ops-&gt;ndo_start_xmit(skb, dev); 而internal在接口创建的时候定义了 ndo_start_xmit。 1234567891011static struct vport *internal_dev_create(const struct vport_parms *parms)&#123; …… vport-&gt;dev = alloc_netdev(sizeof(struct internal_dev), parms-&gt;name, NET_NAME_USER, do_setup); if (!vport-&gt;dev) &#123; err = -ENOMEM; goto error_free_vport; &#125; ……&#125; 123456789101112131415161718static const struct net_device_ops internal_dev_netdev_ops = &#123; .ndo_open = internal_dev_open, .ndo_stop = internal_dev_stop, .ndo_start_xmit = internal_dev_xmit, .ndo_set_mac_address = eth_mac_addr, .ndo_change_mtu = internal_dev_change_mtu, .ndo_get_stats64 = internal_get_stats,#ifdef HAVE_IFF_PHONY_HEADROOM .ndo_set_rx_headroom = internal_set_rx_headroom,#endif&#125;;static void do_setup(struct net_device *netdev)&#123; …… netdev-&gt;netdev_ops = &amp;internal_dev_netdev_ops; ……&#125; 继续走读函数 123456789static int internal_dev_xmit(struct sk_buff *skb, struct net_device *netdev)&#123; rcu_read_lock(); /*函数前面已经解析*/ ovs_vport_receive(internal_dev_priv(netdev)-&gt;vport, skb); rcu_read_unlock(); …… return 0;&#125; vxlan接口vxlan接口的 收包处理(netdev_frame_hook) 和 发包处理(vxlan_xmit)，在以前的文章已经分析过。 优秀资料OVS中端口数据包收发流程 Openvswitch原理与代码分析(1)：总体架构]]></content>
      <categories>
        <category>openvswitch</category>
      </categories>
      <tags>
        <tag>ovs-patch</tag>
        <tag>ovs-internal</tag>
        <tag>ovs-system</tag>
        <tag>ovs-vxlan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux对vxlan的支持]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux%E5%AF%B9vxlan%E7%9A%84%E6%94%AF%E6%8C%81.html</url>
    <content type="text"><![CDATA[此文基于内核 3.10.105 分析linux内核中vxlan接口流量路径。主要是对报文流程内函数进行分析，所以代码居多。 基本报文流程回顾已经知道vxlan是 MAC IN UDP中的封装，因此，在解封装之前，一切按照原有流程走，在此复习一下（驱动层的数据处理这次不再解析，直接从__netif_receive_skb_core开始）： __netif_receive_skb_core 12345678910111213/*type，二层封装内的协议，IP为 0x0800*/type = skb-&gt;protocol;/*获取协议注册的入口函数，ip为 ip_rcv，声明的变量为 ip_packet_type*/list_for_each_entry_rcu(ptype, &amp;ptype_base[ntohs(type) &amp; PTYPE_HASH_MASK], list) &#123; if (ptype-&gt;type == type &amp;&amp; (ptype-&gt;dev == null_or_dev || ptype-&gt;dev == skb-&gt;dev || ptype-&gt;dev == orig_dev)) &#123; if (pt_prev) ret = deliver_skb(skb, pt_prev, orig_dev); pt_prev = ptype; &#125;&#125; ip_rcv 此函数只是对报文进行可靠性验证，最后到 钩子函数 ‘NF_HOOK’。 钩子函数中就是配置的netfilter，通过验证就会直接进入函数 ‘ip_rcv_finish’。 ip_rcv_finish 12345678910111213/*sysctl_ip_early_demux 是二进制值，该值用于对发往本地数据包的优化。当前仅对建立连接的套接字起作用。*/if (sysctl_ip_early_demux &amp;&amp; !skb_dst(skb) &amp;&amp; skb-&gt;sk == NULL) &#123; const struct net_protocol *ipprot; int protocol = iph-&gt;protocol; ipprot = rcu_dereference(inet_protos[protocol]); if (ipprot &amp;&amp; ipprot-&gt;early_demux) &#123; ipprot-&gt;early_demux(skb); /* must reload iph, skb-&gt;head might have changed */ iph = ip_hdr(skb); &#125; &#125; 123456789101112131415/*这一部分时查找路由，判断是local in还是 forwarding。本次分析按照 local in分析*/if (!skb_dst(skb)) &#123; int err = ip_route_input_noref(skb, iph-&gt;daddr, iph-&gt;saddr, iph-&gt;tos, skb-&gt;dev); if (unlikely(err)) &#123; if (err == -EXDEV) NET_INC_STATS_BH(dev_net(skb-&gt;dev), LINUX_MIB_IPRPFILTER); goto drop; &#125; &#125;…… /*按照local in分析，则此处相当于调用 ip_local_deliver （可深入 查找路由函数，里面有函数指针赋值）*/ return dst_input(skb); ip_local_deliver 钩子函数检测，不深入，直接到最后。 ip_local_deliver_finish 1234ipprot = rcu_dereference(inet_protos[protocol]);…… ret = ipprot-&gt;handler(skb);…… 到了传输层注册的入口函数。UDP入口函数为 ‘udp_rcv’。 __udp4_lib_rcv 123456/*根据源目端口号及IP查找 插口*/sk = __udp4_lib_lookup_skb(skb, uh-&gt;source, uh-&gt;dest, udptable);…… /*进入udp队列收包流程*/ int ret = udp_queue_rcv_skb(sk, skb);…… udp_queue_rcv_skb 12345678910111213/*插口如果是封装类型，vxlan等，则进入封装处理入口，下面开始分析vxlan部分代码*/encap_rcv = ACCESS_ONCE(up-&gt;encap_rcv);if (skb-&gt;len &gt; sizeof(struct udphdr) &amp;&amp; encap_rcv != NULL) &#123; int ret; ret = encap_rcv(sk, skb); if (ret &lt;= 0) &#123; UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INDATAGRAMS, is_udplite); return -ret; &#125;&#125; vxlan 模块初始化 vxlan_init_module 1rc = register_pernet_device(&amp;vxlan_net_ops); 123456static struct pernet_operations vxlan_net_ops = &#123; .init = vxlan_init_net, .exit = vxlan_exit_net, .id = &amp;vxlan_net_id, .size = sizeof(struct vxlan_net),&#125;; register_pernet_device内部执行init函数。此处主要看收包注册函数。 123456/* Disable multicast loopback */inet_sk(sk)-&gt;mc_loop = 0;/* Mark socket as an encapsulation socket. */udp_sk(sk)-&gt;encap_type = 1;udp_sk(sk)-&gt;encap_rcv = vxlan_udp_encap_recv; vxlan模块收包函数 vxlan_udp_encap_recv 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102 static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb) &#123;struct iphdr *oip;struct vxlanhdr *vxh;struct vxlan_dev *vxlan;struct pcpu_tstats *stats;__u32 vni;int err;/*去掉UDP头*/__skb_pull(skb, sizeof(struct udphdr));/* 判断是否有 vxlan 头*/if (!pskb_may_pull(skb, sizeof(struct vxlanhdr))) goto error;/*如果flag不是vxlan flag表示不是vxlan 封装或vx_vni后8位有值，也表示错误，因为之后会 右移8位，所以低8位完全没有意义*/vxh = (struct vxlanhdr *) skb-&gt;data;if (vxh-&gt;vx_flags != htonl(VXLAN_FLAGS) || (vxh-&gt;vx_vni &amp; htonl(0xff))) &#123; netdev_dbg(skb-&gt;dev, "invalid vxlan flags=%#x vni=%#x\n", ntohl(vxh-&gt;vx_flags), ntohl(vxh-&gt;vx_vni)); goto error;&#125; /*去掉vxlan 头*/__skb_pull(skb, sizeof(struct vxlanhdr));/* 本地 未定义 vni 则丢包*/vni = ntohl(vxh-&gt;vx_vni) &gt;&gt; 8;vxlan = vxlan_find_vni(sock_net(sk), vni);if (!vxlan) &#123; netdev_dbg(skb-&gt;dev, "unknown vni %d\n", vni); goto drop;&#125; /*解析original l2 frame，没有l2头则丢包*/if (!pskb_may_pull(skb, ETH_HLEN)) &#123; vxlan-&gt;dev-&gt;stats.rx_length_errors++; vxlan-&gt;dev-&gt;stats.rx_errors++; goto drop;&#125;/*重置skb mac数据，因为解包了*/skb_reset_mac_header(skb);/* Re-examine inner Ethernet packet */oip = ip_hdr(skb);skb-&gt;protocol = eth_type_trans(skb, vxlan-&gt;dev);if (compare_ether_addr(eth_hdr(skb)-&gt;h_source, vxlan-&gt;dev-&gt;dev_addr) == 0) goto drop; /*学习报文记录到本地 vxlan fdb表*/if ((vxlan-&gt;flags &amp; VXLAN_F_LEARN) &amp;&amp; vxlan_snoop(skb-&gt;dev, oip-&gt;saddr, eth_hdr(skb)-&gt;h_source)) goto drop; /*更改skb收包接口*/__skb_tunnel_rx(skb, vxlan-&gt;dev);skb_reset_network_header(skb);if (skb-&gt;ip_summed != CHECKSUM_UNNECESSARY || !skb-&gt;encapsulation || !(vxlan-&gt;dev-&gt;features &amp; NETIF_F_RXCSUM)) skb-&gt;ip_summed = CHECKSUM_NONE;skb-&gt;encapsulation = 0;err = IP_ECN_decapsulate(oip, skb);if (unlikely(err)) &#123; if (log_ecn_error) net_info_ratelimited("non-ECT from %pI4 with TOS=%#x\n", &amp;oip-&gt;saddr, oip-&gt;tos); if (err &gt; 1) &#123; ++vxlan-&gt;dev-&gt;stats.rx_frame_errors; ++vxlan-&gt;dev-&gt;stats.rx_errors; goto drop; &#125;&#125;stats = this_cpu_ptr(vxlan-&gt;dev-&gt;tstats);u64_stats_update_begin(&amp;stats-&gt;syncp);stats-&gt;rx_packets++;stats-&gt;rx_bytes += skb-&gt;len;u64_stats_update_end(&amp;stats-&gt;syncp); /*skb处理完成，进入主要函数*/netif_rx(skb);return 0; error:/* Put UDP header back */__skb_push(skb, sizeof(struct udphdr));return 1; drop:/* Consume bad packet */kfree_skb(skb);return 0; &#125; netif_rx 1234567891011121314151617181920212223242526272829303132333435363738int netif_rx(struct sk_buff *skb)&#123; int ret; /* netpoll开启，走netpoll流程*/ if (netpoll_rx(skb)) return NET_RX_DROP; /*时间戳检查*/ net_timestamp_check(netdev_tstamp_prequeue, skb); trace_netif_rx(skb); /*内核开启了RPS则进入此流程，关于RPS，以后写文章详细介绍*/#ifdef CONFIG_RPS if (static_key_false(&amp;rps_needed)) &#123; struct rps_dev_flow voidflow, *rflow = &amp;voidflow; int cpu; preempt_disable(); rcu_read_lock(); cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow); if (cpu &lt; 0) cpu = smp_processor_id(); ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail); rcu_read_unlock(); preempt_enable(); &#125; else#endif &#123; unsigned int qtail; /*将包挂到某个cpu的处理列表中*/ ret = enqueue_to_backlog(skb, get_cpu(), &amp;qtail); put_cpu(); &#125; return ret;&#125; 那么，每个CPU在什么地方进行处理呢？ 在网络子系统的初始化函数中’net_dev_init’有一段代码，如下 12345678910111213141516171819202122for_each_possible_cpu(i) &#123; struct softnet_data *sd = &amp;per_cpu(softnet_data, i); memset(sd, 0, sizeof(*sd)); skb_queue_head_init(&amp;sd-&gt;input_pkt_queue); skb_queue_head_init(&amp;sd-&gt;process_queue); sd-&gt;completion_queue = NULL; INIT_LIST_HEAD(&amp;sd-&gt;poll_list); sd-&gt;output_queue = NULL; sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue;#ifdef CONFIG_RPS sd-&gt;csd.func = rps_trigger_softirq; sd-&gt;csd.info = sd; sd-&gt;csd.flags = 0; sd-&gt;cpu = i;#endif sd-&gt;backlog.poll = process_backlog; sd-&gt;backlog.weight = weight_p; sd-&gt;backlog.gro_list = NULL; sd-&gt;backlog.gro_count = 0; &#125; 可以看到，其处理函数为 process_backlog。 process_backlog 部分代码如下 1234567891011121314while ((skb = __skb_dequeue(&amp;sd-&gt;process_queue))) &#123; rcu_read_lock(); local_irq_enable(); /*队列内报文走一遍 __netif_receive_skb 函数； 对于vxlan来说，此时的报文已经是内部 original l2 frame*/ __netif_receive_skb(skb); rcu_read_unlock(); local_irq_disable(); input_queue_head_incr(sd); if (++work &gt;= quota) &#123; local_irq_enable(); return work; &#125; &#125; vxlan模块发包函数vxlan模块初始化函数进行了link初始化，其操作定义如下 123456789101112static struct rtnl_link_ops vxlan_link_ops __read_mostly = &#123; .kind = "vxlan", .maxtype = IFLA_VXLAN_MAX, .policy = vxlan_policy, .priv_size = sizeof(struct vxlan_dev), .setup = vxlan_setup, .validate = vxlan_validate, .newlink = vxlan_newlink, .dellink = vxlan_dellink, .get_size = vxlan_get_size, .fill_info = vxlan_fill_info,&#125;; 其内接口的setup操作函数为vxlan_setup，内部对于接口的操作初始化如下 12345678910111213141516dev-&gt;netdev_ops = &amp;vxlan_netdev_ops;--&gt;&gt;static const struct net_device_ops vxlan_netdev_ops = &#123; .ndo_init = vxlan_init, .ndo_open = vxlan_open, .ndo_stop = vxlan_stop, .ndo_start_xmit = vxlan_xmit, .ndo_get_stats64 = ip_tunnel_get_stats64, .ndo_set_rx_mode = vxlan_set_multicast_list, .ndo_change_mtu = eth_change_mtu, .ndo_validate_addr = eth_validate_addr, .ndo_set_mac_address = eth_mac_addr, .ndo_fdb_add = vxlan_fdb_add, .ndo_fdb_del = vxlan_fdb_delete, .ndo_fdb_dump = vxlan_fdb_dump,&#125;; 可知发包函数为 ‘vxlan_xmit’。 vxlan_xmit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/* 此函数主要是找 远端 IP及MAC，封装包的函数为 vxlan_xmit_one*/static netdev_tx_t vxlan_xmit(struct sk_buff *skb, struct net_device *dev)&#123; struct vxlan_dev *vxlan = netdev_priv(dev); struct ethhdr *eth; bool did_rsc = false; struct vxlan_rdst *rdst0, *rdst; struct vxlan_fdb *f; int rc1, rc; skb_reset_mac_header(skb); eth = eth_hdr(skb); if ((vxlan-&gt;flags &amp; VXLAN_F_PROXY) &amp;&amp; ntohs(eth-&gt;h_proto) == ETH_P_ARP) return arp_reduce(dev, skb); f = vxlan_find_mac(vxlan, eth-&gt;h_dest); did_rsc = false; if (f &amp;&amp; (f-&gt;flags &amp; NTF_ROUTER) &amp;&amp; (vxlan-&gt;flags &amp; VXLAN_F_RSC) &amp;&amp; ntohs(eth-&gt;h_proto) == ETH_P_IP) &#123; did_rsc = route_shortcircuit(dev, skb); if (did_rsc) f = vxlan_find_mac(vxlan, eth-&gt;h_dest); &#125; if (f == NULL) &#123; rdst0 = &amp;vxlan-&gt;default_dst; if (rdst0-&gt;remote_ip == htonl(INADDR_ANY) &amp;&amp; (vxlan-&gt;flags &amp; VXLAN_F_L2MISS) &amp;&amp; !is_multicast_ether_addr(eth-&gt;h_dest)) vxlan_fdb_miss(vxlan, eth-&gt;h_dest); &#125; else rdst0 = &amp;f-&gt;remote; rc = NETDEV_TX_OK; /* if there are multiple destinations, send copies */ for (rdst = rdst0-&gt;remote_next; rdst; rdst = rdst-&gt;remote_next) &#123; struct sk_buff *skb1; skb1 = skb_clone(skb, GFP_ATOMIC); if (skb1) &#123; rc1 = vxlan_xmit_one(skb1, dev, rdst, did_rsc); if (rc == NETDEV_TX_OK) rc = rc1; &#125; &#125; rc1 = vxlan_xmit_one(skb, dev, rdst0, did_rsc); if (rc == NETDEV_TX_OK) rc = rc1; return rc;&#125; vxlan_xmit_one 封装报文，具体内容可自己分析。 vxlan实例实验环境： 1234567891011121314151617# vm1[root@test-1 ~]# uname -srLinux 3.10.0-693.el7.x86_64[root@test-1 ~]# ip addr show dev eth0 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:0c:e3:c6 brd ff:ff:ff:ff:ff:ff inet 10.10.10.11/24 brd 10.10.10.255 scope global dynamic eth0 valid_lft 84967sec preferred_lft 84967sec # vm2[root@test-2 ~]# uname -srLinux 3.10.0-693.el7.x86_64[root@test-2 ~]# ip addr show dev eth0 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:50:61:0d brd ff:ff:ff:ff:ff:ff inet 10.10.10.9/24 brd 10.10.10.255 scope global dynamic eth0 valid_lft 84929sec preferred_lft 84929sec 点对点的vxlan完成一个最简单的vxlan网络，拓扑如下： 开始配置 1[root@test-1 ~]# ip link add vxlan1 type vxlan id 1 dstport 4789 remote 10.10.10.9 local 10.10.10.11 dev eth0 vxlan1 即为创建的接口名称 ，type 为 vxlan 类型。 id 即为 VNI。 dstport 指定 UDP的目的端口，IANA 为vxlan分配的目的端口是 4789。 remote 和 local ，即远端和本地的IP地址，因为vxlan是MAC-IN-UDP，需要指定外层IP，此处即指。 dev 本地流量接口。用于 vtep 通信的网卡设备，用来读取 IP 地址。注意这个参数和 local参数含义是相同的，在这里写出来是为了告诉大家有两个参数存在。 创建完成可看到 123[root@test-1 ~]# ip addr show type vxlan3: vxlan1: &lt;BROADCAST,MULTICAST&gt; mtu 1396 qdisc noop state DOWN qlen 1000 link/ether 86:65:ef:3d:a1:e7 brd ff:ff:ff:ff:ff:ff 接口现在还没有地址，也没有开启，接下来进行如下配置 1234567[root@test-1 ~]# ip addr add 192.168.1.3/24 dev vxlan1[root@test-1 ~]# ip link set vxlan1 up[root@test-1 ~]# ip addr show type vxlan3: vxlan1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1396 qdisc noqueue state UNKNOWN qlen 1000 link/ether 86:65:ef:3d:a1:e7 brd ff:ff:ff:ff:ff:ff inet 192.168.1.3/24 scope global vxlan1 valid_lft forever preferred_lft forever vxlan1已经配置完成。 123为什么vxlan1的MTU是 1396呢？因为我的vm启在openstack云环境中，租户网络为vxlan，外网为vlan类型。因此创建的内网网络MTU为 1446(1500 - 4(vlan) - 50(vxlan))，在vm中基于eth0又配置vxlan，因此MTU需要再减50，即 1396。 之后看一下路由表即vxlan 的 FDB表 123456#(仅列出vxlan部分)[root@test-1 ~]# ip route 192.168.1.0/24 dev vxlan1 proto kernel scope link src 192.168.1.3 [root@test-1 ~]# bridge fdb00:00:00:00:00:00 dev vxlan1 dst 10.10.10.9 via eth0 self permanent# 即默认vxlan1 对端地址为 10.10.10.9，通过eth0进行报文交换 对test-2进行同样的配置，保证VNI和dstport一致。VNI一致是为了不进行vxlan隔离，dstport一致是因为IANA 为vxlan分配的目的端口是 4789。 之后在test-1上进行测试连通性 123456789[root@test-1 ~]# ping 192.168.1.4 -c 3PING 192.168.1.4 (192.168.1.4) 56(84) bytes of data.64 bytes from 192.168.1.4: icmp_seq=1 ttl=64 time=0.424 ms64 bytes from 192.168.1.4: icmp_seq=2 ttl=64 time=0.437 ms64 bytes from 192.168.1.4: icmp_seq=3 ttl=64 time=0.404 ms--- 192.168.1.4 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2000msrtt min/avg/max/mdev = 0.404/0.421/0.437/0.027 ms 在对端抓取eth0报文如下 组播模式vxlan要组成同一个 vxlan 网络，vtep 必须能感知到彼此的存在。多播组本来的功能就是把网络中的某些节点组成一个虚拟的组，所以 vxlan 最初想到用多播来实现是很自然的事情。拓扑如下 1234567[root@test-1 ~]# ip link add vxlan1 type vxlan id 2 dstport 4789 group 224.0.0.1 dev eth0 [root@test-1 ~]# ip addr add 192.168.1.3/24 dev vxlan1[root@test-1 ~]# ip link set vxlan1 up[root@test-1 ~]# ip route 192.168.1.0/24 dev vxlan1 proto kernel scope link src 192.168.1.3 [root@test-1 ~]# bridge fdb00:00:00:00:00:00 dev vxlan1 dst 224.0.0.1 via eth0 self permanent 这里最重要的参数是 group 224.0.0.1 （多播地址范围为224.0.0.0到239.255.255.255）表示把 vtep 加入到这个多播组。其他设备进行类似配置，确保VNI与group相同。 由于将vxlan1加入多播组，因此ARP请求这类报文不在进行广播，而是多播。 可以与点对点形式的做对比 ARP回应依然是单播报文。通信结束之后，查看ARP及fdb表项为 12345[root@test-1 ~]# ip neigh192.168.1.4 dev vxlan1 lladdr a6:8b:d5:2d:83:3c STALE[root@test-1 ~]# bridge fdb00:00:00:00:00:00 dev vxlan1 dst 224.0.0.1 via eth0 self permanenta6:8b:d5:2d:83:3c dev vxlan1 dst 10.10.10.9 self 利用 bridge 来接入容器此处的bridge可以是linux的bridge，也可以是ovs的bridge，为了配置简单化，此处以linux举例。 上面两种拓扑将vxlan作为三层口，在云环境中基本没什么意义。在实际的生产中，每台主机上都有几十台甚至上百台的虚拟机或者容器需要通信，因此我们需要找到一种方法能够把这些通信实体组织起来——这正是引入桥的意义。 下面用 network namespace来模拟tap(vm接口)，进行如下配置 123456789101112131415161718192021222324252627282930313233#创建vxlan接口[root@test-1 ~]# ip link add vxlan1 type vxlan id 2 dstport 4789 group 224.0.0.6 dev eth0 #创建NS[root@test-1 ~]# ip netns add vm0[root@test-1 ~]# ip netns exec vm0 ip link set dev lo up#创建veth 接口，并将其中一个veth口放到 NS中[root@test-1 ~]# ip link add veth0 type veth peer name veth1[root@test-1 ~]# ip link set veth0 netns vm0[root@test-1 ~]# ip netns exec vm0 ip link set veth0 name eth0[root@test-1 ~]# ip netns exec vm0 ip addr add 192.168.1.3/24 dev eth0[root@test-1 ~]# ip netns exec vm0 ip link set dev eth0 up[root@test-1 ~]# ip netns exec vm0 ip addr show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever8: eth0@if7: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN qlen 1000 link/ether 9e:ab:18:d5:25:55 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.1.3/24 scope global eth0 valid_lft forever preferred_lft forever#创建桥，并将vxlan、veth加入桥[root@test-1 ~]# ip link add br0 type bridge[root@test-1 ~]# ip link set vxlan1 master br0[root@test-1 ~]# ip link set vxlan1 up[root@test-1 ~]# [root@test-1 ~]# ip link set dev veth1 master br0[root@test-1 ~]# ip link set dev veth1 up[root@test-1 ~]# ip link set dev br0 up[root@test-1 ~]# bridge link6: vxlan1 state UNKNOWN : &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1396 master br0 state forwarding priority 32 cost 100 7: veth1 state UP @(null): &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master br0 state forwarding priority 32 cost 2 其他设备进行类似配置（设备较多可用脚本实现）。利用命令 ip netns exec vm0 ping IP来验证连通性。 此过程为： NS eth0 ARP广播，通过veth1到达br0。 br0上没有二层表，进行学习之后，每个口（除收报文口）进行转发，到达vxlan1。 vxlan1口fdb表默认走多播，进行外层封装之后从eth0接口发出。 手动维护 vtep 组对 overlay 网络来说，它的网段范围是分布在多个主机上的，因此传统 ARP 报文的广播无法直接使用。要想做到 overlay 网络的广播，必须把报文发送到所有 vtep 在的节点，这才引入了多播。 如果提前知道哪些 vtep 要组成一个网络，以及这些 vtep 在哪些主机上，那么就可以不使用多播。 Linux 的 vxlan 模块也提供了这个功能，而且实现起来并不复杂。创建 vtep interface 的时候不使用 remote 或者 group 参数就行： 123[root@test-1 ~]# ip link add vxlan1 type vxlan id 2 dstport 4789 dev eth0 [root@test-1 ~]# bridge fdb01:00:5e:00:00:01 dev eth0 self permanent 由于没有指定 remote 和 group ，因此进行广播时不知道要发送给谁。但是可以手动添加默认的 FDB 表项 1234#环境只有两台设备，因此只添加一条表项[root@test-1 ~]# bridge fdb append 00:00:00:00:00:00 dev vxlan1 dst 10.10.10.9[root@test-1 ~]# bridge fdb00:00:00:00:00:00 dev vxlan1 dst 10.10.10.9 self permanent 这种方式是手动维护多播组，解决了在某些 underlay 网络中不能使用多播的问题。但是由于需要手动维护，比较不方便，当然，可以编写脚本自动维护。 除此之外，这种方式也没有解决多播的另外一个问题：每次要查找 MAC 地址要发送大量的无用报文，如果 vtep 组节点数量很大，那么每次查询都发送 N 个报文，其中只有一个报文真正有用。 这个问题的解决可以参考此种方式，添加单播fdb表项！ 手动维护 fdb 表如果提前知道目的容器（或vm） MAC 地址和它所在主机的 IP 地址，也可以通过更新 fdb 表项来减少广播的报文数量。 1[root@test-1 ~]# ip link add vxlan1 type vxlan id 2 dstport 4789 dev eth0 nolearning nolearning 参数表示 vtep 不通过收到的报文来学习 fdb 表项的内容而是管理员维护。 12[root@test-1 ~]# bridge fdb append 00:00:00:00:00:00 dev vxlan1 dst 10.10.10.9[root@test-1 ~]# bridge fdb append 4e:e9:90:34:2e:b5 dev vxlan1 dst 10.10.10.9 第一条是默认表项； 第二条是明确的IP-MAC对应关系，注意：MAC是容器（vm）端口的MAC地址，而IP是NVE(Network Virtual Endpoint)的IP，即MAC是overlay的MAC，IP是underlay的IP。 不过此种方法只是把fdb进行手动维护，ARP广播没有任何改进，不过如果确定环境，那么可以手动维护ARP表项。 手动维护 ARP 表如果能通过某个方式知道容器的 IP 和 MAC 地址对应关系，只要更新到每个节点，就能实现网络的连通。 但是，需要维护的是每个容器里面的 ARP 表项，因为最终通信的双方是容器。到每个容器里面（所有的 network namespace）去更新对应的 ARP 表，是件工作量很大的事情，而且容器的创建和删除还是动态的。linux 提供了一个解决方案，vtep 可以作为 arp 代理，回复 arp 请求，也就是说只要 vtep interface 知道对应的 IP - MAC 关系，在接收到容器发来的 ARP 请求时可以直接作出应答。这样的话，我们只需要更新 vtep interface 上 ARP 表项就行了。 1[root@test-1 ~]# ip link add vxlan1 type vxlan id 2 dstport 4789 dev eth0 nolearning proxy proxy表示 vtep 承担ARP代理的功能。手动添加表项 1234[root@test-1 ~]# bridge fdb append 00:00:00:00:00:00 dev vxlan1 dst 10.10.10.9[root@test-1 ~]# bridge fdb append 4e:e9:90:34:2e:b5 dev vxlan1 dst 10.10.10.9# 添加 IP-MAC对应关系[root@test-1 ~]# ip neigh add 192.168.1.4 lladdr 4e:e9:90:34:2e:b5 dev vxlan0 优秀资料linux 上实现 vxlan 网络]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>VXLAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux隧道之VXLAN]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux%E9%9A%A7%E9%81%93%E4%B9%8BVXLAN.html</url>
    <content type="text"><![CDATA[起因任何技术的产生，都有其特定的时代背景与实际需求，VXLAN正是为了解决云计算时代虚拟化中的一系列问题而产生的一项技术。 云计算平台中，服务器虚拟化技术的广泛部署，极大地增加了数据中心的计算密度；同时，为了实现业务的灵活变更，虚拟机VM（Virtual Machine）需要能够在网络中不受限迁移。 然而，虚拟机数量的快速增长与虚拟机迁移业务的日趋频繁，给传统的“二层+三层”数据中心网络带来了新的挑战： 虚拟机规模受网络设备表项规格的限制 对于同网段主机的通信而言，报文通过查询MAC表进行二层转发。服务器虚拟化后，数据中心中VM的数量比原有的物理机发生了数量级的增长，伴随而来的便是虚拟机网卡MAC地址数量的空前增加。 传统网络的隔离能力有限 VLAN作为当前主流的网络隔离技术，在标准定义中只有12比特，也就是说可用的VLAN数量只有4000个左右。对于公有云或其它大型虚拟化云计算服务这种动辄上万甚至更多租户的场景而言，VLAN的隔离能力显然已经力不从心。 虚拟机迁移范围受限 虚拟机迁移，顾名思义，就是将虚拟机从一个物理机迁移到另一个物理机，但是要求在迁移过程中业务不能中断。要做到这一点，需要保证虚拟机迁移前后，其IP地址、MAC地址等参数维持不变。这就决定了，虚拟机迁移必须发生在一个二层域中。而传统数据中心网络的二层域，将虚拟机迁移限制在了一个较小的局部范围内。 原理VXLAN（Virtual eXtensible Local Area Network，虚拟扩展局域网），是由IETF定义的NVO3（Network Virtualization over Layer 3）标准技术之一，采用L2 over L4（MAC-in-UDP）的报文封装模式，将二层报文用三层协议进行封装，可实现二层网络在三层范围内进行扩展，同时满足数据中心大二层虚拟迁移和多租户的需求。 12NVO3是基于三层IP overlay网络构建虚拟网络的技术的统称，VXLAN只是NVO3技术之一。除此之外，比较有代表性的还有NVGRE、STT。 报文格式在分析报文格式之前，先明确几个概念： 概念 描述 NVE-Network Virtual Endpoint NVE是实现网络虚拟化功能的网络实体。报文经过NVE封装转换后，NVE间就可基于三层基础网络建立二层虚拟化网络。 VTEP-VXLAN Tunnel Endpoints VTEP是VXLAN隧道端点，封装在NVE中，用于VXLAN报文的封装和解封装。 VNI-VXLAN Network Identifier 类似VLAN ID，用于区分VXLAN段，不同VXLAN段的虚拟机不能直接二层相互通信。 关系如下 NVE直接传输的VXLAN报文格式如下 如图所示，原始报文（从vm发出的报文）经过了4层包装。由内而外依次为 VXLAN Header VXLAN头（8字节），其中包含24比特的VNI字段，用来定义VXLAN网络中不同的租户。此外，还包含VXLAN Flags（8比特，取值为00001000）和两个保留字段（分别为24比特和8比特）。 UDP Header UDP头（8字节），VXLAN头和原始以太帧一起作为UDP的数据。UDP头中，目的端口号（VXLAN Port）固定为4789，源端口号（UDP Src. Port）是原始以太帧通过哈希算法计算后的值。 Outer IP Header 封装外层IP头（20字节）。其中，源IP地址（Outer Src. IP）为源VM所属VTEP的IP地址，目的IP地址（Outer Dst. IP）为目的VM所属VTEP的IP地址。 Outer MAC Header 封装外层以太头（14字节），如果加上VLAN段的话应该是 18字节。其中，源MAC地址（Src. MAC Addr.）为源VM所属VTEP的MAC地址，目的MAC地址（Dst. MAC Addr.）为到达目的VTEP的路径上下一跳设备的MAC地址。 知道了其逻辑及报文格式，那么下面来看vxlan是如何进行报文转发的。 报文转发基本的二三层转发中，二层转发依赖的是MAC表，如果没有对应的MAC表，则主机发送ARP广播报文请求对端的MAC地址；三层转发依赖的是FIB表。在VXLAN中，其实也是同样的道理。 同子网互通 VM_A、VM_B和VM_C同属于10.1.1.0/24网段，且同属于VNI 5000。此时，VM_A想与VM_C进行通信。 由于是首次进行通信，VM_A上没有VM_C的MAC地址，所以会发送ARP广播报文请求VM_C的MAC地址。下面就让我们根据ARP请求报文及ARP应答报文的转发流程，来看下MAC地址是如何进行学习的。 ARP请求 VM_A发送源MAC为MAC_A、目的MAC为全F、源IP为IP_A、目的IP为IP_C的ARP广播报文，请求VM_C的MAC地址。 VTEP_1收到ARP请求后，根据二层子接口上的配置判断报文需要进入VXLAN隧道。确定了报文所属BD（Bridge Domain，类似VLAN的概念-虚拟局域网）后，也就确定了报文所属的VNI。同时，VTEP_1学习MAC_A、VNI和报文入接口（Port_1，即二层子接口对应的物理接口）的对应关系，并记录在本地MAC表中。之后，VTEP_1会根据头端复制列表对报文进行复制，并分别进行封装。 这里封装的外层源IP地址为本地VTEP（VTEP_1）的IP地址，外层目的IP地址为对端VTEP（VTEP_2和VTEP_3）的IP地址；外层源MAC地址为本地VTEP的MAC地址，而外层目的MAC地址为去往目的IP的网络中下一跳设备的MAC地址。 封装后的报文，根据外层MAC和IP信息，在IP网络中进行传输，直至到达对端VTEP。 报文到达VTEP_2和VTEP_3后，VTEP对报文进行解封装，得到VM_A发送的原始报文。同时，VTEP_2和VTEP_3学习VM_A的MAC地址、VNI和远端VTEP的IP地址（IP_1）的对应关系，并记录在本地MAC表中。之后，VTEP_2和VTEP_3根据二层子接口上的配置对报文进行相应的处理并在对应的二层域内广播。 VM_B和VM_C接收到ARP请求后，比较报文中的目的IP地址是否为本机的IP地址。VM_B发现目的IP不是本机IP，故将报文丢弃；VM_C发现目的IP是本机IP，则对ARP请求做出应答。 ARP应答 由于此时VM_C上已经学习到了VM_A的MAC地址，所以ARP应答报文为单播报文。报文源MAC为MAC_C，目的MAC为MAC_A，源IP为IP_C、目的IP为IP_A。 VTEP_3接收到VM_C发送的ARP应答报文后，识别报文所属的VNI（识别过程与步骤2类似）。同时，VTEP_3学习MAC_C、VNI和报文入接口（Port_3）的对应关系，并记录在本地MAC表中。之后，VTEP_3对报文进行封装。 可以看到，这里封装的外层源IP地址为本地VTEP（VTEP_3）的IP地址，外层目的IP地址为对端VTEP（VTEP_1）的IP地址；外层源MAC地址为本地VTEP的MAC地址，而外层目的MAC地址为去往目的IP的网络中下一跳设备的MAC地址。 封装后的报文，根据外层MAC和IP信息，在IP网络中进行传输，直至到达对端VTEP。 报文到达VTEP_1后，VTEP_1对报文进行解封装，得到VM_C发送的原始报文。同时，VTEP_1学习VM_C的MAC地址、VNI和远端VTEP的IP地址（IP_3）的对应关系，并记录在本地MAC表中。之后，VTEP_1将解封装后的报文发送给VM_A。 至此，VM_A和VM_C均已学习到了对方的MAC地址。之后，VM_A和VM_C将采用单播方式进行通信。 不同子网互通 VM_A和VM_B分别属于10.1.10.0/24网段和10.1.20.0/24网段，且分别属于VNI 5000和VNI 6000。VM_A和VM_B对应的三层网关分别是VTEP_3上BDIF 10和BDIF 20的IP地址。VTEP_3上存在到10.1.10.0/24网段和10.1.20.0/24网段的路由。此时，VM_A想与VM_B进行通信。 1BDIF接口的功能与VLANIF接口类似，是基于BD创建的三层逻辑接口，用以实现不同子网VM之间或VXLAN网络与非VXLAN网络之间的通信。 由于是首次进行通信，且VM_A和VM_B处于不同网段，VM_A需要先发送ARP广播报文请求网关（BDIF 10）的MAC，获得网关的MAC后，VM_A先将数据报文发送给网关；之后网关也将发送ARP广播报文请求VM_B的MAC，获得VM_B的MAC后，网关再将数据报文发送给VM_B。以上MAC地址学习的过程与同子网互通中MAC地址学习的流程一致，不再赘述。现在假设VM_A和VM_B均已学到网关的MAC、网关也已经学到VM_A和VM_B的MAC。 VM_A先将数据报文发送给网关。报文的源MAC为MAC_A，目的MAC为网关BDIF 10的MAC_10，源IP地址为IP_A，目的IP为IP_B。 VTEP_1收到数据报文后，识别此报文所属的VNI（VNI 5000），并根据MAC表项对报文进行封装。可以看到，这里封装的外层源IP地址为本地VTEP的IP地址（IP_1），外层目的IP地址为对端VTEP的IP地址（IP_3）；外层源MAC地址为本地VTEP的MAC地址（MAC_1），而外层目的MAC地址为去往目的IP的网络中下一跳设备的MAC地址。 封装后的报文，根据外层MAC和IP信息，在IP网络中进行传输，直至到达对端VTEP。 报文进入VTEP_3，VTEP_3对报文进行解封装，得到VM_A发送的原始报文。然后，VTEP_3会对报文做如下处理： VTEP_3发现该报文的目的MAC为本机BDIF 10接口的MAC，而目的IP地址为IP_B（10.1.20.1），所以会根据路由表查找到IP_B的下一跳。 发现下一跳为10.1.20.10，出接口为BDIF 20。此时VTEP_3查询ARP表项，并将原始报文的源MAC修改为BDIF 20接口的MAC（MAC_20），将目的MAC修改为VM_B的MAC（MAC_B）。 报文到BDIF 20接口时，识别到需要进入VXLAN隧道（VNI 6000），所以根据MAC表对报文进行封装。这里封装的外层源IP地址为本地VTEP的IP地址（IP_3），外层目的IP地址为对端VTEP的IP地址（IP_2）；外层源MAC地址为本地VTEP的MAC地址（MAC_3），而外层目的MAC地址为去往目的IP的网络中下一跳设备的MAC地址。 封装后的报文，根据外层MAC和IP信息，在IP网络中进行传输，直至到达对端VTEP。 报文到达VTEP_2后，VTEP_2对报文进行解封装，得到内层的数据报文，并将其发送给VM_B。 123VXLAN网络与非VXLAN网络之间的互通，也需要借助于三层网关。其实现不同点在于报文在VXLAN网络侧会进行封装，而在非VXLAN网络侧不需要进行封装。报文从VXLAN侧进入网关并解封装后，就按照普通的单播报文发送方式进行转发。 优秀链接Vxlan基础理解 Vxlan学习笔记——原理 VXLAN介绍 技术发烧友：认识VXLAN Neutron VxLAN + Linux Bridge 环境中的网络 MTU]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>VXLAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux隧道之GRE]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux%E9%9A%A7%E9%81%93%E4%B9%8BGRE.html</url>
    <content type="text"><![CDATA[协议简介GRE（Generic Routing Encapsulation，通用路由封装）协议是对某些网络层协议（如IP和IPX）的数据报文进行封装，使这些被封装的数据报文能够在另一个网络层协议（如IP）中传输。GRE采用了Tunnel（隧道）技术，是VPN（Virtual Private Network）的第三层隧道协议。 Tunnel是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在一个Tunnel的两端分别对数据报进行封装及解封装。 报文格式封装好的报文的形式如下 Payload可以是任何协议，GRE允许非IP协议在有效载荷中被传输。如果 Delivery header 是ipv4的头，则GRE被归入IP协议，协议号为47（Linux 支持的协议号）。 一个封装在IP Tunnel中的X协议报文的格式如下： 需要封装和传输的数据报文，称之为净荷（Payload），净荷的协议类型为乘客协议（Passenger Protocol）。系统收到一个净荷后，首先使用封装协议（Encapsulation Protocol）对这个净荷进行GRE封装，即把乘客协议报文进行了“包装”，加上了一个GRE头部成为GRE报文；然后再把封装好的原始报文和GRE头部封装在IP报文中，这样就可完全由IP层负责此报文的前向转发（Forwarding）。通常把这个负责前向转发的IP协议称为传输协议（Delivery Protocol或者Transport Protocol）。 根据传输协议的不同，可以分为GRE over IPv4和GRE over IPv6两种隧道模式。以IPV4为例分析封装、解封过程，拓扑如下 封装过程 Private A中发送报文，检查报文头中的目的地址域来确定如何发送此包； 若报文的目的地址要经过Tunnel才能到达，则设备将此报文发给相应的Tunnel接口； Tunnel口收到此报文后进行GRE封装，在封装IP报文头后，设备根据此IP包的目的地址及路由表对报文进行转发，从相应的网络接口发送出去。 解封过程 Private B从Tunnel接口收到IP报文，检查目的地址； 发现目的地是本路由器，则Router B剥掉此报文的IP报头，交给GRE协议处理（进行检验密钥、检查校验和及报文的序列号等）； GRE协议完成相应的处理后，剥掉GRE报头，再交由内部协议对此数据报进行后续的转发处理。 GRE的安全选项为了提高GRE隧道的安全性，GRE还支持由用户选择设置Tunnel接口的识别关键字（或称密钥），和对隧道封装的报文进行端到端校验。 在RFC1701中规定： 若GRE报文头中的Key标识位置1，则收发双方将进行通道识别关键字的验证，只有Tunnel两端设置的识别关键字完全一致时才能通过验证，否则将报文丢弃。 若GRE报文头中的Checksum标识位置1，则校验和有效。发送方将根据GRE头及Payload信息计算校验和，并将包含校验和的报文发送给对端。接收方对接收到的报文计算校验和，并与报文中的校验和比较，如果一致则对报文进一步处理，否则丢弃。 Linux对GRE的支持代码基于 kernel 3.10.105。 模块初始化12345678910111213141516static int __init gre_init(void)&#123; pr_info("GRE over IPv4 demultiplexor driver\n"); /*IP 内部协议号， 47*/ if (inet_add_protocol(&amp;net_gre_protocol, IPPROTO_GRE) &lt; 0) &#123; pr_err("can't add protocol\n"); return -EAGAIN; &#125; /*offload相关函数*/ if (inet_add_offload(&amp;gre_offload, IPPROTO_GRE)) &#123; pr_err("can't add protocol offload\n"); inet_del_protocol(&amp;net_gre_protocol, IPPROTO_GRE); return -EAGAIN; &#125; return 0;&#125; 而 net_gre_protocol 定义为 12345static const struct net_protocol net_gre_protocol = &#123; .handler = gre_rcv, .err_handler = gre_err, .netns_ok = 1,&#125;; 因此，在收到GRE报文并上送到本地后，在函数ip_local_deliver_finish中会调用函数gre_rcv。 收包12TIPS:接口创建函数 ip_tunnel_create。 gre_rcv12345678910111213141516171819202122232425262728static int gre_rcv(struct sk_buff *skb)&#123; const struct gre_protocol *proto; u8 ver; int ret; if (!pskb_may_pull(skb, 12)) goto drop; /*判断GRE 版本；3.10.105中有 CISCO 和 PPTP 两个版本*/ ver = skb-&gt;data[1]&amp;0x7f; if (ver &gt;= GREPROTO_MAX) goto drop; /*找出此版本的处理函数，以函数 gre_add_protocol 进行注册*/ rcu_read_lock(); /*本例以 CISCO为例进行分析*/ proto = rcu_dereference(gre_proto[ver]); if (!proto || !proto-&gt;handler) goto drop_unlock; ret = proto-&gt;handler(skb); rcu_read_unlock(); return ret;drop_unlock: rcu_read_unlock();drop: kfree_skb(skb); return NET_RX_DROP;&#125; 12345678910111213141516171819202122232425262728static int __init ipgre_init(void)&#123; int err; /*定义了两种不同的设备类型 ： gre、gretap； gretap 处理 内部封装为 ETH_P_TEB 类型报文； 其他报文 gre类型处理 */ err = register_pernet_device(&amp;ipgre_net_ops); if (err &lt; 0) return err; err = register_pernet_device(&amp;ipgre_tap_net_ops); if (err &lt; 0) goto pnet_tap_faied; err = gre_add_protocol(&amp;ipgre_protocol, GREPROTO_CISCO); if (err &lt; 0) &#123; pr_info("%s: can't add protocol\n", __func__); goto add_proto_failed; &#125; ……&#125;--&gt;&gt;static const struct gre_protocol ipgre_protocol = &#123; .handler = ipgre_rcv, .err_handler = ipgre_err,&#125;; ipgre_rcv123456789101112131415161718192021222324252627282930313233343536373839static int ipgre_rcv(struct sk_buff *skb)&#123; struct net *net = dev_net(skb-&gt;dev); struct ip_tunnel_net *itn; const struct iphdr *iph; struct ip_tunnel *tunnel; struct tnl_ptk_info tpi; int hdr_len; bool csum_err = false; /*根据skb解析gre头部，并将flag，内部协议等存入tpi； hdr_len 为GRE头部长度*/ if (parse_gre_header(skb, &amp;tpi, &amp;csum_err, &amp;hdr_len) &lt; 0) goto drop; /*proto 有效载荷数据包的协议类型。 通常，该值将是数据包的以太网协议类型字段。 ETH_P_TEB 0x6558 表示 Transparent Ethernet Bridging */ if (tpi.proto == htons(ETH_P_TEB)) itn = net_generic(net, gre_tap_net_id); else itn = net_generic(net, ipgre_net_id); /*获取IP头，查找隧道信息； 找到的话证明本地存在处理此报文的接口； 没有的话发送ICMP 目的不可达警告 */ iph = ip_hdr(skb); tunnel = ip_tunnel_lookup(itn, skb-&gt;dev-&gt;ifindex, tpi.flags, iph-&gt;saddr, iph-&gt;daddr, tpi.key); if (tunnel) &#123; skb_pop_mac_header(skb); ip_tunnel_rcv(tunnel, skb, &amp;tpi, hdr_len, log_ecn_error); return 0; &#125; icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);drop: kfree_skb(skb); return 0;&#125; ip_tunnel_rcv123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869int ip_tunnel_rcv(struct ip_tunnel *tunnel, struct sk_buff *skb, const struct tnl_ptk_info *tpi, int hdr_len, bool log_ecn_error)&#123; struct pcpu_tstats *tstats; const struct iphdr *iph = ip_hdr(skb); int err; secpath_reset(skb); /*重置skb protocol 、二层头、去掉gre头部、重新校验*/ skb-&gt;protocol = tpi-&gt;proto; skb-&gt;mac_header = skb-&gt;network_header; __pskb_pull(skb, hdr_len); skb_postpull_rcsum(skb, skb_transport_header(skb), tunnel-&gt;hlen); /*开启了GRE广播，判断目的地址是否为广播，之后进行广播处理*/#ifdef CONFIG_NET_IPGRE_BROADCAST if (ipv4_is_multicast(iph-&gt;daddr)) &#123; /* Looped back packet, drop it! */ if (rt_is_output_route(skb_rtable(skb))) goto drop; tunnel-&gt;dev-&gt;stats.multicast++; skb-&gt;pkt_type = PACKET_BROADCAST; &#125;#endif /*校验和标志和隧道信息不符则丢包*/ if ((!(tpi-&gt;flags&amp;TUNNEL_CSUM) &amp;&amp; (tunnel-&gt;parms.i_flags&amp;TUNNEL_CSUM)) || ((tpi-&gt;flags&amp;TUNNEL_CSUM) &amp;&amp; !(tunnel-&gt;parms.i_flags&amp;TUNNEL_CSUM))) &#123; tunnel-&gt;dev-&gt;stats.rx_crc_errors++; tunnel-&gt;dev-&gt;stats.rx_errors++; goto drop; &#125; /*隧道开启序列号标志 但是 报文中没开启或序列号错误 则丢包*/ if (tunnel-&gt;parms.i_flags&amp;TUNNEL_SEQ) &#123; if (!(tpi-&gt;flags&amp;TUNNEL_SEQ) || (tunnel-&gt;i_seqno &amp;&amp; (s32)(ntohl(tpi-&gt;seq) - tunnel-&gt;i_seqno) &lt; 0)) &#123; tunnel-&gt;dev-&gt;stats.rx_fifo_errors++; tunnel-&gt;dev-&gt;stats.rx_errors++; goto drop; &#125; tunnel-&gt;i_seqno = ntohl(tpi-&gt;seq) + 1; &#125; /*隧道接口类型为 ARPHRD_ETHER（Ethernet）*/ if (tunnel-&gt;dev-&gt;type == ARPHRD_ETHER) &#123; if (!pskb_may_pull(skb, ETH_HLEN)) &#123; tunnel-&gt;dev-&gt;stats.rx_length_errors++; tunnel-&gt;dev-&gt;stats.rx_errors++; goto drop; &#125; /*处理内部报文，获取内部ip header； 修改skb内部信息：dev、protocol等； 重新计算校验和*/ iph = ip_hdr(skb); skb-&gt;protocol = eth_type_trans(skb, tunnel-&gt;dev); skb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN); &#125; /*同样重置信息*/ skb-&gt;pkt_type = PACKET_HOST; __skb_tunnel_rx(skb, tunnel-&gt;dev); skb_reset_network_header(skb); /*解封装*/ err = IP_ECN_decapsulate(iph, skb); ………… gro_cells_receive(&amp;tunnel-&gt;gro_cells, skb); return 0;drop: kfree_skb(skb); return 0;&#125; gro_cells_receive12345678910111213141516171819202122232425262728static inline void gro_cells_receive(struct gro_cells *gcells, struct sk_buff *skb)&#123; struct gro_cell *cell = gcells-&gt;cells; struct net_device *dev = skb-&gt;dev; /*接口不支持GRO或 skb为复制报文，则进入netif_rx，此函数不在分析*/ if (!cell || skb_cloned(skb) || !(dev-&gt;features &amp; NETIF_F_GRO)) &#123; netif_rx(skb); return; &#125; /*下面是NAPI方式处理报文*/ if (skb_rx_queue_recorded(skb)) cell += skb_get_rx_queue(skb) &amp; gcells-&gt;gro_cells_mask; if (skb_queue_len(&amp;cell-&gt;napi_skbs) &gt; netdev_max_backlog) &#123; atomic_long_inc(&amp;dev-&gt;rx_dropped); kfree_skb(skb); return; &#125; /* We run in BH context */ spin_lock(&amp;cell-&gt;napi_skbs.lock); __skb_queue_tail(&amp;cell-&gt;napi_skbs, skb); if (skb_queue_len(&amp;cell-&gt;napi_skbs) == 1) napi_schedule(&amp;cell-&gt;napi); spin_unlock(&amp;cell-&gt;napi_skbs.lock);&#125; 发包模块初始化时，注册的链路处理函数中 12345678910111213/*只看setup*/static struct rtnl_link_ops ipgre_link_ops __read_mostly = &#123; .kind = "gre", …… .setup = ipgre_tunnel_setup, ……&#125;;static struct rtnl_link_ops ipgre_tap_ops __read_mostly = &#123; .kind = "gretap", …… .setup = ipgre_tap_setup, ……&#125;; 以’gretap’为例分析 12345678910111213141516static void ipgre_tap_setup(struct net_device *dev)&#123; ether_setup(dev); dev-&gt;netdev_ops = &amp;gre_tap_netdev_ops; ip_tunnel_setup(dev, gre_tap_net_id);&#125;--&gt;&gt;static const struct net_device_ops gre_tap_netdev_ops = &#123; .ndo_init = gre_tap_init, .ndo_uninit = ip_tunnel_uninit, .ndo_start_xmit = gre_tap_xmit, .ndo_set_mac_address = eth_mac_addr, .ndo_validate_addr = eth_validate_addr, .ndo_change_mtu = ip_tunnel_change_mtu, .ndo_get_stats64 = ip_tunnel_get_stats64,&#125;; gre_tap_xmit12345678910111213141516static netdev_tx_t gre_tap_xmit(struct sk_buff *skb, struct net_device *dev)&#123; struct ip_tunnel *tunnel = netdev_priv(dev); /*接口支持gso，以gso方式处理*/ skb = handle_offloads(tunnel, skb); if (IS_ERR(skb)) goto out; /*推头部空间而不修改数据*/ if (skb_cow_head(skb, dev-&gt;needed_headroom)) goto free_skb; __gre_xmit(skb, dev, &amp;tunnel-&gt;parms.iph, htons(ETH_P_TEB)); return NETDEV_TX_OK; ……&#125; __gre_xmit1234567891011121314151617181920212223242526272829static void __gre_xmit(struct sk_buff *skb, struct net_device *dev, const struct iphdr *tnl_params, __be16 proto)&#123; struct ip_tunnel *tunnel = netdev_priv(dev); struct tnl_ptk_info tpi; /*需要增加封装，因此将原有skb头赋值给inner头部字段*/ if (likely(!skb-&gt;encapsulation)) &#123; skb_reset_inner_headers(skb); skb-&gt;encapsulation = 1; &#125; /*tpi为gre头部一些数据，之后build gre头部时会将此数据放到skb中*/ tpi.flags = tunnel-&gt;parms.o_flags; tpi.proto = proto; tpi.key = tunnel-&gt;parms.o_key; if (tunnel-&gt;parms.o_flags &amp; TUNNEL_SEQ) tunnel-&gt;o_seqno++; tpi.seq = htonl(tunnel-&gt;o_seqno); /* Push GRE header. */ skb = gre_build_header(skb, &amp;tpi, tunnel-&gt;hlen); if (unlikely(!skb)) &#123; dev-&gt;stats.tx_dropped++; return; &#125; ip_tunnel_xmit(skb, dev, tnl_params);&#125; ip_tunnel_xmit123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117void ip_tunnel_xmit(struct sk_buff *skb, struct net_device *dev, const struct iphdr *tnl_params)&#123; struct ip_tunnel *tunnel = netdev_priv(dev); const struct iphdr *inner_iph; struct iphdr *iph; struct flowi4 fl4; u8 tos, ttl; __be16 df; struct rtable *rt; /* Route to the other host */ struct net_device *tdev; /* Device to other host */ unsigned int max_headroom; /* The extra header space needed */ __be32 dst; inner_iph = (const struct iphdr *)skb_inner_network_header(skb); memset(IPCB(skb), 0, sizeof(*IPCB(skb))); /*隧道没有指定远端IP则自动查找匹配路由*/ dst = tnl_params-&gt;daddr; if (dst == 0) &#123; /* NBMA tunnel */ if (skb_dst(skb) == NULL) &#123; dev-&gt;stats.tx_fifo_errors++; goto tx_error; &#125; if (skb-&gt;protocol == htons(ETH_P_IP)) &#123; rt = skb_rtable(skb); dst = rt_nexthop(rt, inner_iph-&gt;daddr); &#125; …… else goto tx_error; &#125; tos = tnl_params-&gt;tos; if (tos &amp; 0x1) &#123; tos &amp;= ~0x1; if (skb-&gt;protocol == htons(ETH_P_IP)) tos = inner_iph-&gt;tos; else if (skb-&gt;protocol == htons(ETH_P_IPV6)) tos = ipv6_get_dsfield((const struct ipv6hdr *)inner_iph); &#125; /*根据源目IP等信息查找路由*/ rt = ip_route_output_tunnel(dev_net(dev), &amp;fl4, tunnel-&gt;parms.iph.protocol, dst, tnl_params-&gt;saddr, tunnel-&gt;parms.o_key, RT_TOS(tos), tunnel-&gt;parms.link); if (IS_ERR(rt)) &#123; dev-&gt;stats.tx_carrier_errors++; goto tx_error; &#125; tdev = rt-&gt;dst.dev; /*同一接口 入和出 */ if (tdev == dev) &#123; ip_rt_put(rt); dev-&gt;stats.collisions++; goto tx_error; &#125; if (tnl_update_pmtu(dev, skb, rt, tnl_params-&gt;frag_off)) &#123; ip_rt_put(rt); goto tx_error; &#125; …… /*隧道信息没有设置ttl,则使用内部报文的ttl*/ ttl = tnl_params-&gt;ttl; if (ttl == 0) &#123; if (skb-&gt;protocol == htons(ETH_P_IP)) ttl = inner_iph-&gt;ttl; …… else ttl = ip4_dst_hoplimit(&amp;rt-&gt;dst); &#125; /*是否需要分片*/ df = tnl_params-&gt;frag_off; if (skb-&gt;protocol == htons(ETH_P_IP)) df |= (inner_iph-&gt;frag_off&amp;htons(IP_DF)); max_headroom = LL_RESERVED_SPACE(tdev) + sizeof(struct iphdr) + rt-&gt;dst.header_len; if (max_headroom &gt; dev-&gt;needed_headroom) dev-&gt;needed_headroom = max_headroom; if (skb_cow_head(skb, dev-&gt;needed_headroom)) &#123; dev-&gt;stats.tx_dropped++; dev_kfree_skb(skb); return; &#125; skb_dst_drop(skb); skb_dst_set(skb, &amp;rt-&gt;dst); /*插入IP头部*/ skb_push(skb, sizeof(struct iphdr)); skb_reset_network_header(skb); iph = ip_hdr(skb); inner_iph = (const struct iphdr *)skb_inner_network_header(skb); iph-&gt;version = 4; iph-&gt;ihl = sizeof(struct iphdr) &gt;&gt; 2; iph-&gt;frag_off = df; iph-&gt;protocol = tnl_params-&gt;protocol; iph-&gt;tos = ip_tunnel_ecn_encap(tos, inner_iph, skb); iph-&gt;daddr = fl4.daddr; iph-&gt;saddr = fl4.saddr; iph-&gt;ttl = ttl; __ip_select_ident(iph, skb_shinfo(skb)-&gt;gso_segs ?: 1); iptunnel_xmit(skb, dev); return; ……&#125; iptunnel_xmit123456789101112static inline void iptunnel_xmit(struct sk_buff *skb, struct net_device *dev)&#123; int err; int pkt_len = skb-&gt;len - skb_transport_offset(skb); struct pcpu_tstats *tstats = this_cpu_ptr(dev-&gt;tstats); nf_reset(skb); /*标准发包流程*/ err = ip_local_out(skb); ……&#125; GRE实例 期望拓扑如下 环境信息 12345678910111213141516171819#vm 1[root@test-1 ~]# uname -srLinux 3.10.0-693.el7.x86_64[root@test-1 ~]# ip addr show ……2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:0c:e3:c6 brd ff:ff:ff:ff:ff:ff inet 10.10.10.11/24 brd 10.10.10.255 scope global dynamic eth0 valid_lft 82685sec preferred_lft 82685sec#vm2[root@test-2 net]# uname -srLinux 3.10.0-693.el7.x86_64[root@test-2 net]# ip addr show ……2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:50:61:0d brd ff:ff:ff:ff:ff:ff inet 10.10.10.9/24 brd 10.10.10.255 scope global dynamic eth0 valid_lft 82645sec preferred_lft 82645sec 开始配置 首先需要给设备加载ip_gre模块 1[root@test-1 ~]# modprobe ip_gre 加载完成之后，设备应该会多出两个接口：gre0、gretap0。这是模块初始过程中创建的两个针对不同gre类型的两个接口(first device)。 创建接口并设置IP。 123[root@test-1 ~]# ip tunnel add gre1 mode gre remote 10.10.10.9 local 10.10.10.11 ttl 255[root@test-1 ~]# ip link set gre1 up[root@test-1 ~]# ip addr add 192.168.1.3/24 peer 192.168.1.4/24 dev gre1 对端设备做类似配置。 路由表项。 12[root@test-1 ~]# ip route 192.168.1.0/24 dev gre1 proto kernel scope link src 192.168.1.3 测试连通性。 对端抓包如下 troubleshootingping 不通时，进行抓包，命令如下 12# tcpdump -i eno1 ip[9]=47# ip头第9（从0开始）位为protocol位，gre在linux中的支持为47。 两端都进行抓包，判断报文是否到达。 对端已经接收到，但是无回应：判断对端规则（iptables等）是否已经放开。 本端收到回应但ping依然不通：估计是本地规则问题。 优秀资料GRE技术介绍]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>GRE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VLAN 简介]]></title>
    <url>%2FOpenstack%2FVLAN-%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[简述以太网是一种基于CSMA/CD (Carrier Sense Multiple Access/Collision Detect，载波侦听多路访问/冲突检测)的共享通讯介质的数据网络通讯技术，当主机数目较多时会导致冲突严重、广播泛滥、性能显著下降甚至使网络不可用等问题。通过交换机实现LAN互联虽然可以解决冲突(Collision)严重的问题，但仍然不能隔离广播报文。在这种情况下出现了VLAN (Virtual Local Area Network，虚拟局域网)技术，这种技术可以把一个LAN划分成多个逻辑的LAN——VLAN，每个VLAN是一个广播域，VLAN内的主机间通信就和在一个LAN内一样，而VLAN间则不能直接互通，这样，广播报文被限制在一个VLAN内，如下图所示。 VLAN的划分不受物理位置的限制：不在同一物理位置范围的主机可以属于同一个VLAN；一个VLAN包含的用户可以连接在同一个交换机上，也可以跨越交换机，甚至可以跨越路由器。 VLAN的优点如下： 限制广播域。广播域被限制在一个VLAN内，节省了带宽，提高了网络处理能力。 增强局域网的安全性。VLAN问的二层报文是相互隔离的，即一个VLAN内的用户不能和其它VLAN内的用户直接通信，如果不同VLAN要进行通信，则需通过路由器或三层交换机等三层设备。 灵活构建虚拟工作组。用VLAN可以划分不同的用户到不同的工作组，同一工作组的用户也不必局限于某一固定的物理范围，网络构建和维护更方便灵活。 原理传统以太网封装格式如下图。 要使网络设备能够分辨不同VLAN的报文，需要在报文中添加标识VLAN的字段。由于VLAN是对二层网络进行限制，因此，识别字段需要添加到数据链路层封装中，如下图。 VLAN Tag包含四个字段，分别是TPID(Tag Protocol Identifier，标签协议标识符)、Priority, CFI(Canonical Format Indicator，标准格式指示位)和VLAN ID。 TPID用来判断本数据帧是否带有VLAN Tag，长度为16bit，缺省取值为0x8100。 Priority表示报文的802.1P优先级，长度为3bit。 CFI字段标识MAC地址在不同的传输介质中是否以标准格式进行封装，长度为1 bit，取值为0表示MAC地址以标准格式进行封装，为1表示以非标准格式封装，缺省取值为0。 VLAN ID标识该报文所属VLAN的编号，长度为12bit，取值范围为0-4095。由于0和4095为协议保留取值，所以VLAN ID的取值范围为1-4094。 网络设备利用VLAN ID来识别报文所属的VLAN，根据报文是否携带VLAN Tag以及携带的VLAN Tag值，来对报文进行处理。 VLAN划分Linux中配置VLAN的工具 ‘vconfig’是基于端口对VLAN进行划分。命令行如下（仅截取部分）： 此命令只是简单支持，一次只能增加一个vlan 接口。看了下面对接口类型的介绍就会知道，对于trunk类型的接口配置很不利。 接口类型 类型 简述 Linux是否支持 Access 端口发出去的报文不带tag标签。一般用于和不能识别VLAN tag的终端设备相连，或者不需要区分不同VLAN成员时使用。 原生代码不区分类型，都打Tag出去。 Trunk 端口发出去的报文，端口缺省VLAN内的报文不带tag，其它VLAN内的报文都必须带tag。 原生代码不区分类型，都打Tag出去。 Hybrid 端口发出去的报文可根据需要设置某些VLAN内的报文带tag，某些VLAN内的报文不带tag。 不支持。 Neutron 中 VLAN 支持网络节点配置修改文件 ‘/etc/neutron/plugins/ml2/ml2_conf.ini’ 12[ml2_type_vlan]network_vlan_ranges = vlannet:1000:2000 vlannet 是在计算节点定义的 从物理接口映射来的虚拟网络。 1000:2000 是创建网络时可选择的vlan id范围。 计算节点配置修改文件 ‘/etc/neutron/plugins/ml2/openvswitch_agent.ini ’ 1234[ovs]……bridge_mappings =vlannet:br-vlan…… Neutron网络中 配置文件，bridge_mappings是将公共虚拟网络和公共物理网络接口对应起来。 之后在每个节点上重启有关服务即可。 验证 创建VLAN网络。 创建虚拟机。 查看openflow规则。 由br-int 流表可知，从 br-vlan 进入的流量，会去掉 1000的 vlan tag，打上1的tag。由br-vlan流表可知，从br-int进入的流量，会去掉1的tag，打上1000 的 vlan tag出去。]]></content>
      <tags>
        <tag>VLAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack中的嵌套虚拟化]]></title>
    <url>%2FOpenstack%2Fopenstack%E4%B8%AD%E7%9A%84%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96.html</url>
    <content type="text"><![CDATA[在Intel处理器上，KVM使用Intel的vmx(virtul machine eXtensions)来提高虚拟机性能，即硬件辅助虚拟化技术。 那么如果在处理器中的启动的虚拟机也想使用vmx技术，应该如何处理？此时，就需要开启嵌套式虚拟nested。nested是一个可通过内核参数来启用的功能。它能够使一台虚拟机具有物理机CPU特性,支持vmx或者svm(AMD)硬件虚拟化。 服务器配置 服务器CPU需要支持 vmx(Intel架构) 或 svm(AMD架构)。 1# grep -E '(vmx|svm)' /proc/cpuinfo 服务器的内核版本需要3.0+。 12# uname -r3.10.0-693.5.2.el7.x86_64 服务器需要开启nested功能。两种架构开启方式如下。 Intel 架构 检测服务器内核是否启动嵌套。 12# cat /sys/module/kvm_intel/parameters/nestedN // Y(YES) 或 N(NO) 卸载kvm_intel模块（移除之前确保无vm在运行，否则无法卸载）。 1# modprobe -r kvm-intel 配置参数并启动 临时生效 1# modprobe kvm_intel nested=1 永久生效 12# echo 'options kvm-intel nested=y' &gt;&gt; /etc/modprobe.d/dist.conf# modprobe kvm-intel 查看是否生效 12# cat /sys/module/kvm_intel/parameters/nestedY AMD 架构 检测服务器内核是否启动嵌套。 12# cat /sys/module/kvm_amd/parameters/nested0 // 1(YES) 或 0(NO) 卸载kvm_intel模块（移除之前确保无vm在运行，否则无法卸载）。 1# modprobe -r kvm-amd 配置参数并启动 永久生效 12# echo 'options amd nested=1' &gt;&gt; /etc/modprobe.d/dist.conf# modprobe kvm-amd 查看是否生效 12# cat /sys/module/kvm_amd/parameters/nested1 虚拟机配置qemu-kvm 修改 ‘/etc/libvirt/qemu/vm.xml’ 文件中有关虚拟机CPU的特性。 在CPU数量设定下下增加’host-passthrough’支持，如下 12&lt;vcpu placement='static' current='20'&gt;22&lt;/vcpu&gt;&lt;cpu mode='host-passthrough'/&gt; 之后，重启虚拟机即可。 如果用’qemu-kvm’命令启动，则增加如下参数即可 1-enable-kvm -cpu qemu64,+vmx Tips： 为了保证虚拟机在不同宿主机之间迁移时候的兼容性，Libvirt对CPU提炼出标准的几种类型，在’/usr/share/libvirt/cpu_map.xml’中可以查到。cpu_map.xml不仅是CPU型号，还有生产商信息、每种型号的CPU特性定义等信息。 CPU配置模式可以有以下几种。 host-model模式。根据物理CPU的特性，选择一个最靠近的标准CPU型号。如果没有指定CPU模式，默认也是使用这种模式。 1&lt;cpu mode='host-model'/&gt; host-passthrough模式。直接将物理CPU暴露给虚拟机使用，在虚拟机上完全可以看到的就是物理CPU的型号。 1&lt;cpu mode='host-passthrough'/&gt; custom模式。匹配标准类型中的一种。 123&lt;cpu mode='custom' match='exact'&gt; &lt;model fallback='allow'&gt;core2duo&lt;/model&gt;&lt;/cpu&gt; ‘core2duo’为 cpu_map.xml中定义的CPU属性的模块名称。 openstack平台openstack集成了qemu-kvm，启动虚拟机属于nova操作，因此只需要如下形式修改计算节点’/etc/nova/nova.conf’文件即可。 12345[libvirt]…………cpu_mode=host-passthroughvirt_type=kvm………… 之后重启计算节点compute服务即可。 1# systemctl restart openstack-nova-compute 启动虚拟机之后，进入查看是否已经支持。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>嵌套虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Namespace简介]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-Namespace%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[Tips:系统环境为 Ubuntu 16.04，centos系统在namespace支持方面有些问题。 Namespace(命名空间)是一种纯软件方式的资源隔离方案，是Linux Container的基础，也是Docker实现的基础之一。 Linux内核中提供了6种namespace隔离的系统调用，由带有CLONE_NEW*标志的clone()所创建。这些标志如下表所示： Namespace 系统调用参数 值 隔离内容 Mount CLONE_NEWNS 0x00020000 挂载点（文件系统）。 mount namespace是第一个namespace且当时没有人想到会将这套机制扩展到其它的子系统， 等它成了API， 由于兼容性问题，也不能改名了。 UTS CLONE_NEWUTS 0x04000000 主机名与域名。 影响setdomainname()、sethostname()这类接口。 IPC CLONE_NEWIPC 0x08000000 信号量、消息队列和共享内存等进程间通信机制。 User CLONE_NEWUSER 0x10000000 用户和用户组。 PID CLONE_NEWPID 0x20000000 进程编号。 Network CLONE_NEWNET 0x40000000 网络设备、网络栈、端口等。 Linux内核实现namespace的主要目的就是为了实现轻量级虚拟化（容器）服务。在同一个namespace下的进程可以感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身于一个独立的系统环境中，以此达到独立和隔离的目的。 调用namespace的APInamespace的API包括clone()、setns()以及unshare()，还有/proc下的部分文件。为了确定隔离的到底是哪种namespace，在使用这些API时，通常需要指定以下六个常数的一个或多个，通过’|’（位或）操作来实现。 CLONE()clone()在内核实现函数为do_fork()，形式如下： 12#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...); 参数child_func传入子进程运行的程序主函数。 参数child_stack传入子进程使用的栈空间。 参数flags表示使用哪些CLONE_*标志位。 参数args则可用于传入用户参数。 在内核实现函数为do_fork()，形式如下： 12345long do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr) ‘clone_flags’ 即可赋值为上面提到的标志。 PROC文件从3.8版本的内核开始，用户就可以在/proc/[pid]/ns文件下看到指向不同namespace号的文件，效果如下所示： 12345678910# ls -al /proc/$$/ns &lt;&lt;-- $$ 表示当前进程的PIDtotal 0dr-x--x--x 2 root root 0 Dec 11 16:39 .dr-xr-xr-x 9 root root 0 Dec 11 16:22 ..lrwxrwxrwx 1 root root 0 Dec 11 16:39 ipc -&gt; ipc:[4026531839]lrwxrwxrwx 1 root root 0 Dec 11 16:39 mnt -&gt; mnt:[4026531840]lrwxrwxrwx 1 root root 0 Dec 11 16:39 net -&gt; net:[4026531956]lrwxrwxrwx 1 root root 0 Dec 11 16:39 pid -&gt; pid:[4026531836]lrwxrwxrwx 1 root root 0 Dec 11 16:39 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Dec 11 16:39 uts -&gt; uts:[4026531838] 其下面的文件依次表示每个namespace, 例如user就表示user namespace。所有文件均为符号链接, 链接指向\$namespace:[$namespace-inode-number]，前半部份为namespace的名称，后半部份的数字表示这个namespace的inode number。因此，如果两个进程指向的namespace inode number相同，就说明他们在同一个namespace下，否则则在不同namespace里面。 该链接指向的文件比较特殊，它不能直接访问，事实上指向的文件存放在被称为”nsfs”的文件系统中，该文件系统用户不可见。可以用stat()看到指向文件的inode信息： 123456789# stat -L /proc/$$/ns/net File: ‘/proc/927/ns/net’ Size: 0 Blocks: 0 IO Block: 1024 regular empty fileDevice: 3h/3d Inode: 4026531956 Links: 1Access: (0444/-r--r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-12-11 16:51:16.531134197 +0800Modify: 2017-12-11 16:51:16.531134197 +0800Change: 2017-12-11 16:51:16.531134197 +0800 Birth: - SETNS()加入一个已经存在的namespace中以通过setns() 系统调用来完成。它的原型如下： 12345int setns(int fd, int nstype);/*fd表示我们要加入的namespace的文件描述符。nstype让调用者可以去检查fd指向的namespace类型是否符合我们实际的要求。如果填0表示不检查。*/ util-linux包里提供了nsenter命令，其提供了一种方式将新创建的进程运行在指定的namespace里面，它的实现很简单，就是通过命令行指定要进入的namespace的file，然后利用setns()指当前的进程放到指定的namespace里面，再clone()运行指定的执行文件。我们可以用strace来看看它的运行情况： 1234567891011121314151617181920# strace nsenter -t 27242 -i -m -n -p -u /bin/bashexecve("/usr/bin/nsenter", ["nsenter", "-t", "27242", "-i", "-m", "-n", "-p", "-u", "/bin/bash"], [/* 21 vars */]) = 0……………………pen("/proc/27242/ns/ipc", O_RDONLY) = 3open("/proc/27242/ns/uts", O_RDONLY) = 4open("/proc/27242/ns/net", O_RDONLY) = 5open("/proc/27242/ns/pid", O_RDONLY) = 6open("/proc/27242/ns/mnt", O_RDONLY) = 7setns(3, CLONE_NEWIPC) = 0close(3) = 0setns(4, CLONE_NEWUTS) = 0close(4) = 0setns(5, CLONE_NEWNET) = 0close(5) = 0setns(6, CLONE_NEWPID) = 0close(6) = 0setns(7, CLONE_NEWNS) = 0close(7) = 0clone(child_stack=0, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f4deb1faad0) = 4968 nsenter先获得target进程(-t参数指定)所在的namespace的文件, 然后再调用setns()将当前所在的进程加入到对应的namespace里面, 最后再clone()运行我们指定的二进制文件。 UNSHARE()unshare()系统调用用于将当前进程和所在的namespace分离并且加入到新创建的namespace之中。unshare()运行在原先的进程上，不需要启动一个新进程，使用方法如下 1int unshare(int flags); Linux中自带的unshare命令，就是通过unshare()系统调用实现的。 UTS namespaceUTS namespace提供了主机名和域名的隔离，这样每个容器就可以拥有了独立的主机名和域名，在网络上可以被视作一个独立的节点而非宿主机上的一个进程。 编译并运行以下程序： 1234567891011121314151617181920212223242526272829303132333435#define _GNU_SOURCE#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt; /* 定义一个给 clone 用的栈，栈大小1M */#define STACK_SIZE (1024 * 1024)static char container_stack[STACK_SIZE]; char* const container_args[] = &#123; "/bin/bash", NULL&#125;; int container_main(void* arg)&#123; printf("Container - inside the container!\n"); /* 直接执行一个shell，以便我们观察这个进程空间里的资源是否被隔离了 */ execv(container_args[0], container_args); return 1;&#125; int main()&#123; printf("Parent - start a container!\n"); /* 调用clone函数，其中传出一个函数，还有一个栈空间的（为什么传尾指针，因为栈是反着的） */ int container_pid = clone(container_main, container_stack+STACK_SIZE, SIGCHLD, NULL); /* 等待子进程结束 */ waitpid(container_pid, NULL, 0); printf("Parent - container stopped!\n"); return 0;&#125; 执行结果为： 12345678root@ubuntu:~# gcc uts.c ; ./a.outParent - start a container!Container - inside the container!root@ubuntu:~# root@ubuntu:~# exitexitParent - container stopped!root@ubuntu:~# 加入UTS隔离。 12345678910111213141516//[...]int child_main(void* arg) &#123; printf("Container - inside the container!\n"); /* 直接执行一个shell，以便我们观察这个进程空间里的资源是否被隔离了 */ sethostname("container",10); /* 设置hostname */ execv(container_args[0], container_args); return 1;&#125;int main() &#123;//[...]/*启用CLONE_NEWUTS Namespace隔离 */int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | SIGCHLD, NULL); //[...]&#125; 运行结果为： 12345678root@ubuntu:~# gcc uts.c ; ./a.outParent - start a container!Container - inside the container!root@container:~# hostnamecontainerroot@container:~# exitexitParent - container stopped! 不加CLONE_NEWUTS参数运行上述代码，发现主机名也变了，输入exit以后主机名也会变回来，似乎没什么区别。实际上不加CLONE_NEWUTS参数进行隔离而使用sethostname已经把宿主机的主机名改掉了。你看到exit退出后还原只是因为bash只在刚登录的时候读取一次UTS，当你重新登陆或者使用uname命令进行查看时，就会发现产生了变化。 IPC namespace进程间通信采用的方法包括常见的信号量、消息队列和共享内存。对不在原namespace中的进程来说，之间的通信，实际上是具有相同’PID namespace’中的进程间通信，因此需要一个唯一的标识符来进行区别。申请IPC资源就申请了这样一个全局唯一的32位ID，所以IPC namespace中实际上包含了系统IPC标识符以及实现POSIX消息队列的文件系统。在同一个IPC namespace下的进程彼此可见，而与其他的IPC namespace下的进程则互相不可见。 修改上面的代码： 1234//[...]int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWIPC | SIGCHLD, NULL);//[...] 在shell中使用’ipcmk -Q’命令创建一个message queue，并使用’ipcs -q’查看已经开启的message queue。 1234567root@ubuntu:~# ipcmk -QMessage queue id: 0root@ubuntu:~# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages 0x875028f6 0 root 644 0 0 编译并运行修改后的程序： 1234567891011root@ubuntu:~# gcc uts.c ; ./a.out Parent - start a container!Container - inside the container!root@container:~# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages root@container:~# exitexitParent - container stopped! 上面的结果显示中可以发现，已经找不到原先声明的message queue，实现了IPC的隔离。 PID namespace PID namespace隔离非常实用，它对进程PID重新标号，即两个不同namespace下的进程可以有同一个PID。每个PID namespace都有自己的计数程序。内核为所有的PID namespace维护了一个树状结构，最顶层的是系统初始时创建的，我们称之为root namespace。它创建的新PID namespace就称之为child namespace（树的子节点），而原先的PID namespace就是新创建的PID namespace的parent namespace（树的父节点）。通过这种方式，不同的PID namespaces会形成一个等级体系。所属的父节点可以看到子节点中的进程，并可以通过信号量等方式对子节点中的进程产生影响。反过来，子节点不能看到父节点PID namespace中的任何内容。由此产生如下结论。 每个PID namespace中的第一个进程“PID 1“，都会像传统Linux中的init进程一样拥有特权，起特殊作用。 一个namespace中的进程，不可能通过kill或ptrace影响父节点或者兄弟节点中的进程，因为其他节点的PID在这个namespace中没有任何意义。 如果你在新的PID namespace中重新挂载/proc文件系统，会发现其下只显示同属一个PID namespace中的其他进程。 在root namespace中可以看到所有的进程，并且递归包含所有子节点中的进程。 修改上文的代码，加入PID namespace的标识位： 1234//[...]int child_pid = clone(child_main, child_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWPID | SIGCHLD, NULL); //[...] 运行结果如下： 12345678910root@ubuntu:~# echo $$ 894root@ubuntu:~# gcc uts.c ; ./a.out ;Parent - start a container!Container - inside the container!root@container:~# echo $$ &lt;&lt;&lt;--- shell 程序的PID已经改变1root@container:~# exitexitParent - container stopped! 在子进程的shell中执行了ps aux/top之类的命令，发现还是可以看到所有父进程的PID，那是因为还没有对文件系统进行隔离，ps/top之类的命令调用的是真实系统下的/proc文件内容，看到的自然是所有的进程。 此外，与其他的namespace不同的是，为了实现一个稳定安全的隔离空间（例如，容器），PID namespace还需要进行一些额外的工作才能确保其中的进程运行顺利。 PID NAMESPACE中的INIT进程 当我们新建一个PID namespace时，默认启动的进程PID为1。在传统的UNIX系统中，PID为1的进程是init，地位非常特殊。它作为所有进程的父进程，维护一张进程表，不断检查进程的状态，一旦有某个子进程因为程序错误成为了“孤儿”进程，init就会负责回收资源并结束这个子进程。所以在实现的容器中，启动的第一个进程也需要实现类似init的功能，维护所有后续启动进程的运行状态。 PID namespace维护这样一个树状结构，非常有利于系统的资源监控与回收。 信号量与INIT进程 PID namespace中的init进程如此特殊，自然内核也为它赋予了特权——信号量屏蔽。如果init中没有处理某个信号量的代码逻辑，那么与init在同一个PID namespace下的进程（即使有超级权限）发送给它的该信号量都会被屏蔽。这个功能的主要作用是防止init进程被误杀。 父节点中的进程发送的信号量，如果不是SIGKILL（销毁进程）或SIGSTOP（暂停进程）也会被忽略。但如果发送SIGKILL或SIGSTOP，子节点的init会强制执行（无法通过代码捕捉进行特殊处理），也就是说父节点中的进程有权终止子节点中的进程。 一旦init进程被销毁，同一PID namespace中的其他进程也会随之接收到SIGKILL信号量而被销毁。理论上，该PID namespace自然也就不复存在了。但是如果/proc/[pid]/ns/pid处于被挂载或者打开状态，namespace就会被保留下来。然而，保留下来的namespace无法通过setns()或者fork()创建进程，所以实际上并没有什么作用。 挂载PROC文件系统 如果你在新的PID namespace中使用ps命令查看，看到的还是所有的进程，因为与PID直接相关的/proc文件系统（procfs）没有挂载到与原/proc不同的位置。所以如果你只想看到PID namespace本身应该看到的进程，需要重新挂载/proc，命令如下： 12345root@Changed Name:~# mount -t proc proc /procroot@Changed Name:~# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 115388 2024 pts/0 S 19:49 0:00 /bin/bashroot 13 0.0 0.0 151064 1800 pts/0 R+ 19:49 0:00 ps aux 可以看到实际的PID namespace就只有两个进程在运行。 注意：因为此时我们没有进行mount namespace的隔离，所以这一步操作实际上已经影响了 root namespace的文件系统，当你退出新建的PID namespace以后再执行ps a就会发现出错，再次执行mount -t proc proc /proc可以修复错误。 UNSHARE()和SETNS() unshare()允许用户在原有进程中建立namespace进行隔离。但是创建了PID namespace后，原先unshare()调用者进程并不进入新的PID namespace，接下来创建的子进程才会进入新的namespace，这个子进程也就随之成为新namespace中的init进程。 类似的，调用setns()创建新PID namespace时，调用者进程也不进入新的PID namespace，而是随后创建的子进程进入。 这样设计是因为调用getpid()函数得到的PID是根据调用者所在的PID namespace而决定返回哪个PID，进入新的PID namespace会导致PID产生变化。而对用户态的程序和库函数来说，他们都认为进程的PID是一个常量，PID的变化会引起这些进程崩溃。 换句话说，一旦程序进程创建以后，那么它的PID namespace的关系就确定下来了，进程不会变更他们对应的PID namespace。 Mount namespacesMount namespace通过隔离文件系统挂载点对隔离文件系统提供支持，它是历史上第一个Linux namespace，所以它的标识位比较特殊，就是CLONE_NEWNS。隔离后，不同mount namespace中的文件结构发生变化也互不影响。可以通过/proc/[pid]/mounts查看到所有挂载在当前namespace中的文件系统，还可以通过/proc/[pid]/mountstats看到mount namespace中文件设备的统计信息，包括挂载文件的名字、文件系统类型、挂载位置等等。 进程在创建mount namespace时，会把当前的文件结构复制给新的namespace。新namespace中的所有mount操作都只影响自身的文件系统，而对外界不会产生任何影响。这样做非常严格地实现了隔离，但是某些情况可能并不适用。比如父节点namespace中的进程挂载了一张CD-ROM，这时子节点namespace拷贝的目录结构就无法自动挂载上这张CD-ROM，因为这种操作会影响到父节点的文件系统。 2006 年引入的挂载传播（mount propagation）解决了这个问题，挂载传播定义了挂载对象（mount object）之间的关系，系统用这些关系决定任何挂载对象中的挂载事件如何传播到其他挂载对象参考自：http://www.ibm.com/developerworks/library/l-mount-namespaces/。 所谓传播事件，是指由一个挂载对象的状态变化导致的其它挂载对象的挂载与解除挂载动作的事件。 共享关系（share relationship）。如果两个挂载对象具有共享关系，那么一个挂载对象中的挂载事件会传播到另一个挂载对象，反之亦然。 从属关系（slave relationship）。如果两个挂载对象形成从属关系，那么一个挂载对象中的挂载事件会传播到另一个挂载对象，但是反过来不行；在这种关系中，从属对象是事件的接收者。 一个挂载状态可能为如下的其中一种： 共享挂载（shared） 从属挂载（slave） 共享/从属挂载（shared and slave） 私有挂载（private） 不可绑定挂载（unbindable） 传播事件的挂载对象称为共享挂载（shared mount）；接收传播事件的挂载对象称为从属挂载（slave mount）。既不传播也不接收传播事件的挂载对象称为私有挂载（private mount）。另一种特殊的挂载对象称为不可绑定的挂载（unbindable mount），它们与私有挂载相似，但是不允许执行绑定挂载，即创建mount namespace时这块文件对象不可被复制。 共享挂载的应用场景非常明显，就是为了文件数据的共享所必须存在的一种挂载方式；从属挂载更大的意义在于某些“只读”场景；私有挂载其实就是纯粹的隔离，作为一个独立的个体而存在；不可绑定挂载则有助于防止没有必要的文件拷贝，如某个用户数据目录，当根目录被递归式的复制时，用户目录无论从隐私还是实际用途考虑都需要有一个不可被复制的选项。 默认情况下，所有挂载都是私有的。 12345678910/*设置为共享挂载。从共享挂载克隆的挂载对象也是共享的挂载；它们相互传播挂载事件。*/# mount --make-shared &lt;mount-object&gt;/*设置为从属挂载。从从属挂载克隆的挂载对象也是从属的挂载，它也从属于原来的从属挂载的主挂载对象。*/# mount --make-slave &lt;shared-mount-object&gt;/*将一个从属挂载对象设置为共享/从属挂载，可以执行如下命令或者将其移动到一个共享挂载对象下。*/# mount --make-shared &lt;slave-mount-object&gt;/*把修改过的挂载对象重新标记为私有的。*/# mount --make-private &lt;mount-object&gt;/*将挂载对象标记为不可绑定的。*/# mount --make-unbindable &lt;mount-object&gt; 这些设置都可以递归式地应用到所有子目录中，可搜索到相关的命令进行深入。 修改上面的代码： 123456789//[...]int container_main(void* arg) //[...] system("mount -t proc proc /proc"); execv(container_args[0], container_args);//[...]int child_pid = clone(child_main, child_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL);//[...] 在子namespace中，能看到挂载，而在父空间中无挂载。在父空间执行’mount -t proc proc /proc’即可恢复。 Network namespaceNetwork namespace主要提供了关于网络资源的隔离，包括网络设备、IPv4和IPv6协议栈、IP路由表、防火墙、/proc/net目录、/sys/class/net目录、端口（socket）等等。一个物理的网络设备最多存在在一个network namespace中，你可以通过创建veth pair（虚拟网络设备对：有两端，类似管道，如果数据从一端传入另一端也能接收到，反之亦然）在不同的network namespace间创建通道，以此达到通信的目的。 一般情况下，物理网络设备都分配在最初的root namespace中。但是如果你有多块物理网卡，也可以把其中一块或多块分配给新创建的network namespace。需要注意的是，当新创建的network namespace被释放时（所有内部的进程都终止并且namespace文件没有被挂载或打开），在这个namespace中的物理网卡会返回到root namespace而非创建该进程的父进程所在的network namespace。 为了使新创建的namespace与外部进行网络通信，经典做法就是创建一个veth pair，一端放置在新的namespace中，一端放在另一个namespace中连接物理网络设备，再通过网桥把别的设备连接进来或者进行路由转发，以此网络实现通信的目的。 对network namespace的使用其实就是在创建的时候添加CLONE_NEWNET标识位。可以通过命令行工具ip创建network namespace。 1234567# ip netns add &lt;network namespace name&gt; // 创建net namespace# ip netns [list] // 显示当前所有net namespace# ip netns delete &lt;network namespace name&gt; // 删除net namespace# ip netns exec &lt;network namespace name&gt; &lt;command&gt; // 在net namespace中执行命令当然，你也可以进入net namespace# ip netns exec &lt;network namespace name&gt; bash之后可以在其中执行命令 下面开始执行如下命令： 1234# ip netns add test_ns# ip netns exec test_ns ip addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 当ip命令工具创建一个network namespace时，会默认创建一个回环设备（loopback interface：lo），并在/var/run/netns目录下绑定一个挂载点，这就保证了就算network namespace中没有进程在运行也不会被释放，也给系统管理员对新创建的network namespace进行配置提供了充足的时间。 在新创建的namespace中，lo接口状态是’DOWN’的，因此，第一个任务应该是把它启动。 12345678910# ip netns exec test_ns ip link set dev lo up# ip netns exec test_ns ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever# ip link add veth0 type veth peer name veth1# ip link set veth1 netns test_ns# ip netns exec test_ns ifconfig veth1 10.1.1.1/24 up# ifconfig veth0 10.1.1.2/24 up 通过ping命令进行测试： 12345678# ping 10.1.1.1PING 10.1.1.1 (10.1.1.1) 56(84) bytes of data.64 bytes from 10.1.1.1: icmp_seq=1 ttl=64 time=0.048 ms……[root@pro4-node ~]# ip netns exec test_ns ping 10.1.1.2PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.040 ms……… 通信正常。 net namespace实现了在同一设备内部虚拟出多个网络设备，极大利用了现有设备性能。 User namespacesUser namespace主要隔离了安全相关的标识符（identifiers）和属性（attributes），包括用户ID、用户组ID、root目录、key（指密钥）以及特殊权限。说得通俗一点，一个普通用户的进程通过clone()创建的新进程在新user namespace中可以拥有不同的用户和用户组。 User namespace是目前的六个namespace中最后一个支持的，并且直到Linux内核3.8版本的时候还未完全实现（还有部分文件系统不支持）。因为user namespace实际上并不算完全成熟，很多发行版担心安全问题，在编译内核的时候并未开启USER_NS。所以在进行接下来的代码实验时，请确保你系统的Linux内核版本高于3.8并且内核编译时开启了USER_NS。 Linux中，特权用户的user ID就是0，演示的最终我们将看到user ID非0的进程启动user namespace后user ID可以变为0。使用user namespace的方法跟别的namespace相同，即调用clone()或unshare()时加入CLONE_NEWUSER标识位。为了看到用户权限(Capabilities)，可能还需要安装一下libcap-dev包。 头文件以调用Capabilities包。 1#include &lt;sys/capability.h&gt; 在子进程函数中加入geteuid()和getegid()得到namespace内部的user ID，其次通过cap_get_proc()得到当前进程的用户拥有的权限，并通过cap_to_text（）输出。 12345678910int child_main(void* args) &#123; printf("在子进程中!\n"); cap_t caps; printf("eUID = %ld; eGID = %ld; ", (long) geteuid(), (long) getegid()); caps = cap_get_proc(); printf("capabilities: %s\n", cap_to_text(caps, NULL)); execv(child_args[0], child_args); return 1;&#125; 在主函数的clone()调用中加入CLONE_NEWUSER标识符。 123//[...]int child_pid = clone(child_main, child_stack+STACK_SIZE, CLONE_NEWUSER | SIGCHLD, NULL);//[...] 123456789#当前的user id 和 group idstack@ubuntu:~$ iduid=1000(stack) gid=1000(stack) groups=1000(stack)#非root 用户stack@ubuntu:~$ ./uts 程序开始: 在子进程中!eUID = 65534; eGID = 65534; capabilities: = cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,37+epnobody@ubuntu:~$ user namespace被创建后，第一个进程被赋予了该namespace中的全部权限，这样这个init进程就可以完成所有必要的初始化工作，而不会因权限不足而出现错误。 namespace内部看到的UID和GID已经与外部不同了，默认显示为65534，表示尚未与外部namespace用户映射。需要对user namespace内部的这个初始user和其外部namespace某个用户建立映射，这样可以保证当涉及到一些对外部namespace的操作时，系统可以检验其权限（比如发送一个信号量或操作某个文件）。同样用户组也要建立映射。 还有一点虽然不能从输出中看出来，但是值得注意。用户在新namespace中有全部权限，但是在创建它的父namespace中不含任何权限。就算调用和创建它的进程有全部权限也是如此。所以哪怕是root用户调用了clone()在user namespace中创建出的新用户在外部也没有任何权限。 user namespace的创建其实是一个层层嵌套的树状结构。最上层的根节点就是root namespace，新创建的每个user namespace都有一个父节点user namespace以及零个或多个子节点user namespace，这一点与PID namespace非常相似。 接下来我们就要进行用户绑定（映射）操作，通过在/proc/[pid]/uid_map和/proc/[pid]/gid_map两个文件中写入对应的绑定信息可以实现这一点，格式如下： 1ID-inside-ns ID-outside-ns length 写这两个文件需要注意以下几点。 这两个文件只允许由拥有该user namespace中CAP_SETUID和CAP_SETGID权限的进程写入一次，但可以一次写多条，并且最多只能5条，不允许修改。 写入的进程必须是该user namespace的父namespace或者子namespace。 第一个字段ID-inside-ns表示新建的user namespace中对应的user/group ID，第二个字段ID-outside-ns表示namespace外部映射的user/group ID。最后一个字段表示映射范围，通常填1，表示只映射一个，如果填大于1的值，则按顺序建立一一映射。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#define _GNU_SOURCE#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/capability.h&gt;#define STACK_SIZE (1024 * 1024)static char child_stack[STACK_SIZE];char* const child_args[] = &#123; "/bin/bash", NULL&#125;;int pipefd[2];void set_map(char* file, int inside_id, int outside_id, int len) &#123; FILE* mapfd = fopen(file, "w+"); if (NULL == mapfd) &#123; printf("open file [%s] error.\n",file); return; &#125; fprintf(mapfd, "%d %d %d", inside_id, outside_id, len); fclose(mapfd);&#125;void set_uid_map(pid_t pid, int inside_id, int outside_id, int len) &#123; char file[256]; sprintf(file, "/proc/%d/uid_map", pid); set_map(file, inside_id, outside_id, len);&#125; void set_gid_map(pid_t pid, int inside_id, int outside_id, int len) &#123; char file[256]; sprintf(file, "/proc/%d/gid_map", pid); set_map(file, inside_id, outside_id, len);&#125;int child_main(void* args) &#123; printf("在子进程中!\n"); printf("eUID = %ld; eGID = %ld, UID=%ld, GID=%ld\n", (long) geteuid(), (long) getegid(), (long) getuid(), (long) getgid()); /* 等待父进程通知后再往下执行（进程间的同步） */ char ch; close(pipefd[1]); read(pipefd[0], &amp;ch, 1); execv(child_args[0], child_args); return 1;&#125;int main() &#123; const int gid=getgid(), uid=getuid(); printf("程序开始: \n"); printf("Parent: eUID = %ld; eGID = %ld, UID=%ld, GID=%ld\n", (long) geteuid(), (long) getegid(), (long) getuid(), (long) getgid()); pipe(pipefd); int child_pid = clone(child_main, child_stack+STACK_SIZE, CLONE_NEWUSER | SIGCHLD, NULL); set_uid_map(child_pid, 0, uid, 1); set_gid_map(child_pid, 0, gid, 1); /* 通知子进程 */ close(pipefd[1]); waitpid(child_pid, NULL, 0); printf("已退出\n"); return 0;&#125; 编译并运行后即可看到user已经变成了root。 12345678stack@ubuntu:~$ gcc userns.c -Wall -lcap -o userns &amp;&amp; ./userns程序开始: Parent: eUID = 1000; eGID = 1000, UID=1000, GID=1000在子进程中!eUID = 0; eGID = 65534, UID=0, GID=65534root@ubuntu:~# iduid=0(root) gid=65534(nogroup) groups=65534(nogroup)root@ubuntu:~# gid一直没有变过来，调试发现文件已经创建且写入函数返回值正确，这个问题有时间再调试吧。 至此，关于几个namespace的介绍已简单完成。 Linux源码分析基于kernel 3.10.105分析。 以上的实例大多基于clone来创建新的namespace，因此对namespace的分析基本就是分析clone函数有关namespace的部分。内核中clone实际也是调用的do_fork。 直接进入copy_process分析。 123456789101112131415161718192021222324static struct task_struct *copy_process(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *child_tidptr, struct pid *pid, int trace)&#123; int retval; struct task_struct *p; …… retval = -ENOMEM; p = dup_task_struct(current); …… /*CLONE_NEWUSER 相关*/ retval = copy_creds(p, clone_flags); if (retval &lt; 0) goto bad_fork_free; …… /*另外5个 namespace flag*/ retval = copy_namespaces(clone_flags, p); if (retval) goto bad_fork_cleanup_mm; ……&#125; copy_credscopy_creds的作用是复制或创建凭证信息。 1234567891011121314151617int copy_creds(struct task_struct *p, unsigned long clone_flags)&#123; struct cred *new; int ret; …… /*以current为模块，创建新的cred*/ new = prepare_creds(); if (!new) return -ENOMEM; if (clone_flags &amp; CLONE_NEWUSER) &#123; ret = create_user_ns(new); if (ret &lt; 0) goto error_put; &#125; ……&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546int create_user_ns(struct cred *new)&#123; /*parent_ns 为父进程的user namespace(一路copy)*/ struct user_namespace *ns, *parent_ns = new-&gt;user_ns; kuid_t owner = new-&gt;euid; kgid_t group = new-&gt;egid; int ret; if (parent_ns-&gt;level &gt; 32) return -EUSERS; /*判断当前进程文件系统和命名空间的 挂载点、根目录项对象是否相同。相同返回0*/ if (current_chrooted()) return -EPERM; /*创建者需要在父用户名空间中进行映射，否则我们将无法合理地告知创建user_namespace的用户空间。*/ if (!kuid_has_mapping(parent_ns, owner) || !kgid_has_mapping(parent_ns, group)) return -EPERM; /*slab层快速获取user namespace空间*/ ns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL); if (!ns) return -ENOMEM; /*分配INODE number*/ ret = proc_alloc_inum(&amp;ns-&gt;proc_inum); if (ret) &#123; kmem_cache_free(user_ns_cachep, ns); return ret; &#125; /*初始化ns数据*/ atomic_set(&amp;ns-&gt;count, 1); /* Leave the new-&gt;user_ns reference with the new user namespace. */ ns-&gt;parent = parent_ns; ns-&gt;level = parent_ns-&gt;level + 1; ns-&gt;owner = owner; ns-&gt;group = group; /* Inherit USERNS_SETGROUPS_ALLOWED from our parent */ mutex_lock(&amp;userns_state_mutex); ns-&gt;flags = parent_ns-&gt;flags; mutex_unlock(&amp;userns_state_mutex); /*使用与init相同的功能*/ set_cred_user_ns(new, ns); /*更新挂载规则*/ update_mnt_policy(ns); return 0;&#125; copy_namespaces核心结构nsproxy 12345678struct nsproxy &#123; atomic_t count; struct uts_namespace *uts_ns; struct ipc_namespace *ipc_ns; struct mnt_namespace *mnt_ns; struct pid_namespace *pid_ns; struct net *net_ns;&#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445int copy_namespaces(unsigned long flags, struct task_struct *tsk)&#123; struct nsproxy *old_ns = tsk-&gt;nsproxy; /*获取任务的客观上下文。 task_struct 中有两个上下文(context)： real_cred，客观上下文，当其他一些任务试图影响这个部分的时候，就会使用这些部分。 cred，主观上下文，一般在任务作用于另一个对象时使用，是文件，任务，键或其他。 通常，这两个指针相同。具体细节可参考 struct cred结构(include/linux/cred.h)*/ struct user_namespace *user_ns = task_cred_xxx(tsk, user_ns); struct nsproxy *new_ns; int err = 0; if (!old_ns) return 0; /*inc计数*/ get_nsproxy(old_ns); if (!(flags &amp; (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | CLONE_NEWNET))) return 0; if (!ns_capable(user_ns, CAP_SYS_ADMIN)) &#123; err = -EPERM; goto out; &#125; /* CLONE_NEWIPC，旧的IPC namespace中的信号量无法访问； 但是，CLONE_SYSVSEM 会共享父级信号量。 */ if ((flags &amp; CLONE_NEWIPC) &amp;&amp; (flags &amp; CLONE_SYSVSEM)) &#123; err = -EINVAL; goto out; &#125; /*为进程创建新的相关namespace*/ new_ns = create_new_namespaces(flags, tsk, user_ns, tsk-&gt;fs); if (IS_ERR(new_ns)) &#123; err = PTR_ERR(new_ns); goto out; &#125; tsk-&gt;nsproxy = new_ns;out: put_nsproxy(old_ns); return err;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*nsproxy 结构主要分配函数*/static struct nsproxy *create_new_namespaces(unsigned long flags, struct task_struct *tsk, struct user_namespace *user_ns, struct fs_struct *new_fs)&#123; struct nsproxy *new_nsp; int err; /*从slab层分配空间*/ new_nsp = create_nsproxy(); if (!new_nsp) return ERR_PTR(-ENOMEM); /*MNT namespace 拷贝（分配、初始化）*/ new_nsp-&gt;mnt_ns = copy_mnt_ns(flags, tsk-&gt;nsproxy-&gt;mnt_ns, user_ns, new_fs); if (IS_ERR(new_nsp-&gt;mnt_ns)) &#123; err = PTR_ERR(new_nsp-&gt;mnt_ns); goto out_ns; &#125; /*UTS namespace 拷贝（分配、初始化）*/ new_nsp-&gt;uts_ns = copy_utsname(flags, user_ns, tsk-&gt;nsproxy-&gt;uts_ns); if (IS_ERR(new_nsp-&gt;uts_ns)) &#123; err = PTR_ERR(new_nsp-&gt;uts_ns); goto out_uts; &#125; /*IPC namespace 拷贝（分配、初始化）*/ new_nsp-&gt;ipc_ns = copy_ipcs(flags, user_ns, tsk-&gt;nsproxy-&gt;ipc_ns); if (IS_ERR(new_nsp-&gt;ipc_ns)) &#123; err = PTR_ERR(new_nsp-&gt;ipc_ns); goto out_ipc; &#125; /*PID namespace 拷贝（分配、初始化）*/ new_nsp-&gt;pid_ns = copy_pid_ns(flags, user_ns, tsk-&gt;nsproxy-&gt;pid_ns); if (IS_ERR(new_nsp-&gt;pid_ns)) &#123; err = PTR_ERR(new_nsp-&gt;pid_ns); goto out_pid; &#125; /*NET namespace 拷贝（分配、初始化）*/ new_nsp-&gt;net_ns = copy_net_ns(flags, user_ns, tsk-&gt;nsproxy-&gt;net_ns); if (IS_ERR(new_nsp-&gt;net_ns)) &#123; err = PTR_ERR(new_nsp-&gt;net_ns); goto out_net; &#125; return new_nsp; ……&#125; copy_mnt_ns1234567891011121314151617struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns, struct user_namespace *user_ns, struct fs_struct *new_fs)&#123; struct mnt_namespace *new_ns; BUG_ON(!ns); /*原子增计数*/ get_mnt_ns(ns); if (!(flags &amp; CLONE_NEWNS)) return ns; new_ns = dup_mnt_ns(ns, user_ns, new_fs); /*原子减计数*/ put_mnt_ns(ns); return new_ns;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/*分配一个新的名称空间结构，并使用从传入的任务结构的名称空间复制的内容填充它。*/static struct mnt_namespace *dup_mnt_ns(struct mnt_namespace *mnt_ns, struct user_namespace *user_ns, struct fs_struct *fs)&#123; struct mnt_namespace *new_ns; struct vfsmount *rootmnt = NULL, *pwdmnt = NULL; struct mount *p, *q; struct mount *old = mnt_ns-&gt;root; struct mount *new; int copy_flags; /*分配新的mnt_namespace并进行初始化*/ new_ns = alloc_mnt_ns(user_ns); if (IS_ERR(new_ns)) return new_ns; /*加锁读写信号量namespace_sem*/ namespace_lock(); /* 复制mnt树形拓扑 */ copy_flags = CL_COPY_ALL | CL_EXPIRE; if (user_ns != mnt_ns-&gt;user_ns) copy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED; /*主要函数，暂不分析。TBD*/ new = copy_tree(old, old-&gt;mnt.mnt_root, copy_flags); if (IS_ERR(new)) &#123; namespace_unlock(); free_mnt_ns(new_ns); return ERR_CAST(new); &#125; new_ns-&gt;root = new; br_write_lock(&amp;vfsmount_lock); list_add_tail(&amp;new_ns-&gt;list, &amp;new-&gt;mnt_list); br_write_unlock(&amp;vfsmount_lock); /*切换tsk-&gt; fs - &gt; *元素并将新的vfsmount标记为属于新的命名空间。 我们已经获得了私有的fs_struct，所以不需要tsk-&gt; fs-&gt; lock。 */ p = old; q = new; while (p) &#123; q-&gt;mnt_ns = new_ns; if (fs) &#123; if (&amp;p-&gt;mnt == fs-&gt;root.mnt) &#123; fs-&gt;root.mnt = mntget(&amp;q-&gt;mnt); rootmnt = &amp;p-&gt;mnt; &#125; if (&amp;p-&gt;mnt == fs-&gt;pwd.mnt) &#123; fs-&gt;pwd.mnt = mntget(&amp;q-&gt;mnt); pwdmnt = &amp;p-&gt;mnt; &#125; &#125; p = next_mnt(p, old); q = next_mnt(q, new); &#125; namespace_unlock(); if (rootmnt) mntput(rootmnt); if (pwdmnt) mntput(pwdmnt); return new_ns;&#125; copy_utsname12345678910111213141516struct uts_namespace *copy_utsname(unsigned long flags, struct user_namespace *user_ns, struct uts_namespace *old_ns)&#123; struct uts_namespace *new_ns; BUG_ON(!old_ns); get_uts_ns(old_ns); if (!(flags &amp; CLONE_NEWUTS)) return old_ns; new_ns = clone_uts_ns(user_ns, old_ns); put_uts_ns(old_ns); return new_ns;&#125; 12345678910111213141516171819202122static struct uts_namespace *clone_uts_ns(struct user_namespace *user_ns, struct uts_namespace *old_ns)&#123; struct uts_namespace *ns; int err; /*分配空间*/ ns = create_uts_ns(); if (!ns) return ERR_PTR(-ENOMEM); /*分配新的inode number*/ err = proc_alloc_inum(&amp;ns-&gt;proc_inum); if (err) &#123; kfree(ns); return ERR_PTR(err); &#125; down_read(&amp;uts_sem); memcpy(&amp;ns-&gt;name, &amp;old_ns-&gt;name, sizeof(ns-&gt;name)); ns-&gt;user_ns = get_user_ns(user_ns); up_read(&amp;uts_sem); return ns;&#125; copy_ipcs1234567struct ipc_namespace *copy_ipcs(unsigned long flags, struct user_namespace *user_ns, struct ipc_namespace *ns)&#123; if (!(flags &amp; CLONE_NEWIPC)) return get_ipc_ns(ns); return create_ipc_ns(user_ns, ns);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940static struct ipc_namespace *create_ipc_ns(struct user_namespace *user_ns, struct ipc_namespace *old_ns)&#123; struct ipc_namespace *ns; int err; ns = kmalloc(sizeof(struct ipc_namespace), GFP_KERNEL); if (ns == NULL) return ERR_PTR(-ENOMEM); err = proc_alloc_inum(&amp;ns-&gt;proc_inum); if (err) &#123; kfree(ns); return ERR_PTR(err); &#125; atomic_set(&amp;ns-&gt;count, 1); /*消息队列初始化*/ err = mq_init_ns(ns); if (err) &#123; proc_free_inum(ns-&gt;proc_inum); kfree(ns); return ERR_PTR(err); &#125; atomic_inc(&amp;nr_ipc_ns); /*信号量初始化*/ sem_init_ns(ns); /*信号初始化？TBD*/ msg_init_ns(ns); /*共享内存初始化*/ shm_init_ns(ns); /*IPC 创建通知*/ ipcns_notify(IPCNS_CREATED); register_ipcns_notifier(ns); ns-&gt;user_ns = get_user_ns(user_ns); return ns;&#125; copy_pid_ns12345678910struct pid_namespace *copy_pid_ns(unsigned long flags, struct user_namespace *user_ns, struct pid_namespace *old_ns)&#123; if (!(flags &amp; CLONE_NEWPID)) return get_pid_ns(old_ns); /*当前pid namespace不是old_ns（之前copy的current），可能已经发生了进程切换*/ if (task_active_pid_ns(current) != old_ns) return ERR_PTR(-EINVAL); return create_pid_namespace(user_ns, old_ns);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns, struct pid_namespace *parent_pid_ns)&#123; struct pid_namespace *ns; unsigned int level = parent_pid_ns-&gt;level + 1; int i; int err; if (level &gt; MAX_PID_NS_LEVEL) &#123; err = -EINVAL; goto out; &#125; err = -ENOMEM; ns = kmem_cache_zalloc(pid_ns_cachep, GFP_KERNEL); if (ns == NULL) goto out; ns-&gt;pidmap[0].page = kzalloc(PAGE_SIZE, GFP_KERNEL); if (!ns-&gt;pidmap[0].page) goto out_free; ns-&gt;pid_cachep = create_pid_cachep(level + 1); if (ns-&gt;pid_cachep == NULL) goto out_free_map; err = proc_alloc_inum(&amp;ns-&gt;proc_inum); if (err) goto out_free_map; kref_init(&amp;ns-&gt;kref); ns-&gt;level = level; ns-&gt;parent = get_pid_ns(parent_pid_ns); ns-&gt;user_ns = get_user_ns(user_ns); ns-&gt;nr_hashed = PIDNS_HASH_ADDING; INIT_WORK(&amp;ns-&gt;proc_work, proc_cleanup_work); set_bit(0, ns-&gt;pidmap[0].page); atomic_set(&amp;ns-&gt;pidmap[0].nr_free, BITS_PER_PAGE - 1); for (i = 1; i &lt; PIDMAP_ENTRIES; i++) atomic_set(&amp;ns-&gt;pidmap[i].nr_free, BITS_PER_PAGE); return ns;out_free_map: kfree(ns-&gt;pidmap[0].page);out_free: kmem_cache_free(pid_ns_cachep, ns);out: return ERR_PTR(err);&#125; copy_net_ns123456789101112131415161718192021222324252627282930struct net *copy_net_ns(unsigned long flags, struct user_namespace *user_ns, struct net *old_net)&#123; struct net *net; int rv; if (!(flags &amp; CLONE_NEWNET)) return get_net(old_net); /*分配新的net结构*/ net = net_alloc(); if (!net) return ERR_PTR(-ENOMEM); get_user_ns(user_ns); mutex_lock(&amp;net_mutex); rv = setup_net(net, user_ns); if (rv == 0) &#123; rtnl_lock(); list_add_tail_rcu(&amp;net-&gt;list, &amp;net_namespace_list); rtnl_unlock(); &#125; mutex_unlock(&amp;net_mutex); if (rv &lt; 0) &#123; put_user_ns(user_ns); net_drop_ns(net); return ERR_PTR(rv); &#125; return net;&#125; 参考资料DOCKER背后的内核知识——NAMESPACE资源隔离 Linux Kernel Namespace实现: namespace API介绍 Linux Namespace和Cgroup DOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） how to find out namespace of a particular process?]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>namespace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 内核同步方式及原理]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-%E5%86%85%E6%A0%B8%E5%90%8C%E6%AD%A5%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[本文基于 linux kernel 3.10.105。 原子操作原子操作是其他同步方法的基石。原子操作，可以保证指令以原子的方式执行——执行过程不被打断。 内核提供了两组原子操作接口——一组针对整数，一组针对单独的位。在Linux支持的所有体系结构上都实现了这两组接口。 原子整数操作针对整数的原子操作只能对atomic_t类型的数据进行处理。引入特殊数据类型，主要是出于以下原因： 让原子函数只接收atomic_t类型的操作数，可以确保原子操作只与这种特殊类型数据一起使用。同时，这也保证了该类型的数据不会被传递给任何非原子函数。 使用atomic_t类型确保编译器不对相应的值进行访问优化——这点使得原子操作最终接收到正确的内存地址，而不只是一个别名（关于这一点，之前atomic_t定义是 volatile int counter，利用volatile读写时从内存取值。而现在定义只是 int counter，是在外部进行操作时指定volatile）。 最后，在不同体系结构上实现原子操作的时候，使用atomic_t可以屏蔽其间的差异。atomic_t类型定义如下： 123typedef struct &#123; int counter;&#125; atomic_t; 123Tips：由于历史原因，Linux支持的所有机器上atomic_t只能使用该类型数据的高24位，这个限制是因为SPARC体系结构上，Sparc32缺少“比较和交换”类型的指令，因此它使用低8位作为自旋锁以保证SMP安全。不过现在，32位的Sparc已经被转移到了“自旋锁散列表”的方案中，允许完整的32位计数器被实现。本质上，自旋锁阵列基于被操作的atomic_t的地址被索引，并且该锁保护原子操作。 Parisc使用相同的方案。 使用原子整形操作需要的声明都在&lt;asm/atomic.h&gt;中。 原子整数操作最常见的用途就是实现计数器。原子操作通常是内联函数，往往是通过内联汇编指令来实现。如果某个函数本来就是原子的，那么它往往被定义成一个宏。 例如，在大部分体系结构上，读取一个字本身就是一种原子操作，也就是说，在对有个字进行写入操作期间不可能完成对该字的读取。所以，atomic_read()只需返回atomic_t类型的整数值就可以了。 1234Tips：32/64位CPU是指CPU一次能够处理32/64位数据或指令。字 ：CPU进行数据处理时，一次存取、加工和传送的数据长度称为字（word）。一个字通常由一个或多个（一般是字节的整数位）字节构成。字长：一个字的长度。CPU在单位时间内(同一时间)能一次处理的二进制数的位数叫字长。 12345678910/** * atomic_read - read atomic variable * @v: pointer of type atomic_t * * Atomically reads the value of @v. */static inline int atomic_read(const atomic_t *v)&#123; return (*(volatile int *)&amp;(v)-&gt;counter);&#125; 下面分析几个函数的实现（x86架构）： atomic_add12345678910111213141516171819202122232425#define LOCK_PREFIX_HERE \ ".section .smp_locks,\"a\"\n" \ ".balign 4\n" \ ".long 671f - .\n" /* offset */ \ ".previous\n" \ "671:"#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "static inline void atomic_add(int i, atomic_t *v)&#123; asm volatile(LOCK_PREFIX "addl %1,%0" : "+m" (v-&gt;counter) : "ir" (i));&#125;asm可翻译为 ： .section .smp_locks,"a".balign 4.long 671f - ..previous671: lock; addl %1,%0 : "+m" (v-&gt;counter) : "ir" (i) 两步分析 LOCK_PREFIX 参考 GNU Assembler (GAS)手册 ，如果连接失效，可点击 此处，Documentation 部分。 123456.section name [, "flags"[, @type[,flag_specific_arguments]]]使用.section指令将以下代码组装到名为name的节中。flag "a" 表示 section is allocatable。此指令会替代当前的section（.text）。可以查看内核代码，以确定 smp_locks 的作用。简单来说是在单处理器中将锁变为空操作。 123.balign 4 将当前section的位数计数器加到4字节对齐。.long 671f - . .long（等于.int）是一个指令，告诉汇编程序在这里汇编一个32位的数量。当遇到的数据看起来不像任何已知的指令时，反汇编器通常发出这些数据。通常情况下，当存在一个文字池时就是这样，因为那些不包含机器码和数据的反汇编程序会打印它们包含的数据。.long指令对于.bss节无效。.previous 指令继续处理上一节。 12345678lock指令：多处理器环境中，"LOCK#"信号确保了处理器在信号有效时独占使用任何共享存储器。在所有的 X86 CPU 上都具有锁定一个特定内存地址的能力，当这个特定内存地址被锁定后，它就可以阻止其他的系统总线读取或修改这个内存地址。这种能力是通过 LOCK 指令前缀再加上下面的汇编指令来实现的。当使用 LOCK 指令前缀时，它会使 CPU 宣告一个 "LOCK#"信号，这样就能确保在多处理器系统或多线程竞争的环境下互斥地使用这个内存地址。当指令执行完毕，这个锁定动作也就会消失。LOCK前缀只能作为以下指令的前缀：ADD，ADC，AND，BTC，BTR，BTS，CMPXCHG，CMPXCH8B，CMPXCHG16B，DEC，INC，NEG，NOT，OR，SBB，SUB，XOR，XADD和XCHG。如果LOCK前缀与其中一个一起使用这些指令和源操作数是内存操作数，未定义的操作码异常（"#UD"）可能产生。如果LOCK前缀不与任何指令一起使用，也会产生未定义的操作码异常。注意：XCHG 和 XADD (以及所有以 'X' 开头的指令)都能够保证在多处理器系统下的原子操作，它们总会宣告一个 "LOCK#" 信号，而不管有没有 LOCK 前缀。 在X86平台上，CPU提供了在指令执行期间对总线加锁的手段。CPU上有一根引线#HLOCK pin连到北桥，如果汇编语言的程序中在一条指令前面加上前缀“LOCK”，经过汇编以后的机器代码就使CPU在执行这条指令的时候把#HLOCK pin的电位拉低，持续到这条指令结束时放开，从而把总线锁住，这样同一总线上别的CPU就暂时不能通过总线访问内存了，保证了这条指令在多处理器环境中的原子性。 addl指令 1addl %1,%0 : "+m" (v-&gt;counter) : "ir" (i) addl指令含义为 将目的操作数和源操作数求和，并将值写到目的操作数。 DOS/Windows 下的汇编语言，是 Intel 风格的。但在 Unix 和 Linux 系统中，更多采用的还是 AT&amp;T 格式。 AT&amp;T 和 Intel 格式中的源操作数和目标操作数的位置正好相反。在 Intel 汇编格式中，目标操作数在源操作数的左边；而在 AT&amp;T 汇编格式中，目标操作数在源操作数的右边。 GCC中asm格式为： 123456789101112131415asm [volatile] ( AssemblerTemplate : OutputOperands [ : InputOperands [ : Clobbers ] ])asm [volatile] goto ( AssemblerTemplate : : InputOperands : Clobbers : GotoLabels)OutputOperands（输出约束）必须以“=”（覆盖现有值的变量）或“+”（读写时）开始。在前缀之后，必须有一个或多个附加约束来描述值所在的位置。常见的约束条件包括“r”表示寄存器，“m”表示内存。当列出多个可能的位置（例如，“= rm”）时，编译器将根据当前上下文选择最有效的位置。InputOperands（输入约束）不能以'='或'+'开头。当列出多个可能位置（例如，“irm”）时，编译器将根据当前上下文选择最有效的位置。 %0、%1可以简单的理解为将 OutputOperands和InputOperands按顺序排列。“+m” 表示由内存中读取或直接写入内存。“i” 立即整型操作数。“r” 表示数据可以从寄存器读取或写入到寄存器。 因此，代码中的语句就比较好理解了。 atomic_sub 与 atomic_add 类似。 atomic_sub_and_test12345678910111213static inline int atomic_sub_and_test(int i, atomic_t *v)&#123; unsigned char c; asm volatile(LOCK_PREFIX "subl %2,%0; sete %1" : "+m" (v-&gt;counter), "=qm" (c) : "ir" (i) : "memory"); return c;&#125;subl i, v-&gt;counter; sete cv-&gt;counter -= i; 结果为0则设置c。'q'的约束和'r'相同。'memory' clobber 告诉编译器，汇编代码对输入和输出操作数（例如，访问输入参数之一指向的内存）以外的项目执行内存读取或写入操作。 atomic_add_return123456789101112131415161718192021222324252627282930static inline int atomic_add_return(int i, atomic_t *v)&#123; int __i;#ifdef CONFIG_M386 unsigned long flags; if (unlikely(boot_cpu_data.x86 &lt;= 3)) goto no_xadd;#endif /* Modern 486+ processor */ __i = i; /* xaddl i, v-&gt;counter i 与 v-&gt;counter 进行交换，之后 v-&gt;counter += i */ asm volatile(LOCK_PREFIX "xaddl %0, %1" : "+r" (i), "+m" (v-&gt;counter) : : "memory"); return i + __i;#ifdef CONFIG_M386no_xadd: /* Legacy 386 processor */ /*无 xaddl指令 则利用中断实现 中断函数实现之后在分析 todo */ raw_local_irq_save(flags); __i = atomic_read(v); atomic_set(v, i + __i); raw_local_irq_restore(flags); return i + __i;#endif&#125; 原子位操作内核也提供了一组针对位这一级数据进行操作的函数，定义在&lt;asm/bitops.h&gt;中。位操作函数是对普通的内存地址进行操作的，参数是一个指针和一个位号，第0位是给定地址的最低有效位。 set_bit12345678910111213141516171819202122232425#if __GNUC__ &lt; 4 || (__GNUC__ == 4 &amp;&amp; __GNUC_MINOR__ &lt; 1)/* Technically wrong, but this avoids compilation errors on some gcc versions. */#define BITOP_ADDR(x) "=m" (*(volatile long *) (x))#else#define BITOP_ADDR(x) "+m" (*(volatile long *) (x))#endif#define IS_IMMEDIATE(nr) (__builtin_constant_p(nr))#define CONST_MASK_ADDR(nr, addr) BITOP_ADDR((void *)(addr) + ((nr)&gt;&gt;3))#define CONST_MASK(nr) (1 &lt;&lt; ((nr) &amp; 7))static __always_inline voidset_bit(unsigned int nr, volatile unsigned long *addr)&#123; if (IS_IMMEDIATE(nr)) &#123; asm volatile(LOCK_PREFIX "orb %1,%0" : CONST_MASK_ADDR(nr, addr) : "iq" ((u8)CONST_MASK(nr)) : "memory"); &#125; else &#123; asm volatile(LOCK_PREFIX "bts %1,%0" : BITOP_ADDR(addr) : "Ir" (nr) : "memory"); &#125;&#125; GCC内置函数__builtin_constant_p用来检测值是否为常量。 BITOP_ADDR 是将传入的指针进行截取，只保留需要操作的字节。 CONST_MASK_ADDR 可理解为将 addr 以字节进行划分，根据 nr 找到需要操作的字节。 CONST_MASK 字节偏移量。 ‘orb’ 是 “位或”操作。’orb src, dest’，dest的第src位进行位或操作。 ‘bts’ 是 ‘bit test and set’，’bts src, dest’，将所选位（dest 的 src位）的值写入CF寄存器，然后将所选位 置1。 约束符’I’表示值1到8的范围。可参考此处。 clear_bit 与 set_bit 类似，不过将’orb’ 变为 ‘andb’(位与)，将’bts’ 变为 ‘btr’(bit test and reset)。 change_bit 使用了 ‘xorb’(位异或)和’btc’(bit test and complement，select bit &lt;- not (select bit))。 test_and_set_bit123456789static inline int test_and_set_bit(int nr, volatile unsigned long *addr)&#123; int oldbit; asm volatile(LOCK_PREFIX "bts %2,%1\n\t" "sbb %0,%0" : "=r" (oldbit), ADDR : "Ir" (nr) : "memory"); return oldbit;&#125; ‘bts’ 第一步就是将所选位的值写入 CF 寄存器。 ‘sbb’ 含义为 （DEST ← (DEST – (SRC + CF));），源目相同时，仅是将CF寄存器的值写入目的操作数。 其他原子位操作原理基本与这两种函数类似。 为方便起见内核还提供了一组与原子位操作对应的非原子位操作（无 ‘LOCK_PREFIX’ 锁操作）。非原子位操作函数名比原子位操作函数名前缀多两个下划线。如果已确定操作不需要原子性（已经用锁保护了自己的数据），那么这些非原子的操作可能会执行得更快些。 自旋锁Linux内核中最常见的锁是自旋锁（spin lock）。自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个被己经持有(即所谓的争用)的自旋锁，那么该线程就会一直进行忙循环——旋转——等待锁重新可用。要是锁未被争用，请求锁的执行线程便能立刻得到它，继续执行。在任意时间，自旋锁都可以防止多于一个的执行线程同时进入临界区。同一个锁可以用在多个位置。 一个被争用的自旋锁使得请求它的线程在等待锁重新可用时自旋(特别浪费处理器时间)，这种行为是自旋锁的要点。所以自旋锁不应该被长时间持有。事实上，这点正是使用自旋锁的初衷：在短期间内进行轻量级加锁。还可以采取另外的方式来处理对锁的争用:让请求线程睡眠，直到锁重新可用时再唤醒它。这样处理器就不必循环等待，可以去执行其他代码。这也会带来一定的开销—这里有两次明显的上下文切换，被阻塞的线程要换出和换入，与实现自旋锁的少数几行代码相比，上下文切换当然有较多的代码。因此，持有自旋锁的时间最好小于完成两次上下文切换的耗时。当然我们大多数人都不会无聊到去测量上下文切换的耗时，所以我们让持有自旋锁的时间应尽可能的短就可以。 自旋锁的实现和体系结构密切相关，代码往往通过汇编实现。与体系结构相关的代码定义在文件&lt;asm/spinlock.h&gt;中，实际需要用到的接口定义在文件&lt;linux/spinlock.h&gt;中。 自旋锁在同一时刻只能被一个执行线程持有，因此一个时刻只能有一个线程位于临界区内，这就为多处理器机器提供了防止并发访问所需的保护机制。注意在单处理器机器上，编译的时候并不会加入自旋锁。它仅仅被当做一个设置内核抢占机制是否被启用的开关。如果禁止内核抢占，那么在编译时自旋锁会被完全剔除出内核。 警告：自旋锁是不可递归的! 内核实现的自旋锁是不可递归的，这点不同于自旋锁在其他操作系统中的实现。如果你试图得到一个你正持有的锁，你必须自旋，等待你自己释放这个锁。但你处于自旋忙等待中，所以你永远没有机会释放锁，于是你被自己锁死了。千万小心自旋锁! 自旋锁可以使用在中断处理程序中。在中断处理程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断(在当前处理器上的中断请求)，否则，中断处理程序就会打断正持有锁的内核代码，有可能会试图去争用这个已经被持有的自旋锁。这样一来，中断处理程序就会自旋，等待该锁重新可用，但是锁的持有者在这个中断处理程序执行完毕前不可能运行。这正是我们在前面的内容中提到的双重请求死锁。注意，需要关闭的只是当前处理器上的中断。如果中断发生在不同的处理器上，即使中断处理程序在同一锁上自旋，也不会妨碍锁的持有者(在不同处理器上)最终释放锁。 调调试自旋锁 配置选项CONFIG_DEBUG_SPINLOCK为使用自旋锁的代码加入了许多调试检测手段。例如，激活了该选项，内核就会检查是否使用了未初始化的锁，是否在还没加锁的时候就要对锁执行开锁操作。在测试代码时，总是应该激活这个选项。如果需要进一步全程调试锁，还应该打开CONFIG_DEBUG_LOCK_ALLOC选项。 方法spin_lock_init123456789101112131415include/linux/spinlock.h/*通过返回rlock地址来检查传参是否是 spinlock_t *类型如果不是的话，会有错误或告警*/static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)&#123; return &amp;lock-&gt;rlock;&#125;#define spin_lock_init(_lock) \do &#123; \ spinlock_check(_lock); \ raw_spin_lock_init(&amp;(_lock)-&gt;rlock); \&#125; while (0) 1234567891011121314include/linux/spinlock.h/*开启自旋锁调试开关 todo*/#ifdef CONFIG_DEBUG_SPINLOCK extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name, struct lock_class_key *key);# define raw_spin_lock_init(lock) \do &#123; \ static struct lock_class_key __key; \ __raw_spin_lock_init((lock), #lock, &amp;__key); \&#125; while (0)#else // disable DEBUG# define raw_spin_lock_init(lock) \ do &#123; *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); &#125; while (0)#endif 1234567891011include/linux/spinlock_types.h#define __RAW_SPIN_LOCK_INITIALIZER(lockname) \ &#123; \ .raw_lock = __ARCH_SPIN_LOCK_UNLOCKED, \ /*todo*/ SPIN_DEBUG_INIT(lockname) \ SPIN_DEP_MAP_INIT(lockname) &#125;#define __RAW_SPIN_LOCK_UNLOCKED(lockname) \ (raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname) 123456/*分析x86架构*/arch/x86/include/asm/spinlock_types.htypedef struct arch_spinlock &#123; unsigned int slock;&#125; arch_spinlock_t;#define __ARCH_SPIN_LOCK_UNLOCKED &#123; 0 &#125; x86架构，初始化自旋锁，就是对最内层的lock置0。 spin_lock12345678include/linux/spinlock.hstatic inline void spin_lock(spinlock_t *lock)&#123; raw_spin_lock(&amp;lock-&gt;rlock);&#125;---&gt;&gt;&gt;include/linux/spinlock.h#define raw_spin_lock(lock) _raw_spin_lock(lock) 单处理器 123//单处理器 UPinclude/linux/spinlock_api_up.h#define _raw_spin_lock(lock) __LOCK(lock) 123include/linux/spinlock_api_up.h#define __LOCK(lock) \ do &#123; preempt_disable(); __acquire(lock); (void)(lock); &#125; while (0) 123456789/*当前线程preempt_count加1barrier()为内存屏障，以保证数据 顺序性*/#define preempt_disable() \do &#123; \ inc_preempt_count(); \ barrier(); \&#125; while (0) 123456/*Sparse 的 GCC 扩展，利用 __context__ 来对代码进行检查，参数x 的引用计数 +1*/include/linux/compiler.h# define __acquire(x) __context__(x,1)# define __release(x) __context__(x,-1) 单处理器时，并没有发生真正的锁定，所以内核唯一要做的就是保持抢占计数和irq标志，来抑制未使用的锁变量的编译器警告，并添加适当的检查器注释。 多处理器 123456//多处理器 SMPkernel/spinlock.cvoid __lockfunc _raw_spin_lock(raw_spinlock_t *lock)&#123; __raw_spin_lock(lock);&#125; 123456789include/linux/spinlock_api_smp.hstatic inline void __raw_spin_lock(raw_spinlock_t *lock)&#123; /*禁止内核抢占，本CPU生效*/ preempt_disable(); /*未定义锁调试时，函数为空。todo*/ spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);&#125; 1234567891011121314//提供锁的统计信息 todo#ifdef CONFIG_LOCK_STAT#define LOCK_CONTENDED(_lock, try, lock) \do &#123; \ if (!try(_lock)) &#123; \ lock_contended(&amp;(_lock)-&gt;dep_map, _RET_IP_); \ lock(_lock); \ &#125; \ lock_acquired(&amp;(_lock)-&gt;dep_map, _RET_IP_); \&#125; while (0)#else /* CONFIG_LOCK_STAT */#define LOCK_CONTENDED(_lock, try, lock) \ lock(_lock)#endif /* CONFIG_LOCK_STAT */ 1234567891011121314151617181920212223242526272829303132#ifdef CONFIG_DEBUG_SPINLOCK/*设置了CONFIG_DEBUG_SPINLOCK, 引用文件 include/linux/spinlock.h 中的函数。*/extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);#else/*不明白函数定义后加一个宏是什么意思？todo*/static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)&#123; __acquire(lock); arch_spin_lock(&amp;lock-&gt;raw_lock);&#125;---&gt;&gt;&gt;arch/x86/include/asm/spinlock.h //选择x86架构分析static __always_inline void arch_spin_lock(arch_spinlock_t *lock)&#123; __ticket_spin_lock(lock);&#125;---&gt;&gt;&gt;arch/x86/include/asm/spinlock.h/*CPU数量32位环境中默认是 3264位环境中默认是 5120*/#if (NR_CPUS &lt; 256)#define TICKET_SHIFT 8#else#define TICKET_SHIFT 16 32位 1234567891011121314151617static __always_inline void __ticket_spin_lock(arch_spinlock_t *lock)&#123; short inc = 0x0100; asm volatile ( LOCK_PREFIX "xaddw %w0, %1\n" "1:\t" "cmpb %h0, %b0\n\t" "je 2f\n\t" "rep ; nop\n\t" "movb %1, %b0\n\t" /* don't need lfence here, because loads are in-order */ "jmp 1b\n" "2:" : "+Q" (inc), "+m" (lock-&gt;slock) : : "memory", "cc");&#125; “xaddw %w0, %1\n” ‘xaddw ‘指令含义为 交换后求和，’xaddw SRC , DEST’—&gt;&gt;&gt; (TEMP ← SRC + DEST;SRC ← DEST;DEST ← TEMP;)，’%w0’ 在 GCC 编译器中表示 ax 寄存器，’%h0’ 代表 ax的高8位，即ah； ‘%b0’ 代表 ax的低8位，即al。 此时， ax寄存器中的值为 inc的值，即 0x0100。 那么，此句含义即为： xaddw %ax, lock-&gt;slock。操作之后，tmp = lock-&gt;slock + inc; inc = lock-&gt;slock;lock-&gt;slock = tmp; 锁初始化时，值为0。经过操作之后，lock-&gt;slock = 0x0100，%ax = 0。 ‘1:’ 定义一个标号。 “cmpb %h0, %b0\n\t” ‘cmpb’ 位比较。’cmpb %ah, %al’，由于 %ax为0，源目操作数相同。 “je 2f\n\t” 如果上一步比较结果为真（相同），则跳转到标号为’2’的地方，本段中’2’表示退出。 “rep ; nop\n\t” 此条指令与’pause’指令相同，用于不支持’pause’指令的汇编程序。在不支持超线程的处理器上，就像’nop’一样不做任何事，但在支持超线程的处理器上，它被用作向处理器提示正在执行spinloop以提高性能。 “movb %1, %b0\n\t” ‘movb SRC, DEST’—&gt;&gt;&gt; (DEST ← SRC)，此处为 %al ← lock-&gt;slock。 “jmp 1b\n” 跳到标志’1’。 “2:” 定义一个标号。 此段含义简单概括为，将锁的值赋给 ax ，然后比较 ax 的高8位和低8位，相同则跳出（锁已经有了新值，其高位已加1，但是低位没变），不同则代表锁已经执行过加锁这一步，那么进入循环，循环中是将 内存中锁的值赋给 al，然后继续比较。为什么仅赋值低8位呢 ？因为 unlock 是 低位加1。 因此，可理解为，lock 高位加1，就是加锁，低位加1就是解锁。解锁之后不在分析。 64位 1234567891011121314151617181920static __always_inline void __ticket_spin_lock(arch_spinlock_t *lock)&#123; int inc = 0x00010000; int tmp; asm volatile(LOCK_PREFIX "xaddl %0, %1\n" "movzwl %w0, %2\n\t" "shrl $16, %0\n\t" "1:\t" "cmpl %0, %2\n\t" "je 2f\n\t" "rep ; nop\n\t" "movzwl %1, %2\n\t" /* don't need lfence here, because loads are in-order */ "jmp 1b\n" "2:" : "+r" (inc), "+m" (lock-&gt;slock), "=&amp;r" (tmp) : : "memory", "cc");&#125; 64位与32位原理一样，不过32位是 高8位 和 低8位进行比较，而64位是 高16位 与 低16位 进行比较。下面仅仅说明几个指令的含义： “movzwl %w0, %2\n\t” 将 %w0 即 eax (64位) 的值赋值到 tmp，但是高16位用0填充，即 低16位赋给 tmp。 “shrl $16, %0\n\t” %0 逻辑右移 16位，即 %0 仅存 高16位。 之后流程和32位相同。 spin_lock_bh之后的函数直接分析SMP系统（x86）。 123456include/linux/spinlock.h#define raw_spin_lock_bh(lock) _raw_spin_lock_bh(lock)static inline void spin_lock_bh(spinlock_t *lock)&#123; raw_spin_lock_bh(&amp;lock-&gt;rlock);&#125; 12345kernel/spinlock.cvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)&#123; __raw_spin_lock_bh(lock);&#125; 12345678include/linux/spinlock_api_smp.hstatic inline void __raw_spin_lock_bh(raw_spinlock_t *lock)&#123; local_bh_disable(); preempt_disable(); spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);&#125; BH 和 非BH区别主要是增加了 函数 “local_bh_disable” ，看一下其定义： 1234567891011kernel/softirq.cvoid local_bh_disable(void)&#123; __local_bh_disable((unsigned long)__builtin_return_address(0));&#125;---&gt;&gt;&gt;static inline void __local_bh_disable(unsigned long ip)&#123; add_preempt_count(SOFTIRQ_OFFSET); barrier();&#125; “__builtin_return_address” 接收一个称为 level 的参数。这个参数定义希望获取返回地址的调用堆栈级别。例如，如果指定 level 为 0，那么就是请求当前函数的返回地址。如果指定 level 为 1，那么就是请求进行调用的函数的返回地址，依此类推。使用 __builtin_return_address捕捉返回地址，以便在以后进行跟踪时使用这个地址。 preempt_count 加 本地软中断偏移(SOFTIRQ_OFFSET)，之后可用 宏 ‘softirq_count()’ 进行判断是否已禁用软中断。 spin_lock_irqirq 锁是禁止了硬中断。过程代码不在赘述，直接到最后 多的 “raw_local_irq_disable()”函数： 12345678910arch/x86/include/asm/irqflags.hstatic inline void raw_local_irq_disable(void)&#123; native_irq_disable();&#125;---&gt;&gt;&gt;static inline void native_irq_disable(void)&#123; asm volatile("cli": : :"memory");&#125; x86架构直接调用 “CLI” 指令。大多数情况下，”CLI”会清除EFLAGS寄存器中的IF标志，并且不会影响其他标志。 清除IF标志会导致处理器忽略可屏蔽的外部中断。 spin_lock_irqsave12345678910#define spin_lock_irqsave(lock, flags) \do &#123; \ raw_spin_lock_irqsave(spinlock_check(lock), flags); \&#125; while (0)---&gt;&gt;&gt;#define raw_spin_lock_irqsave(lock, flags) \ do &#123; \ typecheck(unsigned long, flags); \ flags = _raw_spin_lock_irqsave(lock); \ &#125; while (0) 12345678include/linux/typecheck.h/*检查x是否为type类型*/#define typecheck(type,x) \(&#123; type __dummy; \ typeof(x) __dummy2; \ (void)(&amp;__dummy == &amp;__dummy2); \ 1; \&#125;) 1234567891011121314151617181920212223242526kernel/spinlock.cunsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)&#123; return __raw_spin_lock_irqsave(lock);&#125;---&gt;&gt;&gt;include/linux/spinlock_api_smp.hstatic inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)&#123; unsigned long flags; local_irq_save(flags); preempt_disable(); spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); /* * On lockdep we dont want the hand-coded irq-enable of * do_raw_spin_lock_flags() code, because lockdep assumes * that interrupts are not re-enabled during lock-acquire: */#ifdef CONFIG_LOCKDEP LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);#else do_raw_spin_lock_flags(lock, &amp;flags);#endif return flags;&#125; 仅分析 宏’local_irq_save()’ 及 函数’do_raw_spin_lock_flags()’： local_irq_save 123456789101112131415161718include/linux/irqflags.h#define local_irq_save(flags) \do &#123; \ typecheck(unsigned long, flags); \ raw_local_irq_save(flags); \ trace_hardirqs_off(); \&#125; while (0)---&gt;&gt;&gt;arch/x86/include/asm/irqflags.h#define raw_local_irq_save(flags) \ do &#123; (flags) = __raw_local_irq_save(); &#125; while (0)---&gt;&gt;&gt;static inline unsigned long __raw_local_irq_save(void)&#123; unsigned long flags = __raw_local_save_flags(); raw_local_irq_disable(); return flags;&#125; 12345678910111213141516static inline unsigned long __raw_local_save_flags(void)&#123; return native_save_fl();&#125;---&gt;&gt;&gt;static inline unsigned long native_save_fl(void)&#123; unsigned long flags; asm volatile("# __raw_save_flags\n\t" "pushf ; pop %0" : "=rm" (flags) : : "memory"); return flags;&#125; “# __raw_save_flags\n\t” 注释。 “pushf” 将eflags寄存器的内容入栈。 “pop %0” 栈顶内容载入 目的操作数中，此处为 flags。 1234567891011arch/x86/include/asm/irqflags.hstatic inline void raw_local_irq_disable(void)&#123; native_irq_disable();&#125;---&gt;&gt;&gt;static inline void native_irq_disable(void)&#123; /*CLI 指令前面已做介绍*/ asm volatile("cli": : :"memory");&#125; do_raw_spin_lock_flags 123456static inline voiddo_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)&#123; __acquire(lock); arch_spin_lock_flags(&amp;lock-&gt;raw_lock, *flags);&#125; 12345678arch/x86/include/asm/spinlock.hstatic __always_inline void arch_spin_lock_flags(arch_spinlock_t *lock, unsigned long flags)&#123; arch_spin_lock(lock);&#125;---&gt;&gt;&gt;arch_spin_lock() 之前已经分析过。 这些函数的对应函数都是其逆操作。 自旋锁和下半部由于下半部可以抢占进程上下文中的代码，所以当下半部和进程上下文共享数据时，必须对进程上下文中的共享数据进行保护，所以需要加锁的同时还要禁止下半部执行。同样，由于中断处理程序可以抢占下半部，所以如果中断处理程序和下半部共享数据，那么就必须在获取恰当的锁的同时还要禁止中断。 同类的tasklet不可能同时运行，所以对于同类tasklet中的共享数据不需要保护。但是当数据被两个不同种类的tasklet共享时，就需要在访问下半部中的数据前先获得一个普通的自旋锁。这里不需要禁止下半部，因为在同一个处理器上绝不会有tasklet相互抢占的情况。 对于软中断，无论是否同种类型，如果数据被软中断共享，那么它必须得到锁的保护。这是因为，即使是同种类型的两个软中断也可以同时运行在一个系统的多个处理器上。但是，同一处理器上的一个软中断绝不会抢占另一个软中断，因此，根本投必要禁止下半部。 读写自旋锁有时候，锁的用途明确的分为读取和写入两个场景。当更新(写入)链表时，不能有其他代码井发地写链表或从链表中读取数据，写操作要求完全互斥。另一方面，当对其检索(读取)链表时，只要其他程序不对链表进行写操作就行了。只要没有写操作，多个并发的读操作都是安全的。 当对某个数据结构的操作可以像这样被划分为读/写或者消费者/生产者两种类别时，类似读/写锁这样的机制就很有帮助了。为此，Linux内核提供了专门的读一写自旋锁。这种自旋锁为读和写分别提供了不同的锁。一个或多个读任务可以并发地持有读者锁;相反，用于写的锁最多只能被一个写任务持有，而且此时不能有并发的读操作。有时把读/写锁叫做共享/排斥锁，或者并发/排斥锁，因为这种锁以共亨(对读者而言)和排斥(对写者而言)的形式获得使用。 下面开始分析原理。 rwlock_init123include/linux/rwlock.h# define rwlock_init(lock) \ do &#123; *(lock) = __RW_LOCK_UNLOCKED(lock); &#125; while (0) 1234include/linux/rwlock_types.h#define __RW_LOCK_UNLOCKED(lockname) \ (rwlock_t) &#123; .raw_lock = __ARCH_RW_LOCK_UNLOCKED, \ RW_DEP_MAP_INIT(lockname) &#125; 12#define RW_LOCK_BIAS 0x01000000#define __ARCH_RW_LOCK_UNLOCKED &#123; RW_LOCK_BIAS &#125; 初始化结果为 将 ‘0x01000000’ 赋值给 raw_lock 。 read_lock123456789101112include/linux/rwlock.h#define read_lock(lock) _raw_read_lock(lock)---&gt;&gt;&gt;include/linux/rwlock_api_smp.h#define _raw_read_lock(lock) __raw_read_lock(lock)---&gt;&gt;&gt;static inline void __raw_read_lock(rwlock_t *lock)&#123; preempt_disable(); rwlock_acquire_read(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);&#125; 12345678910111213include/linux/rwlock.h# define do_raw_read_lock(rwlock) \ do &#123;__acquire(lock); arch_read_lock(&amp;(rwlock)-&gt;raw_lock); &#125; while (0)---&gt;&gt;&gt;arch/x86/include/asm/spinlock.hstatic inline void arch_read_lock(arch_rwlock_t *rw)&#123; asm volatile(LOCK_PREFIX " subl $1,(%0)\n\t" "jns 1f\n" "call __read_lock_failed\n\t" "1:\n" ::LOCK_PTR_REG (rw) : "memory");&#125; “ subl $1,(%0)\n\t” 目的操作数值减1。在 AT&amp;T 汇编格式中，用 ‘$’ 前缀表示一个立即操作数；而在 Intel 汇编格式中，立即数的表示不用带任何前缀。 “jns 1f\n” 指令 JNS 表示 ：如果符号位 (SF)不为1，就跳转。 “call __read_lock_failed\n\t” 调用符号 ‘__read_lock_failed’，此符号定义在文件”arch/x86/lib/semaphore_32.S”。 123456789101112131415ENTRY(__read_lock_failed) CFI_STARTPROC FRAME2: LOCK_PREFIX incl (%eax)1: rep; nop cmpl $1,(%eax) js 1b LOCK_PREFIX decl (%eax) js 2b ENDFRAME ret CFI_ENDPROC ENDPROC(__read_lock_failed) 1234567arch/x86/include/asm/dwarf2.h#define CFI_STARTPROC .cfi_startproc#define CFI_ENDPROC .cfi_endproc.cfi_startproc用于每个函数的开头，这些函数应该在.eh_frame中有一个入口。 它初始化一些内部数据结构。 用.cfi_endproc关闭函数。除非.cfi_startproc与参数"simple"一起使用，否则它还会发出一些与体系结构有关的初始CFI指令。 12345678910伪代码如下：2: incl (%eax); // eax 代表 lock ，因为之前减1没有加锁成功,所以先恢复原值。1: if(lock - 1 &lt; 0) // write lock 直接减去 0x01000000, 为0,也就是说 write locked则一直循环。 goto 1; decl lock; if(lock &lt; 0) // 如果小于0，则说明在decl 之前，又被write 把锁抢占了，那么从头开始 goto 2; return 读锁是减1，值不为负则加锁成功，因此最多可同时有’0x01000000’个读锁，完全足够。但是按照底层代码分析，即使加了读锁，写数据也是有可能的，这就需要内核开发人员必须能够分清需要读还是写。 write_lock123456789101112include/linux/rwlock.h#define write_lock(lock) _raw_write_lock(lock)---&gt;&gt;&gt;include/linux/rwlock_api_smp.h#define _raw_write_lock(lock) __raw_write_lock(lock)---&gt;&gt;&gt;static inline void __raw_write_lock(rwlock_t *lock)&#123; preempt_disable(); rwlock_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);&#125; 12345678910111213include/linux/rwlock.h# define do_raw_write_lock(rwlock) \ do &#123;__acquire(lock); arch_write_lock(&amp;(rwlock)-&gt;raw_lock); &#125; while (0)---&gt;&gt;&gt;arch/x86/include/asm/spinlock.hstatic inline void arch_write_lock(arch_rwlock_t *rw)&#123; asm volatile(LOCK_PREFIX " subl %1,(%0)\n\t" "jz 1f\n" "call __write_lock_failed\n\t" "1:\n" ::LOCK_PTR_REG (rw), "i" (RW_LOCK_BIAS) : "memory");&#125; 1234伪代码如下：if (0 != (rw-&gt;lock - 0x01000000)) call __write_lock_failedreturn 123456789101112131415ENTRY(__write_lock_failed) CFI_STARTPROC simple FRAME2: LOCK_PREFIX addl $ RW_LOCK_BIAS,(%eax)1: rep; nop cmpl $ RW_LOCK_BIAS,(%eax) jne 1b LOCK_PREFIX subl $ RW_LOCK_BIAS,(%eax) jnz 2b ENDFRAME ret CFI_ENDPROC ENDPROC(__write_lock_failed) write_lock 伪代码和 read_lock 类似，可试着自己分析一下。 read_lock_bh直接上最后的函数 12345678include/linux/rwlock_api_smp.hstatic inline void __raw_read_lock_bh(rwlock_t *lock)&#123; local_bh_disable(); preempt_disable(); rwlock_acquire_read(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);&#125; 函数 ‘local_bh_disable()’ 和 宏 ‘do_raw_read_lock’ 为核心，上面已经分析过。 write_lock_bh12345678include/linux/rwlock_api_smp.hstatic inline void __raw_write_lock_bh(rwlock_t *lock)&#123; local_bh_disable(); preempt_disable(); rwlock_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);&#125; read_lock_irq1234567static inline void __raw_read_lock_irq(rwlock_t *lock)&#123; local_irq_disable(); preempt_disable(); rwlock_acquire_read(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);&#125; write_lock_irq1234567static inline void __raw_write_lock_irq(rwlock_t *lock)&#123; local_irq_disable(); preempt_disable(); rwlock_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);&#125; 信号量Linux中的信号量是一种睡眠锁。如果有一个任务试图获得一个不可用(已经被占用)的信号量时，信号且会将其推进一个等待队列，然后让其睡眠。这时处理器能重获自由，从而去执行其他代码。当持有的信号量可用(被释放)后，处于等待队列中的那个任务将被唤醒，并获得该信号量。 由于争用信号量的进程在等待锁重新变为可用时会睡眠，所以信号量适用于锁会被长时间持有的情况。 相反，锁被短时间持有时，使用信号量就不太适宜了。因为睡眠、维护等待队列以及唤醒所花费的开销可能比锁被占用的全部时间还要一长。 由于执行线程在锁被争用时会睡眠，所以只能在进程上下文中才能获取信号量锁，因为在中断上下文中是不能进行调度的。 可以在持有信号量时去睡眠(当然你也可能并不需要睡眠)，因为当其他进程试图获得同一信号量时不会因此而死锁(因为该进程也只是去睡眠而已，而你最终会继续执行的)。 在占用信号量的同时不能占用自旋锁。因为在你等待信号量时可能会睡眠，而在持有自旋锁时是不允许睡眠的。 以上这些结论阐明了信号量和自旋锁在使用上的差异。 信号量可以同时允许任意数量的锁持有者，而自旋锁在一个时刻最多允许一个任务持有它。信号量同时允许的持有者数量可以在声明信号量时指定。这个值称为使用者数量(usage count)或简单地叫数量(count)。通常情况下，信号量和自旋锁一样，在一个时刻仅允许有一个锁持有者。这时计数等于1，这样的信号量被称为二值信号量或互斥信号量(因为它强制进行互斥)。另一方面，初始化时也可以把数量设置为大于1的非0值。这种情况，信号量被称为计数信号童(counting semaphone)，它允许在一个时刻至多有count个锁持有者。计数信号量不能用来进行强制互斥，因为它允许多个执行线程同时访问临界区。相反，这种信号量用来对特定代码加以限制，内核中使用它的机会不多。在使用信号量时，基本上用到的都是互斥信号量(计数等于1的信号量)。 信号量支持两个原子操作P()和V()，这两个名字来自荷兰语Proberen和Vershogen。前者叫做测试操作(字面意思是探查)，后者叫做增加操作。后来的系统把两种操作分别叫做down()和up()。 down()操作通过对信号量计数减1来请求获得一个信号量。如果结果是0或大于0，获得信号量锁，任务就可以进入临界区。如果结果是负数，任务会被放入等待队列，处理器执行其他任务。相反，当临界区中的操作完成后，up()操作用来释放信号量。如果在该信号量上的等待队列不为空，那么处于队列中等待的任务在被唤醒的同时会获得该信号量。 下面开始看代码 sema_init12345678910111213141516171819202122include/linux/semaphore.hstruct semaphore &#123; raw_spinlock_t lock; //原始锁，保护下面的两个数据 unsigned int count; //可用计数 struct list_head wait_list; //等待队列&#125;;static inline void sema_init(struct semaphore *sem, int val)&#123; static struct lock_class_key __key; /*核心函数*/ *sem = (struct semaphore) __SEMAPHORE_INITIALIZER(*sem, val); lockdep_init_map(&amp;sem-&gt;lock.dep_map, "semaphore-&gt;lock", &amp;__key, 0);&#125;---&gt;&gt;&gt;#define __SEMAPHORE_INITIALIZER(name, n) \&#123; \ .lock = __RAW_SPIN_LOCK_UNLOCKED((name).lock), \ .count = n, \ .wait_list = LIST_HEAD_INIT((name).wait_list), \&#125;---&gt;&gt;&gt;__RAW_SPIN_LOCK_UNLOCKED //之前分析过，这个是 lock 的初始化。 初始化仅仅是将结构体内的3个字段进行初始。 down1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586kernel/semaphore.cvoid down(struct semaphore *sem)&#123; unsigned long flags; /*加irqsave锁，防止上下文切换并保护数据*/ raw_spin_lock_irqsave(&amp;sem-&gt;lock, flags); /*大于0说明还有可用计数，仅仅减计数即可； likely-当条件成立时，可优化代码执行速度*/ if (likely(sem-&gt;count &gt; 0)) sem-&gt;count--; else __down(sem); raw_spin_unlock_irqrestore(&amp;sem-&gt;lock, flags);&#125;---&gt;&gt;&gt;static noinline void __sched __down(struct semaphore *sem)&#123; /* #define MAX_SCHEDULE_TIMEOUT LONG_MAX #define LONG_MAX ((long)(~0UL&gt;&gt;1)) */ __down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);&#125;---&gt;&gt;&gt;struct semaphore_waiter &#123; struct list_head list; struct task_struct *task; bool up;&#125;;static inline int __sched __down_common(struct semaphore *sem, long state, long timeout)&#123; /*task为当前进程描述符*/ struct task_struct *task = current; struct semaphore_waiter waiter; list_add_tail(&amp;waiter.list, &amp;sem-&gt;wait_list); waiter.task = task; waiter.up = false; for (;;) &#123; /*signal_pending_state分析见下面。*/ if (signal_pending_state(state, task)) goto interrupted; if (unlikely(timeout &lt;= 0)) goto timed_out; /*将当前任务设置为TASK_UNINTERRUPTIBLE状态*/ __set_task_state(task, state); /*unlock之后进行进程切换*/ raw_spin_unlock_irq(&amp;sem-&gt;lock); timeout = schedule_timeout(timeout); /*切换回来后重新加锁 判断切换期间是否有信号量释放，没有则继续使任务睡眠*/ raw_spin_lock_irq(&amp;sem-&gt;lock); if (waiter.up) return 0; &#125; timed_out: list_del(&amp;waiter.list); return -ETIME; interrupted: list_del(&amp;waiter.list); return -EINTR;&#125;---&gt;&gt;&gt;static inline int signal_pending_state(long state, struct task_struct *p)&#123; /*函数正确返回： 1. 不为 (TASK_INTERRUPTIBLE | TASK_WAKEKILL) 2. TASK_INTERRUPTIBLE 时，没有 要处理的信号 ---&gt;&gt;&gt; 信号会打断状态 3. TASK_WAKEKILL 时，没有 未处理的 KILL 信号 ---&gt;&gt;&gt; 信号会打断状态 */ /*若设置状态为 TASK_INTERRUPTIBLE | TASK_WAKEKILL(仅响应致命信号) 则继续，否则退出*/ if (!(state &amp; (TASK_INTERRUPTIBLE | TASK_WAKEKILL))) return 0; /*如果有未处理的信号，则继续，否则退出*/ if (!signal_pending(p)) return 0; /*若 state为 TASK_INTERRUPTIBLE 或 存在未处理的KILL信号，则返回 true*/ return (state &amp; TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);&#125; down_interruptible、down_killable、down_trylock和down_timeout，这几个函数不在分析，内部函数都分析过，只是状态或timeout重设而已。 up1234567891011121314151617181920212223242526void up(struct semaphore *sem)&#123; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;lock, flags); /*等待队列为空，说明没有进程等待此信号量，则增计数即可*/ if (likely(list_empty(&amp;sem-&gt;wait_list))) sem-&gt;count++; else __up(sem); raw_spin_unlock_irqrestore(&amp;sem-&gt;lock, flags);&#125;---&gt;&gt;&gt;static noinline void __sched __up(struct semaphore *sem)&#123; /*有进程等待此信号量，那么 1.从等待队列从找出第一个数据 2.从等待队列删除 3.将此队列up置为true--&gt;&gt;down 中的循环条件 4.唤醒函数*/ struct semaphore_waiter *waiter = list_first_entry(&amp;sem-&gt;wait_list, struct semaphore_waiter, list); list_del(&amp;waiter-&gt;list); waiter-&gt;up = true; wake_up_process(waiter-&gt;task);&#125; 读写信号量与自旋锁类似，信号量也可优化为读写信号量，直接开始分析代码： init_rwsem12345678910111213141516171819202122232425262728293031323334/*此处定义了 两种结构体，根据是否开启RWSEM选项而用不同结构体及实现*/#ifdef CONFIG_RWSEM_GENERIC_SPINLOCK //专用锁#include &lt;linux/rwsem-spinlock.h&gt; /* use a generic implementation */---&gt;&gt;&gt;struct rw_semaphore &#123; /* 0 为初始状态 &gt;0 表示有读者，数量为读者数量 -1 表示有写者 */ __s32 activity; raw_spinlock_t wait_lock; struct list_head wait_list;#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;;#else/* All arch specific implementations share the same struct */struct rw_semaphore &#123; long count; raw_spinlock_t wait_lock; struct list_head wait_list;#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;;#define init_rwsem(sem) \do &#123; \ static struct lock_class_key __key; \ \ __init_rwsem((sem), #sem, &amp;__key); \&#125; while (0) rwsem.c 1234567891011121314void __init_rwsem(struct rw_semaphore *sem, const char *name, struct lock_class_key *key)&#123;#ifdef CONFIG_DEBUG_LOCK_ALLOC /* * Make sure we are not reinitializing a held semaphore: */ debug_check_no_locks_freed((void *)sem, sizeof(*sem)); lockdep_init_map(&amp;sem-&gt;dep_map, name, key, 0);#endif sem-&gt;count = RWSEM_UNLOCKED_VALUE; raw_spin_lock_init(&amp;sem-&gt;wait_lock); INIT_LIST_HEAD(&amp;sem-&gt;wait_list);&#125; 和普通信号量相同的初始化。 rwsem-spinlock.c 1234567891011121314void __init_rwsem(struct rw_semaphore *sem, const char *name, struct lock_class_key *key)&#123;#ifdef CONFIG_DEBUG_LOCK_ALLOC /* * Make sure we are not reinitializing a held semaphore: */ debug_check_no_locks_freed((void *)sem, sizeof(*sem)); lockdep_init_map(&amp;sem-&gt;dep_map, name, key, 0);#endif sem-&gt;activity = 0; raw_spin_lock_init(&amp;sem-&gt;wait_lock); INIT_LIST_HEAD(&amp;sem-&gt;wait_list);&#125; down_read x86架构 普通实现 12345678910111213static inline void __down_read(struct rw_semaphore *sem)&#123; asm volatile("# beginning down_read\n\t" LOCK_PREFIX _ASM_INC "(%1)\n\t" /* adds 0x00000001 */ " jns 1f\n" " call call_rwsem_down_read_failed\n" "1:\n\t" "# ending down_read\n\t" : "+m" (sem-&gt;count) : "a" (sem) : "memory", "cc");&#125; 自加 1，结果为正则退出，为负则 跳转到函数 ‘call_rwsem_down_read_failed’，请参考之前的查找方式自己查找其实现。 专用锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445void __sched __down_read(struct rw_semaphore *sem)&#123; struct rwsem_waiter waiter; struct task_struct *tsk; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags); /*初始化状态 或 仅有读者则 自加1 退出*/ if (sem-&gt;activity &gt;= 0 &amp;&amp; list_empty(&amp;sem-&gt;wait_list)) &#123; /* granted */ sem-&gt;activity++; raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags); goto out; &#125; /*此处说明，有写者已经加锁*/ /*将任务设置为 TASK_UNINTERRUPTIBLE 状态*/ tsk = current; set_task_state(tsk, TASK_UNINTERRUPTIBLE); /*初始化 waiter 并将其加入 读写信号量 链表*/ waiter.task = tsk; waiter.type = RWSEM_WAITING_FOR_READ; /*增加当前进程使用计数 usage*/ get_task_struct(tsk); list_add_tail(&amp;waiter.list, &amp;sem-&gt;wait_list); /*解锁之后继续等待*/ raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags); /*循环等锁，进程切换再次判断。 waiter列表无任务则退出。 */ for (;;) &#123; if (!waiter.task) break; schedule(); set_task_state(tsk, TASK_UNINTERRUPTIBLE); &#125; tsk-&gt;state = TASK_RUNNING; out: ;&#125; 读者无锁应该等写者解锁：up_write。之后分析专用文件。 up_write1234enum rwsem_waiter_type &#123; RWSEM_WAITING_FOR_WRITE, RWSEM_WAITING_FOR_READ&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556void __up_write(struct rw_semaphore *sem)&#123; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags); /*写者解锁直接赋值为0 即可。若队列有等待进程则唤醒 1 为是否唤醒 写者标志*/ sem-&gt;activity = 0; if (!list_empty(&amp;sem-&gt;wait_list)) sem = __rwsem_do_wake(sem, 1); raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags);&#125;---&gt;&gt;&gt;static inline struct rw_semaphore *__rwsem_do_wake(struct rw_semaphore *sem, int wakewrite)&#123; struct rwsem_waiter *waiter; struct task_struct *tsk; int woken; /*等待队列中获取一个等待者*/ waiter = list_entry(sem-&gt;wait_list.next, struct rwsem_waiter, list); /*写者独占锁，则唤醒即退出*/ if (waiter-&gt;type == RWSEM_WAITING_FOR_WRITE) &#123; if (wakewrite) wake_up_process(waiter-&gt;task); goto out; &#125; /*唤醒全部读者 或者 仅唤醒 等待的写者之前的读者 这么做的原因是保证顺序性，防止在写者之后的读者读到旧的数据 */ woken = 0; do &#123; struct list_head *next = waiter-&gt;list.next; list_del(&amp;waiter-&gt;list); tsk = waiter-&gt;task; /*内存屏障，保证顺序性，防止 tsk为NULL*/ smp_mb(); waiter-&gt;task = NULL; wake_up_process(tsk); /*读的时候 get了一下*/ put_task_struct(tsk); woken++; if (next == &amp;sem-&gt;wait_list) break; waiter = list_entry(next, struct rwsem_waiter, list); &#125; while (waiter-&gt;type != RWSEM_WAITING_FOR_WRITE); /*增加唤醒的读者数量*/ sem-&gt;activity += woken; out: return sem;&#125; down_write1234567891011121314151617181920212223242526272829303132333435void __sched __down_write(struct rw_semaphore *sem)&#123; __down_write_nested(sem, 0);&#125;---&gt;&gt;&gt;void __sched __down_write_nested(struct rw_semaphore *sem, int subclass)&#123; struct rwsem_waiter waiter; struct task_struct *tsk; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags); /*先初始化一个结构体，不能放循环里*/ tsk = current; waiter.task = tsk; waiter.type = RWSEM_WAITING_FOR_WRITE; list_add_tail(&amp;waiter.list, &amp;sem-&gt;wait_list); for (;;) &#123; /*无人状态则加锁退出 此循环进行等锁*/ if (sem-&gt;activity == 0) break; set_task_state(tsk, TASK_UNINTERRUPTIBLE); raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags); schedule(); raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags); &#125; /* got the lock */ sem-&gt;activity = -1; list_del(&amp;waiter.list); raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags);&#125; up_read1234567891011121314151617181920212223void __up_read(struct rw_semaphore *sem)&#123; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags); if (--sem-&gt;activity == 0 &amp;&amp; !list_empty(&amp;sem-&gt;wait_list)) sem = __rwsem_wake_one_writer(sem); raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags);&#125;---&gt;&gt;&gt;/*读锁唤醒时，等待队列中肯定都是写进程*/ static inline struct rw_semaphore * __rwsem_wake_one_writer(struct rw_semaphore *sem) &#123; struct rwsem_waiter *waiter; waiter = list_entry(sem-&gt;wait_list.next, struct rwsem_waiter, list); wake_up_process(waiter-&gt;task); return sem; &#125; 互斥体多数用户使用信号量只使用计数1，把它作为一个互斥的排它锁。信号量用户通用且没多少使用限制，这使得信号量适合用于那些较复杂的、未明情况下的互斥访问，比如内核于用户空间复杂的交互行为。 但这也意味着简单的锁定而使用信号量不方便，并且信号量也缺乏强制的规则来行使任何形式的自动调试，即便受限的调试也不可能。为了找到一个更简单的睡眠锁，内核开发者们引入了互斥体（mutex）。 mutex在内核中对应数据结构体mutex，其行为和使用计数为1的信号量类似，但操作接口更简单，实现也更高效，而且使用限制更强。 1234567891011121314151617181920struct mutex &#123; /* 1: unlocked, 0: locked, negative: locked, possible waiters */ atomic_t count; spinlock_t wait_lock; struct list_head wait_list;#if defined(CONFIG_DEBUG_MUTEXES) || defined(CONFIG_SMP) /*多核架构 */ struct task_struct *owner;#endif#ifdef CONFIG_MUTEX_SPIN_ON_OWNER void *spin_mlock; /* Spinner MCS lock */#endif#ifdef CONFIG_DEBUG_MUTEXES const char *name; void *magic;#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; mutex_init12345678910111213141516171819# define mutex_init(mutex) \do &#123; \ static struct lock_class_key __key; \ __mutex_init((mutex), #mutex, &amp;__key); \&#125; while (0)---&gt;&gt;&gt;void__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)&#123; /*初始化字段*/ atomic_set(&amp;lock-&gt;count, 1); spin_lock_init(&amp;lock-&gt;wait_lock); INIT_LIST_HEAD(&amp;lock-&gt;wait_list); mutex_clear_owner(lock);#ifdef CONFIG_MUTEX_SPIN_ON_OWNER lock-&gt;spin_mlock = NULL;#endif debug_mutex_init(lock, name, key);&#125; mutex_lock12345678910111213141516void __sched mutex_lock(struct mutex *lock)&#123; /*不知道为什么需要睡一下*/ might_sleep(); __mutex_fastpath_lock(&amp;lock-&gt;count, __mutex_lock_slowpath); /*设置进程owner*/ mutex_set_owner(lock);&#125;/*__sched *//* Attach to any functions which should be ignored in wchan output. #define __sched __attribute__((__section__(".sched.text")))把带有__sched的函数放到.sched.text段。kernel有个waiting channel，如果用户空间的进程睡眠了，可以查到是停在内核空间哪个函数中等待的： cat "/proc/&lt;pid&gt;/wchan"显然，.sched.text段的代码是会被wchan忽略的，schedule这个函数是不会出现在wchan的结果中的。*/ 1234567891011121314151617181920x86架构/*count 自减1，结果不为负（1--&gt;0）则退出；否则，调用fail_fn函数*/#define __mutex_fastpath_lock(count, fail_fn) \do &#123; \ unsigned int dummy; \ \ typecheck(atomic_t *, count); \ typecheck_fn(void (*)(atomic_t *), fail_fn); \ \ asm volatile(LOCK_PREFIX " decl (%%eax)\n" \ " jns 1f \n" \ " call " #fail_fn "\n" \ "1:\n" \ : "=a" (dummy) \ : "a" (count) \ : "memory", "ecx", "edx"); \&#125; while (0) 123456789101112static __used noinline void __sched__mutex_lock_slowpath(atomic_t *lock_count)&#123; /*container_of，内核的巧妙设计，请阅读源码*/ struct mutex *lock = container_of(lock_count, struct mutex, count); __mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);&#125;/*# define __used __attribute__((__unused__))告诉编译器无论 GCC 是否发现这个函数的调用实例，都要使用这个函数。这对于从汇编代码中调用 C 函数有帮助。noinline 强制不内联*/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131static inline int __sched__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass, struct lockdep_map *nest_lock, unsigned long ip)&#123; struct task_struct *task = current; struct mutex_waiter waiter; unsigned long flags; /*禁止内核抢占*/ preempt_disable(); mutex_acquire_nest(&amp;lock-&gt;dep_map, subclass, 0, nest_lock, ip);#ifdef CONFIG_MUTEX_SPIN_ON_OWNER /* 当发现没有待处理的服务器并且锁所有者当前正在（不同的）CPU上运行时，尝试旋转获取（fastpath）。 理由是，如果锁主人正在运行，很可能很快就会解锁。 由于这需要锁所有者，而且这个互斥体实现不会在锁定字段中原子地跟踪所有者，所以需要非原子地跟踪它。 */ /*判断 进程描述符 on_cpu 位，以此判断是否在占用cpu(网上查询此标志释义不正确，可查代码自知) 在 cpu 上，认为会很快解锁，所以循环等待。 */ if (!mutex_can_spin_on_owner(lock)) goto slowpath; for (;;) &#123; struct task_struct *owner; struct mspin_node node; /*mspin_lock 旋转等待解锁，函数分析在下面*/ mspin_lock(MLOCK(lock), &amp;node); /*ACCESS_ONCE 保证字段是从内存获取*/ owner = ACCESS_ONCE(lock-&gt;owner); if (owner &amp;&amp; !mutex_spin_on_owner(lock, owner)) &#123; mspin_unlock(MLOCK(lock), &amp;node); break; &#125; if ((atomic_read(&amp;lock-&gt;count) == 1) &amp;&amp; (atomic_cmpxchg(&amp;lock-&gt;count, 1, 0) == 1)) &#123; lock_acquired(&amp;lock-&gt;dep_map, ip); mutex_set_owner(lock); mspin_unlock(MLOCK(lock), &amp;node); preempt_enable(); return 0; &#125; mspin_unlock(MLOCK(lock), &amp;node); /* * When there's no owner, we might have preempted between the * owner acquiring the lock and setting the owner field. If * we're an RT task that will live-lock because we won't let * the owner complete. */ if (!owner &amp;&amp; (need_resched() || rt_task(task))) break; /* * The cpu_relax() call is a compiler barrier which forces * everything in this loop to be re-loaded. We don't need * memory barriers as we'll eventually observe the right * values at the cost of a few extra spins. */ arch_mutex_cpu_relax(); &#125;slowpath:#endif spin_lock_mutex(&amp;lock-&gt;wait_lock, flags); debug_mutex_lock_common(lock, &amp;waiter); debug_mutex_add_waiter(lock, &amp;waiter, task_thread_info(task)); /* add waiting tasks to the end of the waitqueue (FIFO): */ list_add_tail(&amp;waiter.list, &amp;lock-&gt;wait_list); waiter.task = task; if (MUTEX_SHOW_NO_WAITER(lock) &amp;&amp; (atomic_xchg(&amp;lock-&gt;count, -1) == 1)) goto done; lock_contended(&amp;lock-&gt;dep_map, ip); for (;;) &#123; /* * Lets try to take the lock again - this is needed even if * we get here for the first time (shortly after failing to * acquire the lock), to make sure that we get a wakeup once * it's unlocked. Later on, if we sleep, this is the * operation that gives us the lock. We xchg it to -1, so * that when we release the lock, we properly wake up the * other waiters: */ if (MUTEX_SHOW_NO_WAITER(lock) &amp;&amp; (atomic_xchg(&amp;lock-&gt;count, -1) == 1)) break; /* * got a signal? (This code gets eliminated in the * TASK_UNINTERRUPTIBLE case.) */ if (unlikely(signal_pending_state(state, task))) &#123; mutex_remove_waiter(lock, &amp;waiter, task_thread_info(task)); mutex_release(&amp;lock-&gt;dep_map, 1, ip); spin_unlock_mutex(&amp;lock-&gt;wait_lock, flags); debug_mutex_free_waiter(&amp;waiter); preempt_enable(); return -EINTR; &#125; __set_task_state(task, state); /* didn't get the lock, go to sleep: */ spin_unlock_mutex(&amp;lock-&gt;wait_lock, flags); schedule_preempt_disabled(); spin_lock_mutex(&amp;lock-&gt;wait_lock, flags); &#125;done: lock_acquired(&amp;lock-&gt;dep_map, ip); /* got the lock - rejoice! */ mutex_remove_waiter(lock, &amp;waiter, current_thread_info()); mutex_set_owner(lock); /* set it to 0 if there are no waiters left: */ if (likely(list_empty(&amp;lock-&gt;wait_list))) atomic_set(&amp;lock-&gt;count, 0); spin_unlock_mutex(&amp;lock-&gt;wait_lock, flags); debug_mutex_free_waiter(&amp;waiter); preempt_enable(); return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071static noinlinevoid mspin_lock(struct mspin_node **lock, struct mspin_node *node)&#123; struct mspin_node *prev; /* Init node */ node-&gt;locked = 0; node-&gt;next = NULL; prev = xchg(lock, node); /* 可理解为 prev = lock; lock = node; node 无变化 */ if (likely(prev == NULL)) &#123; /* Lock acquired */ node-&gt;locked = 1; return; &#125; ACCESS_ONCE(prev-&gt;next) = node; smp_wmb(); /* 等待锁持有者放行 */ while (!ACCESS_ONCE(node-&gt;locked)) arch_mutex_cpu_relax(); // 执行 nop&#125;---&gt;&gt;&gt;x86#define xchg(ptr, v) __xchg_op((ptr), (v), xchg, "")---&gt;&gt;&gt;#define __xchg_op(ptr, arg, op, lock) \ (&#123; \ /* 定义返回值 __ret arg 在刚开始就已经赋给 __ret，也就是说不对 arg进行操作 lock 传参为""，xchg 指令有 lock 功能 xchg 释义为： TEMP ← DEST; DEST ← SRC; SRC ← TEMP; */ __typeof__ (*(ptr)) __ret = (arg); \ switch (sizeof(*(ptr))) &#123; \ // 1 case __X86_CASE_B: \ asm volatile (lock #op "b %b0, %1\n" \ : "+q" (__ret), "+m" (*(ptr)) \ : : "memory", "cc"); \ break; \ // 2 case __X86_CASE_W: \ asm volatile (lock #op "w %w0, %1\n" \ : "+r" (__ret), "+m" (*(ptr)) \ : : "memory", "cc"); \ break; \ // 4 case __X86_CASE_L: \ asm volatile (lock #op "l %0, %1\n" \ : "+r" (__ret), "+m" (*(ptr)) \ : : "memory", "cc"); \ break; \ // 8 case __X86_CASE_Q: \ asm volatile (lock #op "q %q0, %1\n" \ : "+r" (__ret), "+m" (*(ptr)) \ : : "memory", "cc"); \ break; \ default: \ __ ## op ## _wrong_size(); \ &#125; \ __ret; \ &#125;) mutex_unlockTODO 完成变量init_completion12345678910struct __wait_queue_head &#123; spinlock_t lock; struct list_head task_list;&#125;;typedef struct __wait_queue_head wait_queue_head_t;struct completion &#123; unsigned int done; wait_queue_head_t wait;&#125;; 1234567891011121314151617181920static inline void init_completion(struct completion *x)&#123; x-&gt;done = 0; init_waitqueue_head(&amp;x-&gt;wait);&#125;--&gt;&gt;#define init_waitqueue_head(q) \ do &#123; \ static struct lock_class_key __key; \ \ __init_waitqueue_head((q), #q, &amp;__key); \ &#125; while (0)--&gt;&gt;//init the spinlock and the listvoid __init_waitqueue_head(wait_queue_head_t *q, const char *name, struct lock_class_key *key)&#123; spin_lock_init(&amp;q-&gt;lock); lockdep_set_class_and_name(&amp;q-&gt;lock, key, name); INIT_LIST_HEAD(&amp;q-&gt;task_list);&#125; wait_for_completion123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172void __sched wait_for_completion(struct completion *x)&#123; wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);&#125;--&gt;&gt;static long __schedwait_for_common(struct completion *x, long timeout, int state)&#123; return __wait_for_common(x, schedule_timeout, timeout, state);&#125;--&gt;&gt;static inline long __sched__wait_for_common(struct completion *x, long (*action)(long), long timeout, int state)&#123; might_sleep(); spin_lock_irq(&amp;x-&gt;wait.lock); timeout = do_wait_for_common(x, action, timeout, state); spin_unlock_irq(&amp;x-&gt;wait.lock); return timeout;&#125;--&gt;&gt;static inline long __scheddo_wait_for_common(struct completion *x, long (*action)(long), long timeout, int state)&#123; if (!x-&gt;done) &#123; /* #define DECLARE_WAITQUEUE(name, tsk) \ wait_queue_t name = __WAITQUEUE_INITIALIZER(name, tsk) --&gt;&gt; #define __WAITQUEUE_INITIALIZER(name, tsk) &#123; \ .private = tsk, \ .func = default_wake_function, \ .task_list = &#123; NULL, NULL &#125; &#125; --&gt;&gt; int default_wake_function(wait_queue_t *curr, unsigned mode, int wake_flags, void *key) &#123; try_to_wake_up 改变状态成功则 return success = 1; return try_to_wake_up(curr-&gt;private, mode, wake_flags); &#125; */ /* 定义一个等待队列；更改flags 并加入到 完成变量的 等待队列中。 */ DECLARE_WAITQUEUE(wait, current); __add_wait_queue_tail_exclusive(&amp;x-&gt;wait, &amp;wait); do &#123; /* state = TASK_UNINTERRUPTIBLE; signal_pending_state 前面分析过 */ if (signal_pending_state(state, current)) &#123; timeout = -ERESTARTSYS; break; &#125; /*设置当前进程状态；之后解锁进行进程调度*/ __set_current_state(state); spin_unlock_irq(&amp;x-&gt;wait.lock); timeout = action(timeout); spin_lock_irq(&amp;x-&gt;wait.lock); &#125; while (!x-&gt;done &amp;&amp; timeout); __remove_wait_queue(&amp;x-&gt;wait, &amp;wait); if (!x-&gt;done) return timeout; &#125; x-&gt;done--; return timeout ?: 1;&#125; 可自己分析函数变体 wait_for_completion_*。 complete1234567891011121314151617181920212223242526void complete(struct completion *x)&#123; unsigned long flags; spin_lock_irqsave(&amp;x-&gt;wait.lock, flags); /*增加完成标志*/ x-&gt;done++; __wake_up_common(&amp;x-&gt;wait, TASK_NORMAL, 1, 0, NULL); spin_unlock_irqrestore(&amp;x-&gt;wait.lock, flags);&#125;--&gt;&gt;static void __wake_up_common(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, int wake_flags, void *key)&#123; wait_queue_t *curr, *next; /*循环列表中每一个 queue，第一个唤醒成功 则退出； 唤醒之后就到了 do_wait_for_common 中继续执行代码； 循环列表内部 条件从左至右，因此，成功 nr_exclusive个就退出*/ list_for_each_entry_safe(curr, next, &amp;q-&gt;task_list, task_list) &#123; unsigned flags = curr-&gt;flags; if (curr-&gt;func(curr, mode, wake_flags, key) &amp;&amp; (flags &amp; WQ_FLAG_EXCLUSIVE) &amp;&amp; !--nr_exclusive) break; &#125;&#125; 可以自己分析 函数变体 complete_all。 大内核锁（BKL）对整个内核加锁，现在已不在使用。 顺序锁顺序锁，简称seq锁，是在2.6版本内核中引入的一种新型锁。这种锁提供了一种很简单的机制，用于读写共享数据。实现这种锁主要依靠一个序列计数器。当有疑义的数据被写入时，会得到一个锁，并且序列值会增加。在读取数据之前和之后，序列号都被读取。如果读取的序列号值相同，说明在读操作进行的过程中没有被写操作打断过。此外，如果读取的值是偶数，那么久表明写操作没有发生（锁的初始值是0，写锁会使值成奇数，释放会使值变成偶数）。 初始化 DEFINE_SEQLOCKseq锁的基本结构为 12345/*一个计数器；一个自旋锁*/typedef struct &#123; struct seqcount seqcount; spinlock_t lock;&#125; seqlock_t; 123456789#define DEFINE_SEQLOCK(x) \ seqlock_t x = __SEQLOCK_UNLOCKED(x)--&gt;&gt;#define __SEQLOCK_UNLOCKED(lockname) \ &#123; \ /*#define SEQCNT_ZERO &#123; 0 &#125;*/ .seqcount = SEQCNT_ZERO, \ .lock = __SPIN_LOCK_UNLOCKED(lockname) \ &#125; 初始化即为 将计数值为0并初始化自旋锁。也可以利用宏 seqlock_init进行初始化。 write_seqlock123456789101112static inline void write_seqlock(seqlock_t *sl)&#123; /*先加锁，然后计数加一*/ spin_lock(&amp;sl-&gt;lock); write_seqcount_begin(&amp;sl-&gt;seqcount);&#125;--&gt;&gt;static inline void write_seqcount_begin(seqcount_t *s)&#123; s-&gt;sequence++; smp_wmb();&#125; write_sequnlock123456789101112static inline void write_sequnlock(seqlock_t *sl)&#123; /*计数加一 之后 解锁*/ write_seqcount_end(&amp;sl-&gt;seqcount); spin_unlock(&amp;sl-&gt;lock);&#125;--&gt;&gt;static inline void write_seqcount_end(seqcount_t *s)&#123; smp_wmb(); s-&gt;sequence++;&#125; 写的顺序锁基本无难度，下面举例看一下read的用法。 12345678910111213/* * Setup the device for a periodic tick */void tick_setup_periodic(struct clock_event_device *dev, int broadcast)&#123; …… do &#123; seq = read_seqbegin(&amp;jiffies_lock); next = tick_next_period; &#125; while (read_seqretry(&amp;jiffies_lock, seq)); ……&#125; read_seqbegin1234567891011121314151617181920212223242526static inline unsigned read_seqbegin(const seqlock_t *sl)&#123; return read_seqcount_begin(&amp;sl-&gt;seqcount);&#125;--&gt;&gt;static inline unsigned read_seqcount_begin(const seqcount_t *s)&#123; unsigned ret = __read_seqcount_begin(s); smp_rmb(); return ret;&#125;--&gt;&gt;static inline unsigned __read_seqcount_begin(const seqcount_t *s)&#123; unsigned ret;repeat: ret = ACCESS_ONCE(s-&gt;sequence); /*为真表示奇数，write已加锁*/ if (unlikely(ret &amp; 1)) &#123; cpu_relax(); goto repeat; &#125; /*返回之后，write又加锁了咋办？继续看*/ return ret;&#125; read_seqretry12345678910111213141516static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)&#123; return read_seqcount_retry(&amp;sl-&gt;seqcount, start);&#125;--&gt;&gt;static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)&#123; smp_rmb(); return __read_seqcount_retry(s, start);&#125;--&gt;&gt;static inline int __read_seqcount_retry(const seqcount_t *s, unsigned start)&#123; /*不相同表示读后又被写了，那么继续循环！*/ return unlikely(s-&gt;sequence != start);&#125; seq锁有助于提供一种非常轻量级和具有扩展性的外观。但是seq锁对写者更有利。seq锁在遇到如下需求时将是最理想的选择： 数据存在很多读者。 数据写者很少。 写者很少，但是希望写优先于读，而且不允许读者让写着饥饿。 数据简单，如简单结构，甚至是简单的整型——在某些场合，是不能使用原子量的。（啥场合暂时未遇到） jiffies中利用seq锁（函数 get_jiffies_64）。 禁止抢占由于内核是抢占性的，内核中的进程在任何时刻都可能停下来以便另一个具有更高优先权的进程运行。这意味着一个任务与被枪占的任务可能会在同一个临界区内运行。为了避免这种情况，内核抢占代码使用自旋锁作为非抢占区域的标记。如果一个自旋锁被持有，内核便不能进行抢占。因为内核抢占和SMP面对相同的并发问题，并且内核已经是SMP安全的（SMP-safe），所以，这种简单的变化使得内核也是抢占安全的（preempt-safe）。 实际中，某些情况并不需要自旋锁，但是仍然需要关闭内核抢占。最频繁出现的情况就是每个处理器上的数据。如果数据对每个处理器是唯一的，那么，这样的数据可能就不需要使用锁来保护，因为数据只能被一个处理器访问。如果自旋锁没有被持有，内核又是抢占式的，那么一个新调度的任务就可能访问同一个变量。 为了解决这个问题，可以通过preempt_disable()禁止内核抢占。这是一个可以嵌套调用的函数，可以调用任意次。每次调用都必须有一个相应的preempt_enable()调用。当最后一次preempt_enable()被调用后，内核抢占才重新启用。 抢占计数存放着被持有锁的数量和preempt_disable()的调用次数，如果计数是0，那么内核可以进行枪占；如果为1或更大的值，那么，内核就不会进行抢占。 preempt_disable() 和 preempt_enable()实现比较简单，此处不在分析。 顺序和屏障当处理多处理器之间或硬件设备之间的同步问题时，有时需要在你的程序代码中以指定的顺序发出读内存和写内存指令。在和硬件交互时，时常需要确保一个给定的读操作发生在其他读或写操作之前。另外，在多处理器上，可能需要按写数据的顺序读数据。但是编译器和处理器为了提高效率，可能对读和写程序排序（x86处理器不会这样做）。 不过，所有可能重新排序和写的处理器提供了机器指令来确保顺序要求。同样也可以指示编译器不要对给定点周围的指令序列进行重新排序。这些确保顺序的指令称为屏障（barriers）。 rmb()方法提供了一个“读”内存屏障，它确保跨越rmb()的载入动作不会发生重排序。 wmb()方法提供了一个“写”内存屏障，功能和rmb()类似，区别仅仅是它是针对存储而非载入——它确保跨越屏障的存储不发生重排序。 mb()方法既提供了读屏障也提供了写屏障。载入和存储动作都不会跨越屏障重排序。 read_barrier_depends()是rmb()的变种，它提供了一个读屏障，但是仅仅是针对后续读操作所依靠的那些载入。因为屏障后的读操作依赖于屏障前的读操作，因此该屏障确保屏障前的读操作在屏障后的读操作之前完成。 宏smp_rmb()、smp_wmb()、smp_mb()和smp_read_barrier_depends()提供了一个有用的优化。在SMP内核中它们被定义成常用的内存屏障，而在单处理机内核中，它们被定义成编译器的屏障。 barrier()方法可以防止编译器跨屏障对载入或存储操作进行优化。编译器不会重新组织存储或载入操作，而防止改变C代码的效果和现有数据的依赖关系。但是，它不知道在当前上下文之外会发生什么事。 注意，对于不同的体系结构，屏障的实际效果差别很大。 参考资料GNU Assembler (GAS)手册 What .long 0xXXXXXXXX stands for in asm? x86 Assembly Language Reference Manual lock指令 原子操作与 x86 上的 lock 指令前缀 Linux 内核 LOCK_PREFIX 的含义 X86 Assembly/Shift and Rotate Linux 汇编语言开发指南 学 Win32 汇编]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>完成变量</tag>
        <tag>信号量</tag>
        <tag>互斥体</tag>
        <tag>顺序锁</tag>
        <tag>内核屏障</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache和Buffer的区别]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FCache%E5%92%8CBuffer%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[Cache的核心作用是加快取用的速度。Buffer的核心作用是用来缓冲，缓和冲击。 Cache 是为了弥补高速设备和低速设备的鸿沟而引入的中间层，最终起到加快访问速度的作用。而 Buffer 的主要目的进行流量整形，把突发的大数量较小规模的 I/O 整理成平稳的小数量较大规模的 I/O，以减少响应次数。 缓存工作的原则，就是“引用的局部性”，这可以分为时间局部性和空间局部性。空间局部性是指CPU在某一时刻需要某个数据，那么很可能下一步就需要其附近的数据；时间局部性是指当某个数据被访问过一次之后，过不了多久时间就会被再一次访问。对于应用程序而言，不管是指令流还是数据流都会出现引用的局部性现象。 Cache（缓存）是系统两端处理速度不匹配时的一种折衷策略。因为CPU和memory之间的速度差异越来越大，所以人们充分利用数据的局部性（locality）特征，通过使用存储系统分级（memory hierarchy）的策略来减小这种差异带来的影响。 123456举例：假设某地发生了自然灾害（比如地震），居民缺衣少食，于是派救火车去给若干个居民点送水。救火车到达第一个居民点，开闸放水，老百姓就拿着盆盆罐罐来接水。假如说救火车在一个居民点停留100分钟放完了水，然后重新储水花半个小时，再开往下一个居民点。这样一个白天来来来回回的，也就是4-5个居民点。但我们想想，救火车是何等存在，如果把水龙头完全打开，其强大的水压能轻易冲上10层楼以上， 10分钟就可以把水全部放完。但因为居民是拿盆罐接水，100%打开水龙头那就是给人洗澡了，所以只能打开一小部分（比如10%的流量）。但这样就降低了放水的效率（只有原来的10%了），10分钟变100分钟。那么，我们是否能改进这个放水的过程，让救火车以最高效率放完水、尽快赶往下一个居民点呢？方法就是：在居民点建蓄水池。救火车把水放到蓄水池里，因为是以100%的效率放水，10分钟结束然后走人。居民再从蓄水池里一点一点的接水。我们分析一下这个例子，就可以知道Cache的含义了。救火车要给居民送水，居民要从救火车接水，就是说居民和救火车之间有交互，有联系。但救火车是“高速设备”，居民是“低速设备”，低速的居民跟不上高速的救火车，所以救火车被迫降低了放水速度以适应居民。为了避免这种情况，在救火车和居民之间多了一层“蓄水池（也就是Cache）”，它一方面以100%的高效和救火车打交道，另一方面以10%的低效和居民打交道，这就解放了救火车，让其以最高的效率运行，而不被低速的居民拖后腿，于是救火车只需要在一个居民点停留10分钟就可以了。 Buffer（缓冲区）是系统两端处理速度平衡（从长时间尺度上看）时使用的。它的引入是为了减小短期内突发I/O的影响，起到流量整形的作用。比如生产者——消费者问题，他们产生和消耗资源的速度大体接近，加一个buffer可以抵消掉资源刚产生/消耗时的突然变化。 123葡萄熟了，要用大卡车装葡萄运出去卖。但问题是，卡车距离葡萄园很远，于是就需要在葡萄园和卡车之间往返多次才能把卡车装满。往返所需要的时间就是I/O延迟，比如往返各15分钟，总共半小时。果园的姑娘采摘葡萄，难道是摘一串葡萄，就花15分钟跑到卡车旁边放进去么？如果是这样，100串葡萄就需要往返100次，所需要的时间就是100X15X2分钟，那是何其漫长!所以聪明的做法就是暂时把100串葡萄放入一个箩筐，再以箩筐为单位倒入卡车，一箩筐（100串）葡萄的往返延迟是半小时。由此可见，“箩筐”把输入参数暂时集中放在一起，不是“一条一条”，而是“一股脑”地提供给下家处理。这就大大降低了I/O的次数，也就降低了I/O延迟，提高了速度。这个箩筐，就是Buffer。 假定以后存储器访问变得跟CPU做计算一样快，cache就可以消失，但是buffer依然存在。 1TLB（Translation Lookaside Buffer，翻译后备缓冲器）其实是一个cache。 参考资料： Cache 和 Buffer 都是缓存，主要区别是什么？]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>Cache</tag>
        <tag>Buffer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 虚拟文件系统]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.html</url>
    <content type="text"><![CDATA[虚拟文件系统（VFS）作为内核子系统，为用户空间程序提供了文件和文件系统相关的接口。系统中所有文件系统不但依赖VFS共存，而且也依靠VFS系统协同工作。 VFS把各种不同的文件系统抽象后采用统一的方式进行操作。为了支持多文件系统，VFS提供了一个通用文件系统模型，该模型囊括了任何文件系统的常用功能集和行为。其定义了所有文件系统都支持的、基本的、概念上的接口和数据结构。 Unix 文件系统Unix使用了四种和文件系统相关的传统抽象概念：文件、目录项、索引节点和安装节点（mount point）。 从本质上讲，文件系统是特殊的数据分层存储结构，它包含文件、目录和相关的控制信息。在Unix中，文件系统被安装在一个特定的安装点上，该安装点在全局层次结构中被称作命名空间（Linux 将层次化概念引入到单个进程中，每个进程都指定一个唯一的命名空间。因为每个进程都会继承父进程的命名空间，所以所有进程往往都只有一个命名空间），所有已安装文件系统都作为根文件系统树的枝叶出现在系统中。 文件其实可以做一个有序字节串，第一个字节是文件的头，最后一个字节是文件的尾。每一个文件为了便于系统和用户识别，都被分配了一个便于理解的名字。 文件通过目录组织起来。文件目录好比一个文件夹，用来容纳相关文件。目录可以嵌套（包含其他目录），形成文件路径。路径中每一部分都被称作目录条目，统称为目录项（“/tmp/log/system”，根目录/，目录tmp、log和文件system都是目录条目）。在Unix中，目录属于普通文件。由于VFS把目录当做文件对待，所以可以对目录执行和文件相同的操作。 Unix系统将文件的相关信息和文件本身这两个概念加以区分，例如控制权限、大小、创建时间等信息。文件相关信息，也被称作文件的元数据，被存储在一个单独的数据结构——索引节点（inode，index node的缩写）。 这些信息和文件系统的控制信息密切相关，文件系统的控制信息存储在超级块——一种包含文件系统信息的数据结构。有时，把这些收集起来的信息称为文件系统数据元，它集单独文件信息和文件系统的信息于一身。 VFS对象及其数据结构VFS其实采用的是面向对象的设计思路，使用一组数据结构来代表通用文件对象。因为内核纯粹使用C代码实现，没有直接利用面向对象的语言，所以内核中的数据结构都使用C语言的结构体实现，而这些结构体包含数据的同时也包含操作这些数据的函数指针，其中的操作函数由具体文件系统实现。 VFS中有四个主要的对象类型，它们分别是： 超级块对象，它代表一个具体的已安装文件系统。 索引节点对象，它代表一个具体文件。 目录项对象，它代表一个目录项，是路径的一个组成部分。 文件对象，它代表由进程打开的文件。 注意，因为VFS将目录作为一个文件来处理，所以不存在目录对象。换句话说，目录项不同于目录，但目录却是另一种形式的文件。 每个主要对象中都包含一个操作对象，这些操作对象描述了内核针对主要对象可以使用的方法： super_operations 对象，其中包括内核针对特定文件系统所能调用的方法，比如write_inode()和sysc_fs()等方法。 inode_operations 对象，其中包括内核针对特定文件所能调用的方法，比如create()和link()等方法。 dentry_operations 对象，其中包括内核针对特定目录所能调用的方法，比如d_compare()和d_delete()等方法。 file_operations 对象，其中包括进程针对己打开文件所能调用的方法，比如read()和write()等方法。 操作对象作为一个结构体指针来实现，此结构体中包含指向操作其父对象的函数指针。对于其中许多方法来说，可以继承使用VFS提供的通用函数，如果通用函数提供的基本功能无法满足需要，那么就必须使用实际文件系统的独有方法填充这些函数指针，使其指向文件系统实例。 再次提醒，我们这里所说的对象就是指结构体。 VFS使用了大量结构体对象，它所包括的对象远远多于上面提到的这几种主要对象。比如每个注册的文件系统都由file_system_type结构体来表示，它描述了文件系统及其性能；另外，每一个安装点也都用vfsmount结构体表示，它包含的是安装点的相关信息，如位置和安装标志等。 之后介绍两个与进程相关的结构体，它们描述了文件系统以及和进程相关的文件，分别是fs_struct结构体和file结构体。 超级块对象各种文件系统都必须实现超级块对象，该对象用于存储文件系统的信息，通常对应于存放在磁盘特定扇区中的文件系统超级块或文件系统控制块。对于并非基于磁盘的文件系统（如基于内存的文件系统，比如sysfs），它们会在使用时创建超级块并将其保存到内存中。 超级块对象由super_block结构体表示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162struct super_block &#123; struct list_head s_list; /*指向所有超级块的链表*/ dev_t s_dev; /*设备标识符*/ unsigned char s_dirt; /*修改标志*/ unsigned char s_blocksize_bits; /*以位为单位的块大小*/ unsigned long s_blocksize; /*以字节为单位的块大小*/ loff_t s_maxbytes; /*文件大小上限*/ struct file_system_type *s_type; /*文件系统类型*/ const struct super_operations *s_op; /*超级块方法*/ const struct dquot_operations *dq_op; /*磁盘限额方法*/ const struct quotactl_ops *s_qcop; /*限额控制方法*/ const struct export_operations *s_export_op; /*导出方法*/ unsigned long s_flags; /*挂载标志*/ unsigned long s_magic; /*文件系统的幻数*/ struct dentry *s_root; /*目录挂载点*/ struct rw_semaphore s_umount; /*卸载信号量*/ struct mutex s_lock; /*超级块信号量*/ int s_count; /*超级块引用计数*/ atomic_t s_active; /*活动引用计数*/#ifdef CONFIG_SECURITY void *s_security; /*安全模块*/#endif const struct xattr_handler **s_xattr; /*扩展的属性操作*/ struct list_head s_inodes; /*索引节点对象列表*/ struct hlist_head s_anon; /*匿名目录项*/ struct list_head s_files; /*被分配的文件链表*/ /* s_dentry_lru and s_nr_dentry_unused are protected by dcache_lock */ struct list_head s_dentry_lru; /*未被使用目录项链表*/ int s_nr_dentry_unused; /*链表中未使用目录项的数目*/ struct block_device *s_bdev; /*相关的块设备*/ struct backing_dev_info *s_bdi; /*块设备信息*/ struct mtd_info *s_mtd; /*存储磁盘信息*/ struct list_head s_instances; /*该类型文件系统*/ struct quota_info s_dquot; /*限额相关选项*/ int s_frozen; /*frozen标志*/ wait_queue_head_t s_wait_unfrozen; /*冻结的等待队列*/ char s_id[32]; /*文本名*/ void *s_fs_info; /*文件系统特殊信息*/ fmode_t s_mode; /*安装权限*/ /* Granularity of c/m/atime in ns. Cannot be worse than a second */ u32 s_time_gran; /*时间戳粒度*/ /* * The next field is for VFS *only*. No filesystems have any business * even looking at it. You had been warned. */ struct mutex s_vfs_rename_mutex; /*重命名信号量*/ /* * Filesystem subtype. If non-empty the filesystem type field * in /proc/mounts will be "type.subtype" */ char *s_subtype; /*子类型名称*/ /* * Saved mount options for lazy filesystems using * generic_show_options() */ char *s_options; /*已安装选项*/&#125;; 创建、管理和撤销超级块对象的代码位于文件&lt;fs/super.c&gt;中。超级块对象通过alloc_super()函数创建并初始化。在文件系统安装时，文件系统会调用该函数以便从磁盘读取文件系统超级块，并将其信息填充到内存中的超级块对象中。 超级块操作超级块对象中最重要的一个域是s_op，它指向超级块的操作函数表。 123456789101112131415161718192021222324252627282930313233343536373839struct super_operations &#123; /*在给定的超级块下创建和初始化一个新的索引节点对象*/ struct inode *(*alloc_inode)(struct super_block *sb); /*释放给定的索引节点对象*/ void (*destroy_inode)(struct inode *); /*VFS在索引节点被修改（脏）时会调用此函数。日志文件系统（如ext3/ext4）执行该函数进行日志更新*/ void (*dirty_inode) (struct inode *); /*用于将给定的索引节点写入磁盘，wbc表示写入时的控制信息*/ int (*write_inode) (struct inode *, struct writeback_control *wbc); /*最后一个索引节点的引用被释放后，VFS调用该函数*/ void (*drop_inode) (struct inode *); /*用于从磁盘上删除给定的索引节点对象*/ void (*delete_inode) (struct inode *); /*卸载文件系统时由VFS调用，用来释放超级块。s_lock进行保护*/ void (*put_super) (struct super_block *); /*用给定的超级块更新磁盘上的超级块。VFS通过此函数对内存中的超级块和磁盘中的超级块进行同步。s_lock进行保护*/ void (*write_super) (struct super_block *); /*使文件系统的数据元与磁盘上的文件系统同步。wait指定操作是否同步。*/ int (*sync_fs)(struct super_block *sb, int wait); /*禁止对文件系统做改变，之后使用给定的超级块更新磁盘上的超级块。LVM（逻辑卷标管理）会调用该函数*/ int (*freeze_fs) (struct super_block *); /*解除锁定*/ int (*unfreeze_fs) (struct super_block *); /*获取目录项对象状态信息*/ int (*statfs) (struct dentry *, struct kstatfs *); /*指定新的安装选项重新安装文件系统时，VFS调用此函数。s_lock进行保护*/ int (*remount_fs) (struct super_block *, int *, char *); /*释放索引节点，并清空包含相关数据的所有页面*/ void (*clear_inode) (struct inode *); /*VFS调用该函数中断安装操作。网络文件系统使用，如NFS*/ void (*umount_begin) (struct super_block *); int (*show_options)(struct seq_file *, struct vfsmount *); int (*show_stats)(struct seq_file *, struct vfsmount *);#ifdef CONFIG_QUOTA ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t); ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);#endif int (*bdev_try_to_free_page)(struct super_block*, struct page*, gfp_t);&#125;; 该结构体中的每一项都是一个指向超级块操作函数的指针，超级块操作函数执行文件系统和索引节点的底层操作。其中的函数都是由VFS在进程上下文调用。除了dirty_inode()其他函数在必要时都可以阻塞。 索引节点对象索引节点对象包含了内核在操作文件或目录时需要的全部信息。对于Unix风格的文件系统来说，这些信息可以从磁盘索引节点直接读入。没有索引节点的文件系统通常将文件的描述信息作为文件的一部分来存放。这些文件系统与Unix风格的文件系统不同，没有将数据与控制信息分开存放。有些现代文件系统使用数据库来存储文件的数据。不管哪种情况、采用哪种方式，索引节点对象必须在内存中创建，以便于文件系统使用。 索引节点对象由inode结构体表示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465struct inode &#123; struct hlist_node i_hash; /*散列表*/ struct list_head i_list; /*索引节点链表*/ struct list_head i_sb_list; /*超级块链表*/ struct list_head i_dentry; /*目录项链表*/ unsigned long i_ino; /*索引号*/ atomic_t i_count; /*引用计数*/ unsigned int i_nlink; /*硬链接数*/ uid_t i_uid; /*使用者 ID*/ gid_t i_gid; /*组 ID*/ dev_t i_rdev; /*实际设备标识符*/ unsigned int i_blkbits; /*以位为单位的块大小*/ u64 i_version; /*版本号*/ loff_t i_size; /*以字节为单位的块大小*/#ifdef __NEED_I_SIZE_ORDERED seqcount_t i_size_seqcount; /*对i_size进行串行计数*/#endif struct timespec i_atime; /*最后访问时间*/ struct timespec i_mtime; /*最后修改时间*/ struct timespec i_ctime; /*创建时间*/ blkcnt_t i_blocks; /*文件的块数*/ unsigned short i_bytes; /*文件的字节数*/ umode_t i_mode; /*访问权限*/ spinlock_t i_lock; /*自旋锁 i_blocks, i_bytes, maybe i_size */ struct mutex i_mutex; /*互斥体*/ struct rw_semaphore i_alloc_sem; /*信号量*/ const struct inode_operations *i_op; /*索引节点操作表*/ const struct file_operations *i_fop; /*缺省的索引节点操作 former -&gt;i_op-&gt;default_file_ops */ struct super_block *i_sb; /*相关的超级块*/ struct file_lock *i_flock; /*文件锁链表*/ struct address_space *i_mapping; /*相关的地址映射*/ struct address_space i_data; /*设备地址映射*/#ifdef CONFIG_QUOTA struct dquot *i_dquot[MAXQUOTAS]; /*索引节点的磁盘限制*/#endif struct list_head i_devices; /*块设备链表*/ union &#123; struct pipe_inode_info *i_pipe; /*管道信息*/ struct block_device *i_bdev; /*块设备驱动*/ struct cdev *i_cdev; /*字符设备驱动*/ &#125;; __u32 i_generation; /**/#ifdef CONFIG_FSNOTIFY __u32 i_fsnotify_mask; /*目录通知掩码 all events this inode cares about */ struct hlist_head i_fsnotify_mark_entries; /* fsnotify mark entries */#endif#ifdef CONFIG_INOTIFY struct list_head inotify_watches; /*索引节点通知监测链表 watches on this inode */ struct mutex inotify_mutex; /*protects the watches list */#endif unsigned long i_state; /*状态*/ unsigned long dirtied_when; /*第一次弄脏数据的时间 jiffies of first dirtying */ unsigned int i_flags; /*文件系统标志*/ atomic_t i_writecount; /*写者计数*/#ifdef CONFIG_SECURITY void *i_security; /*安全模块*/#endif#ifdef CONFIG_FS_POSIX_ACL struct posix_acl *i_acl; struct posix_acl *i_default_acl;#endif void *i_private; /*fs私有指针 fs or device private pointer */&#125;; 一个索引节点代表文件系统中(索引节点仅当文件被访问时，才在内存中创建)的一个文件，它也可以是设备或管道这样的特殊文件。因此索引节点结构体中有一些和特殊文件相关的项，比如i_pipe项就指向一个代表有名管道的数据结构，i_bdev指向块设备结构体，i_cdev指向字符设备结构体。这三个指针被存放在一个公用体中，因为一个给定的索引节点每次只能表示三者之一(或三者均不)。 有时，某些文件系统可能并不能完整地包含索引节点结构体要求的所有信息。举个例子，有的文件系统可能并不记录文件的访问时间，这时，该文件系统就可以在实现中选择任意合适的办法来解决这个问题。它可以在i_atime中存储0，或者让i_atime等于i_mtime,或者只在内存中更新i_atime而不将其写回磁盘，或者由文件系统的实现者来决定。 索引节点操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849struct inode_operations &#123; /*VFS通过系统调用create()和open()来调用该函数，从而为dentry对象创建一个新的索引节点*/ int (*create) (struct inode *,struct dentry *,int, struct nameidata *); /*在特定目录中寻找索引节点，该索引节点要对应于dentry中给出的文件名*/ struct dentry * (*lookup) (struct inode *,struct dentry *, struct nameidata *); /*被系统调用link()调用，用来创建硬连接。名称由最后一个参数指定，连接对象是inode目录中当一个参数（目录项）所代表的文件*/ int (*link) (struct dentry *,struct inode *,struct dentry *); /*系统调用unlink调用*/ int (*unlink) (struct inode *,struct dentry *); /*系统调用symlink，创建符号连接（软连接）*/ int (*symlink) (struct inode *,struct dentry *,const char *); /*系统调用mkdir()调用，创建一个新目录*/ int (*mkdir) (struct inode *,struct dentry *,int); /*系统调用rmdir()调用，删除一个目录*/ int (*rmdir) (struct inode *,struct dentry *); /*系统调用mknod()调用，创建特殊文件（设备文件、命名管道或套接字）*/ int (*mknod) (struct inode *,struct dentry *,int,dev_t); /*VFS调用该函数来移动文件。前两个参数是旧路径，后两个是新路径*/ int (*rename) (struct inode *, struct dentry *, struct inode *, struct dentry *); /*系统调用readlink()调用，拷贝数据到特定的缓冲buffer（char *）中。数据来自dentry指定的符号连接，大小为int*/ int (*readlink) (struct dentry *, char __user *,int); /*VFS调用，从一个符号连接查找它指向的索引节点。由dentry指向的连接被解析，其结果存放在nameidata结构体中*/ void * (*follow_link) (struct dentry *, struct nameidata *); /*follow_link调用之后，VFS调用put_link进行清除工作*/ void (*put_link) (struct dentry *, struct nameidata *, void *); /*VFS调用，修改文件的大小。调用前inode的i_size必须设置为预期的大小*/ void (*truncate) (struct inode *); /*检查给定的inode所代表的文件是否允许特定的访问模式。允许返回零，否则返回负值的错误码。*/ int (*permission) (struct inode *, int); int (*check_acl)(struct inode *, int); /*notify_change()调用，在修改索引节点后，通知发生了“改变事件”*/ int (*setattr) (struct dentry *, struct iattr *); /*通知索引节点需要从磁盘中更新时，VFS会调用该函数。扩展属性允许key/value这样的一对值与文件相关联*/ int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat *); /*VFS调用，给dentry指定的文件设置扩展属性。属性名为第二个参数，值为第三个参数*/ int (*setxattr) (struct dentry *, const char *,const void *,size_t,int); /*VFS调用，获取给定文件的扩展属性（第二个参数）对应的数值*/ ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t); /*将特定文件的所有属性列表拷贝到一个缓冲列表中*/ ssize_t (*listxattr) (struct dentry *, char *, size_t); /*从给定文件中删除指定的属性*/ int (*removexattr) (struct dentry *, const char *); void (*truncate_range)(struct inode *, loff_t, loff_t); long (*fallocate)(struct inode *inode, int mode, loff_t offset, loff_t len); int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start, u64 len);&#125;; 目录项对象VFS把目录当作文件对待，所以在路径/bin/vi中，bin和vi都属于文件——bin是特殊的目录文件而vi是一个普通文件，路径中的每个组成部分都由一个索引节点对象表示。虽然它们可以统一由索引节点表示，但是VFS经常需要执行目录相关的操作，比如路径名查找等。路径名查找需要解析路径中的每一个组成部分，不但要确保它有效，而且还需要再进一步寻找路径中的下一个部分。 为了方便查找操作，VFS引入了目录项的概念。每个dentry代表路径中的一个特定部分。对前一个例子来说，/、bin和vi，都属于目录项对象。前两个是目录，最后一个是普通文件。必须明确一点:在路径中(包括普通文件在内)，每一个部分都是目录项对象。解析一个路径并遍历其分量绝非简单的演练，它是耗时的、常规的字符串比较过程，执行耗时、代码繁琐。目录项对象的引入使得这个过程更加简单。 目录项也可包括安装点。在路径/mnt/cdorm/foo中，构成元素/、mnt、cdorm和foo都属于目录项对象。VFS在执行目录操作时(如果需要的话)会现场创建目录项对象。 目录项对象由dentry结构体表示，定义在文件&lt;linux/dcache.h&gt;中。 1234567891011121314151617181920212223242526272829struct dentry &#123; atomic_t d_count; /*使用计数*/ unsigned int d_flags; /*目录项标识 protected by d_lock */ spinlock_t d_lock; /*单目录项锁 per dentry lock */ int d_mounted; /*是否挂载*/ struct inode *d_inode; /*相关联的索引节点 Where the name belongs to - NULL is * negative */ /* * The next three fields are touched by __d_lookup. Place them here * so they all fit in a cache line. */ struct hlist_node d_hash; /*散列表 lookup hash list */ struct dentry *d_parent; /*父目录的目录项对象 parent directory */ struct qstr d_name; /*目录项名称*/ struct list_head d_lru; /*未使用的链表 LRU list */ /* * d_child and d_rcu can share memory */ union &#123; struct list_head d_child; /*目录项内部形成的链表 child of parent list */ struct rcu_head d_rcu; /*rcu加锁*/ &#125; d_u; struct list_head d_subdirs; /*子目录链表 our children */ struct list_head d_alias; /*索引节点别名链表 inode alias list */ unsigned long d_time; /*重置时间 used by d_revalidate */ const struct dentry_operations *d_op; /*目录项操作指针*/ struct super_block *d_sb; /*文件的超级块 The root of the dentry tree */ void *d_fsdata; /*文件系统特有数据 fs-specific data */ unsigned char d_iname[DNAME_INLINE_LEN_MIN]; /*短文件名 small names */&#125;; 与前面的两个对象不同，目录项对象没有对应的磁盘数据结构，VFS根据字符串形式的路径名现场创建它。而且由于目录项对象并非真正保存在磁盘上，所以目录项结构体没有是非被修改的标志。 目录项状态目录项对象有三种有效状态：被使用、未被使用和负状态。 一个被使用的目录项对应一个有效的索引节点(即d_inode指向相应的索引节点)并且表明该对象存在一个或多个使用者(即d_count为正值)。一个目录项处于被使用状态，意味着它正被VFS使用并且指向有效的数据，因此不能被丢弃。 一个未被使用的目录项对应一个有效的索引节点，但是应指明VFS当前并未使用它(d_count为0)。该目录项对象仍然指向一个有效对象，而且被保留在缓存中以便需要时再使用它。由于该目录项不会过早地被撤销，所以以后再需要它时，不必重新创建，与未缓存的目录项相比，这样使路径查找更迅速。但如果要回收内存的话，可以撤销未使用的目录项。 一个负状态的目录项（无效目录项）。没有对应的有效索引节点（d_inode为NULL），因为索引节点已被删除了，或路径不再正确了，但是目录项仍然保留，以便快速解析以后的路径查询。 目录项对象释放后也可以保存到slab对象缓存中去。此时，任何VFS或文件系统代码都没有指向该目录项对象的有效引用。 目录项缓存如果VFS层遍历路径名中所有的元素并将它们逐个地解析成目录项对象，还要到达最深层目录，将是一件非常费力的工作，会浪费大量的时间。所以内核将目录项对象缓存在目录项缓存(简称dcache)中。 目录项缓存包括两个主要部分： “被使用的”目录项链表。该链表通过索引节点对象中的i_dentry项连接相关的索引节点，因为一个给定的索引节点可能有多个链接，所以就可能有多个目录项对象，因此用一个链表来连接它们。 “最近被使用的”双向链表。该链表含有未被使用的和负状态的日录项对象。由于该链总是在头部插入目录项，所以链头节点的数据总比链尾的数据要新。当内核必须通过删除节点项回收内存时，会从链尾删除节点项，因为尾部的节点最旧，所以它们在近期内再次被使用的可能性最小。 散列表和相应的散列函数用来快速地将给定路径解析为相关目录项对象。 散列表由数组dentry_hashtable表示，其中每一个元素都是一个指向具有相同键值的目录项对象链表的指针。数组的大小取决于系统中物理内存的大小。 实际的散列值由d_hash()函数计算，它是内核提供给文件系统的唯一的一个散列函数。 查找散列表要通过d_lookup()函数，如果该函数在dcache中发现了与其相匹配的目录项对象，则匹配的对象被返回；否则，返回NULL。 而dcache在一定意义上也提供对索引节点的缓存，也就是icache。和目录项对象相关的索引节点对象不会被释放，因为目录项会让相关索引节点的使用计数为正，这样就可以确保索引节点留在内存中。只要目录项被缓存，其相应的索引节点也就被缓存了。 因为文件访问呈现空间和时间的局部性，所以对目录项和索引节点进行缓存非常有益。文件访问有时间上的局部性，是因为程序可能会一次又一次地访问相同的文件。因此，当一个文件被访问时，所缓存的相关目录项和索引节点不久被命中的概率较高。文件访问具有空间的局部性是因为程序可能在同一个目录下访问多个文件，因此一个文件对应的目录项缓存后极有可能被命中，因为相关的文件可能在下次又被使用。 目录项操作dentry_operations结构体指明了VFS操作目录项的所有方法。 123456789101112131415struct dentry_operations &#123; /*判断目录对象是否有效。VFS从dcache中使用一个目录项时，会调用该函数。大部分文件系统将该方法置为NULL，因为它们认为dcache中的目录项对象总是有效的*/ int (*d_revalidate)(struct dentry *, struct nameidata *); /*为目录项生成散列值*/ int (*d_hash) (struct dentry *, struct qstr *); /*比较后两个参数的文件名。VFS默认操作，仅仅作字符串比较。对有些文件系统，如FAT，简单的字符串比较不能满足其需要。因为FAT文件系统不区分大小写，所以需要实现一种不区分大小写的字符串比较函数。dcache_lock锁保护*/ int (*d_compare) (struct dentry *, struct qstr *, struct qstr *); /*目录项对象的d_count为0时，VFS调用该函数。需要dcache_lock和目录项的d_lock同时保护*/ int (*d_delete)(struct dentry *); /*释放目录项对象*/ void (*d_release)(struct dentry *); /*目录项对象丢失了其相关的索引节点时（磁盘索引节点被删除了），VFS调用该函数。默认情况下，VFS会调用iput()函数释放索引节点。如果文件系统重载了该函数，那么除了执行此文件系统特殊的工作外，还必须调用iput()函数*/ void (*d_iput)(struct dentry *, struct inode *); char *(*d_dname)(struct dentry *, char *, int);&#125;; 文件对象VFS的最后一个主要对象是文件对象。文件对象表示进程已打开的文件。如果站在用户角度来看待VFS，文件对象会首先进人我们的视野。进程直接处理的是文件，而不是超级块、索引节点或目录项。 文件对象是已打开的文件在内存中的表示。该对象(不是物理文件)由相应的open()系统调用创建，由close()系统调用撤销，所有这些文件相关的调用实际上都是文件操作表中定义的方法。因为多个进程可以同时打开和操作同一个文件，所以同一个文件也可能存在多个对应的文件对象。文件对象仅仅在进程观点上代表已打开文件，它反过来指向目录项对象(反过来指向索引节点)，其实只有目录项对象才表示已打开的实际文件。虽然一个文件对应的文件对象不是唯一的，但对应的索引节点和目录项对象无疑是唯一的。 文件对象由fi1e结构体表示，定义在文件&lt;linux/fs.h中。 123456789101112131415161718192021222324252627282930313233343536struct file &#123; /* * fu_list becomes invalid after file_free is called and queued via * fu_rcuhead for RCU freeing */ union &#123; struct list_head fu_list; /*文件对象链表*/ struct rcu_head fu_rcuhead; /*释放之后的RCU链表*/ &#125; f_u; struct path f_path; /*包含目录项*/#define f_dentry f_path.dentry#define f_vfsmnt f_path.mnt const struct file_operations *f_op; /*文件对象操作表*/ spinlock_t f_lock; /*文件锁 f_ep_links, f_flags, no IRQ */ atomic_long_t f_count; /*文件对象的使用计数*/ unsigned int f_flags; /*打开文件时指定的标志*/ fmode_t f_mode; /*文件的访问模式*/ loff_t f_pos; /*文件当前位移量（文件指针）*/ struct fown_struct f_owner; /*owner通过消耗进行异步I/O数据的传送*/ const struct cred *f_cred; /*文件的信任状*/ struct file_ra_state f_ra; /*预读状态*/ u64 f_version; /*版本号*/#ifdef CONFIG_SECURITY void *f_security; /*安全模块*/#endif /* needed for tty driver, and maybe others */ void *private_data; /*tty设备驱动的钩子*/#ifdef CONFIG_EPOLL /* Used by fs/eventpoll.c to link all the hooks to this file */ struct list_head f_ep_links; /*事件池链表*/#endif /* #ifdef CONFIG_EPOLL */ struct address_space *f_mapping; /*页缓存映射*/#ifdef CONFIG_DEBUG_WRITECOUNT unsigned long f_mnt_write_state; /*调试状态*/#endif&#125;; 文件对象也没有对应的磁盘数据，所以在结构体中没有代表其对象是否被修改、是否需要写会磁盘的标志。文件对象通过f_dentry指向相关的目录项对象，目录项对象会指向相关的索引节点，索引节点会记录文件是否被修改。 文件操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253struct file_operations &#123; struct module *owner; /*更新偏移量指针*/ loff_t (*llseek) (struct file *, loff_t, int); /*从给定的loff_t偏移处读取size_t字节的数据到__user中，同时更新文件指针*/ ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); /*从给你的__user中取出size_t字节的数据，写入给定文件的loff_t偏移处，同时更新文件指针*/ ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); /*从kiocb描述的文件里，以同步方式读取long字节的数据到 iovec中*/ ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t); /*以同步方式从给定的iovec中取出long字节数据，写入由kiocb描述的文件中*/ ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t); /*返回目录列表中的下一个目录*/ int (*readdir) (struct file *, void *, filldir_t); /*该函数睡眠等待给定文件活动。由系统调用poll()调用它*/ unsigned int (*poll) (struct file *, struct poll_table_struct *); /*该函数用来给设备发送命令参数对。当文件是一个被打开的设备节点时，可以通过它进行设置操作*/ int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); /*与ioctl有类似的功能，只不过不需要调用者持有BKL。*/ long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); /*ioctl可移植变种*/ long (*compat_ioctl) (struct file *, unsigned int, unsigned long); /*将给定的文件映射到指定的地址空间上。*/ int (*mmap) (struct file *, struct vm_area_struct *); /*创建新的文件对象，并将它和相应的索引节点对象关联起来。*/ int (*open) (struct inode *, struct file *); /*打开的文件计数减少时，该函数被VFS调用*/ int (*flush) (struct file *, fl_owner_t id); /*文件的最后一个引用被注销时调用*/ int (*release) (struct inode *, struct file *); /*给定文件的所有缓存数据写会磁盘*/ int (*fsync) (struct file *, int datasync); /*将kiocb描述的文件所有缓存数据写回到磁盘*/ int (*aio_fsync) (struct kiocb *, int datasync); /*打开或关闭异步I/O的通告信号*/ int (*fasync) (int, struct file *, int); /*给指定文件上锁*/ int (*lock) (struct file *, int, struct file_lock *); /*从一个文件向另一个文件发送数据*/ ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); /*获取未使用的地址空间来映射给定的文件*/ unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); /*检查传递给fcntl()系统调用的flags的有效性。*/ int (*check_flags)(int); /**/ int (*flock) (struct file *, int, struct file_lock *); /**/ ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); /**/ ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); /**/ int (*setlease)(struct file *, long, struct file_lock **);&#125;; 和文件系统相关的数据结构内核还使用了另外一些标准数据结构来管理文件系统的其他相关数据。第一个对象是file_system_type，用来描述各种特定文件系统类型，比如ext3、ext4或UDF。第二个结构体是vfsmount，用来描述一个安装文件系统的实例。 因为Linux支持众多不同的文件系统，所以内核必须由一个特殊的结构来描述每种文件系统的功能和行为。file_system_type结构体被定义在&lt;linux/fs.h&gt; 123456789101112131415161718192021222324struct file_system_type &#123; const char *name; /*文件系统的名字*/ int fs_flags; /*文件系统类型标志*/ /*从磁盘读取超级块。安装文件系统时，在内存中组装超级块对象*/ int (*get_sb) (struct file_system_type *, int, const char *, void *, struct vfsmount *); /*终止访问超级块*/ void (*kill_sb) (struct super_block *); /*文件系统模块*/ struct module *owner; /*下一个文件系统类型*/ struct file_system_type * next; /*超级块对象链表*/ struct list_head fs_supers; /*以下字段运行时使锁生效*/ struct lock_class_key s_lock_key; struct lock_class_key s_umount_key; struct lock_class_key s_vfs_rename_key; struct lock_class_key i_lock_key; struct lock_class_key i_mutex_key; struct lock_class_key i_mutex_dir_key; struct lock_class_key i_alloc_sem_key;&#125;; 当文件系统被实际安装时，将有有个vfsmount结构体在安装点被创建。该结构体用来代表文件系统的的实例——代表一个安装点。 vfsmount结构被定义在&lt;linux/mount.h&gt;中 1234567891011121314151617181920212223242526272829303132333435struct vfsmount &#123; struct list_head mnt_hash; /*散列表*/ struct vfsmount *mnt_parent; /*父文件系统 fs we are mounted on */ struct dentry *mnt_mountpoint; /*安装点的目录项 dentry of mountpoint */ struct dentry *mnt_root; /*该文件系统的根目录项 root of the mounted tree */ struct super_block *mnt_sb; /*该文件系统的超级块 pointer to superblock */ struct list_head mnt_mounts; /*子文件系统链表 list of children, anchored here */ struct list_head mnt_child; /*子文件系统链表 and going through their mnt_child */ int mnt_flags; /*安装标志*/ /* 4 bytes hole on 64bits arches */ const char *mnt_devname; /*设备文件名 Name of device e.g. /dev/dsk/hda1 */ struct list_head mnt_list; /*描述符链表*/ struct list_head mnt_expire; /*在到期链表中的入口 link in fs-specific expiry list */ struct list_head mnt_share; /*共享安装链表中的入口 circular list of shared mounts */ struct list_head mnt_slave_list;/*从安装节点链表 list of slave mounts */ struct list_head mnt_slave; /*从安装链表中的入口 slave list entry */ struct vfsmount *mnt_master; /*从安装链表的主人 slave is on master-&gt;mnt_slave_list */ struct mnt_namespace *mnt_ns; /*相关的命名空间 containing namespace */ int mnt_id; /*安装标识符 mount identifier */ int mnt_group_id; /*组标识符 peer group identifier */ /* * We put mnt_count &amp; mnt_expiry_mark at the end of struct vfsmount * to let these frequently modified fields in a separate cache line * (so that reads of mnt_flags wont ping-pong on SMP machines) */ atomic_t mnt_count; /*使用计数*/ int mnt_expiry_mark; /*如果标记为到期，则值为真 true if marked for expiry */ int mnt_pinned; /*“钉住”进程计数*/ int mnt_ghosts; /*“镜像引用计数”*/#ifdef CONFIG_SMP int __percpu *mnt_writers;#else int mnt_writers; /*写者引用计数*/#endif&#125;; 理清文件系统和所有其他安装点间的关系，是维护所有安装点链表中最复杂的工作。所以，vfsmount结构体中维护的各种链表就是为了能够跟踪这些关联信息。 vfsmount结构还保存了在安装时指定的标志信息，该信息存储在mnt_flages域中。 标志 描述 MNT_NOSUID 禁止该文件系统的可执行文件设置setuid和setgid标志 MNT_MODEV 禁止访问该文件系统上的设备文件 MNT_NOEXEC 禁止执行该文件系统上的可执行文件 和进程相关的数据结构系统中的每一个进程都有自己的一组打开的文件。有三个数据结构将VFS层和系统的进程紧密联系在一起，它们分别是：file_struct、fs_struct和namespace结构体。 file_struct结构体定义在&lt;linux/fdtable.h&gt;中。该结构体由进程描述符中的files目录项指向。所有与单个进程（pre-process）相关的信息（如打开的文件及文件描述符）都包含在其中 12345678910111213141516struct files_struct &#123; /* * read mostly part */ atomic_t count; /*使用计数*/ struct fdtable *fdt; /*指向其他fd表的指针*/ struct fdtable fdtab; /*基fd表*/ /* * written part on a separate cache line in SMP */ spinlock_t file_lock ____cacheline_aligned_in_smp; /*单个文件的锁*/ int next_fd; /*缓存下一个可用的fd*/ struct embedded_fd_set close_on_exec_init; /*exec()时关闭的文件描述符表*/ struct embedded_fd_set open_fds_init; /*打开的文件描述符链表*/ struct file * fd_array[NR_OPEN_DEFAULT]; /*缺省的文件对象数组*/&#125;; fd_array数组指针指向已打开的文件对象。因为NR_OPEN_DEFAULT等于BITS_PER_LONG，在64位机器体系结构中这个宏的值为64，所以该数组可以容纳64个文件对象。如果一个进程所打开的文件对象超过64个，内核将分配一个新数组，井且将fdt指针指向它。所以对适当数量的文件对象的访问会执行得很快，因为它是对静态数组进行的操作：如果一个进程打开的文件数量过多，那么内核就需要建立新数组。所以如果系统中有大量的进程都要打开超过64个文件，为了优化性能，管理员可以适当增大NR_OPEN_DEFAULT的预定义值。 和进程相关的第二个结构体是fs_struct。结构由进程描述符的fs域指向。它包含文件系统和进程相关的信息，定义在文件&lt;linux/fs_struct.h&gt;中 1234567struct fs_struct &#123; int users; /*用户数目*/ rwlock_t lock; /*保护该结构的锁*/ int umask; /*掩码*/ int in_exec; /*当前正在执行的文件*/ struct path root, pwd; /*根目录和当前目录的路径*/&#125;; 最后一个相关结构体是namespace结构体，它定义在文件&lt;linux/mnt_namespace&gt;中，由进程描述符中的mnt_namespace域指向（现在应该是nsproxy，待确定）。 1234567struct mnt_namespace &#123; atomic_t count; /*引用计数*/ struct vfsmount * root; /*根目录安装节点对象*/ struct list_head list; /*安装点链表*/ wait_queue_head_t poll; /*轮询的等待队列*/ int event; /*事件计数*/&#125;; list域是连接已安装文件系统的双向链表，它包含的元素组成了全体命名空间。 上述这些数据结构都是通过进程描述符连接起来的。对多数进程来说，它们的描述符都指向唯一的file_struct和fs_struct结构体。但是，对于那些使用克隆标志CLONE_FILLS或CLOINE_FS创建的进程，会共享这两个结构体。 namespace结构体的使用方法却和前两种结构体完全不同，默认情况下，所有的进程共享同样的命名空间(也就是，它们都从相同的挂载表中看到同一个文件系统层次结构)。只有在进行clone()操作时使用CLONE_NEWS标志，才会给进程一个唯一的命名空间结构体的拷贝。因为大多数进程不提供这个标志，所有进程都继承其父进程的命名空间。因此，在大多数系统上只有一个命名空间，不过，CLONE_NEWS标志可以使这一功能失效。 文件系统数据结构体总结超级块是对一个文件系统的描述；索引节点是对一个文件物理属性的描述；而目录项是对一个文件逻辑属性的描述。除此之外，文件与进程之间的关系是由另外的数据结构来描述的。一个进程所处的位置是由fs_struct来描述的，而一个进程（或用户）打开的文件是由files_struct来描述的，而整个系统所打开的文件是由file结构来描述。 每个文件除了有一个索引节点inode数据结构外，还有一个目录项dentry数据结构。dentry 结构中有个d_inode指针指向相应的inode结构。 dentry结构代表的是逻辑意义上的文件，所描述的是文件逻辑上的属性，因此，目录项对象在磁盘上并没有对应的映像；而inode结构代表的是物理意义上的文件，记录的是物理上的属性，一个索引节点对象可能对应多个目录项对象。 各个结构关系图如下： 参考： 超级块对象、索引节点对象、文件对象及目录项对象的数据结构 文件系统中的对象总结及对目录项对象的重点理解]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 更改内核启动顺序]]></title>
    <url>%2FCentos-%E6%9B%B4%E6%94%B9%E5%86%85%E6%A0%B8%E5%90%AF%E5%8A%A8%E9%A1%BA%E5%BA%8F.html</url>
    <content type="text"><![CDATA[Centos 6.x 和 Centos 7.x的不同在于：前者使用grub引导，而后者使用grub2引导。关于内核升级请查看 《Centos 内核升级》 。 Centos 6.x修改 /etc/grub.conf 文件即可。 123default ：启动顺序。内核名称即是以title开始的行，从 0 开始计数。timeout ：系统启动时的等待时间。此时间段内可更改内核。title ：内核的标题名（启动时显示），可更改。 Centos 7.x系统中的可用内核在 /boot/grub2/grub.cfg 文件中，可使用下面的命令查看： 12345678910# cat /boot/grub2/grub.cfg |grep menuentryif [ x"$&#123;feature_menuentry_id&#125;" = xy ]; then menuentry_id_option="--id" menuentry_id_option=""export menuentry_id_optionmenuentry 'CentOS Linux (3.10.0-693.5.2.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-693.5.2.el7.x86_64-advanced-3e109aa3-f171-4614-ad07-c856f20f9d25' &#123;menuentry 'CentOS Linux (3.10.0-693.2.2.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-693.2.2.el7.x86_64-advanced-3e109aa3-f171-4614-ad07-c856f20f9d25' &#123;menuentry 'CentOS Linux (3.10.0-514.26.2.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-514.26.2.el7.x86_64-advanced-3e109aa3-f171-4614-ad07-c856f20f9d25' &#123;menuentry 'CentOS Linux (3.10.0-514.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-514.el7.x86_64-advanced-3e109aa3-f171-4614-ad07-c856f20f9d25' &#123;menuentry 'CentOS Linux (0-rescue-b7ab9add9a761df2e33b16b2038dbf9c) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-0-rescue-b7ab9add9a761df2e33b16b2038dbf9c-advanced-3e109aa3-f171-4614-ad07-c856f20f9d25' &#123; 利用命令 grub2-set-default 来修改启动顺序 或者 根据命令原理来修改文件 /etc/default/grub。 1# grub2-set-default 'CentOS Linux (3.10.0-514.26.2.el7.x86_64) 7 (Core)' 查看是否生效： 12# grub2-editenv listsaved_entry=CentOS Linux (3.10.0-514.26.2.el7.x86_64) 7 (Core) 设置生效后需要重启设备才能真正应用。]]></content>
      <tags>
        <tag>Centos</tag>
        <tag>启动顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack镜像导出]]></title>
    <url>%2FOpenstack%2Fopenstack%E9%95%9C%E5%83%8F%E5%AF%BC%E5%87%BA.html</url>
    <content type="text"><![CDATA[查询镜像信息123456789101112131415161718192021222324252627282930313233343536373839# glance image-show 958aba73-cddd-46e6-ae3c-e90ef0f483f1+------------------+----------------------------------------------------------------------------------+| Property | Value |+------------------+----------------------------------------------------------------------------------+| checksum | ecb6ae37e2c8ac57d4f89177a9b216a4 || container_format | bare || created_at | 2017-11-08T08:57:15Z || direct_url | rbd://05777380-b983-11e7-b6df-525400017028/images/958aba73-cddd-46e6-ae3c- || | e90ef0f483f1/snap || disk_format | qcow2 || id | 958aba73-cddd-46e6-ae3c-e90ef0f483f1 || locations | [&#123;"url": "rbd://05777380-b983-11e7-b6df-525400017028/images/958aba73-cddd-46e6 || | -ae3c-e90ef0f483f1/snap", "metadata": &#123;&#125;&#125;] || min_disk | 0 || min_ram | 0 || name | test-image || owner | a0e2e386e12c4635b2676b7fdd36993e || protected | False || size | 3999126016 || status | active || tags | [] || updated_at | 2017-11-08T09:03:02Z || virtual_size | None || visibility | public |+------------------+----------------------------------------------------------------------------------+# rbd info images/958aba73-cddd-46e6-ae3c-e90ef0f483f1rbd image '958aba73-cddd-46e6-ae3c-e90ef0f483f1': size 3813 MB in 960 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.fe2087216567e2 format: 2 features: layering, striping flags: stripe unit: 512 kB stripe count: 16# rbd snap ls images/958aba73-cddd-46e6-ae3c-e90ef0f483f1SNAPID NAME SIZE 30 snap 3813 MB 镜像导出123456789101112131415# rbd export images/958aba73-cddd-46e6-ae3c-e90ef0f483f1@snap test-image.qcow2 Exporting image: 100% complete...done.# file test-image.qcow2 test-image.qcow2: QEMU QCOW Image (v3), 10737418240 bytes# qemu-img info test-image.qcow2 image: test-image.qcow2file format: qcow2virtual size: 10G (10737418240 bytes)disk size: 3.7Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false 格式转换123456# qemu-img convert -O raw test-image.qcow2 test-image.raw# qemu-img info test-image.raw image: test-image.rawfile format: rawvirtual size: 10G (10737418240 bytes)disk size: 4.5G 至此，导出完成。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>镜像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack镜像制作]]></title>
    <url>%2FOpenstack%2Fopenstack%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C.html</url>
    <content type="text"><![CDATA[本文介绍如何将一个iso镜像制作为openstack启动镜像。iso镜像以 CentOS-6.9-i386-minimal.iso 为例，其他centos镜像可去官网下载。 环境准备制作环境为物理机或虚拟机，但需要需要cpu支持intel VT等硬件虚拟化功能。最好与制作镜像系统的系统，版本可不一样，这个非硬性要求，凭个人习惯。 判断是否支持虚拟化功能命令为： 1# cat /proc/cpuinfo | egrep 'vmx|svm' 安装必要软件1# yum install -y libvirt qemu-kvm virt-install bridge-utils qemu-img virt-manager libguestfs 启动虚拟环境1# systemctl start libvirtd 制作镜像创建虚拟机分配虚拟空间1# qemu-img create -f raw centOS-6.9.raw 10G 启动虚拟机1# virt-install --virt-type kvm --name centos-6.9 --memory 2048 --vcpus=2 --disk centOS-6.9.raw,format=raw --graphics vnc,listen=0.0.0.0 --noautoconsole --os-type=linux --os-variant=rhel6 --network network=default --extra-args='console=tty0 console=ttyS0,115200n8 serial' --location=/var/lib/libvirt/images/centOS-6.9.iso 参数可能因命令版本不同而不一致。 如果需要联网，那么需要配置网络，例如： 1--network network=default libvirtd启动时，会默认创建接口 ‘virbr0’，此接口就是 虚拟默认网络’default’的默认网关，其地址为’192.168.122.1’，在其上创建了许多NAT规则，以确保虚拟机可以连同外网。如下图 其他网络类型及设置请参考命令帮助。 配置了 ‘console’可 console 登录，比较方便的一种方式。 安装系统使用vnc登入机器，按照正常的步骤安装系统。vnc的ip为宿主机的ip，端口可以ps找到，默认是5900，可以利用 1# virsh vncdisplay centos-6.9 查询vnc id，然后利用vnc图形化软件（vncviewer）登录。镜像会自动进行初始化操作，之后会reboot，这时候需start才能再次链接到vnc图形化软件。 1# virsh start centos-6.9 也可以使用virsh从命令行进行安装，部分操作如下： 12345678查看当前虚机 # virsh list [--all] 使用virsh从console登入机器 # virsh console centos-6.9启动虚拟机# virsh start centos-6.9查看vnc端口# virsh vncdisplay centos-6.9 最好使用图形化软件，命令行进入会有些问题。 注意：再次启动报错 “ERROR Guest name ‘centos-6.9’ is already in use.” 时执行 1# virsh undefine centos-6.9 12也可以利用 qemu-system-x86_64 或 qemu-kvm 启动/usr/libexec/qemu-kvm 虚拟机初始化如果虚拟机可以联网，则直接使用默认软件源。 安装基础包123456789安装NetworkManager，用于网卡的自动发现及管理安装acpid，用于虚拟机的电源管理安装epel-release，添加epel源 # yum install -y NetworkManager acpid epel-release 开机启动服务# chkconfig acpid on# chkconfig NetworkManager on cloudinit安装123456789101112安装cloud-init，用于注入密码/密钥和主机名安装qemu-guest-agent，用于在面板更新密码/密钥安装cloud-utils，用于更改虚拟机根分区大小（可选安装，需要启用epel源） # yum install -y cloud-init qemu-guest-agent cloud-utils 开机启动服务（有的linux发行版默认不开机自启这些服务，需要手动设置开机自启）# chkconfig cloud-init on# chkconfig cloud-init-local on# chkconfig cloud-config on# chkconfig cloud-final on# chkconfig qemu-ga on CENTOS7 为qemu-guest-agent /etc/cloud/cloud-init.conf可能需要进行适当修改。 其他设置 ssh开启启动并修改配置，使其可以密码登录（PermitEmptyPasswords 为 yes）及使用root登录（PermitRootLogin 为yes）。 1# chkconfig sshd on 为保证实例能够访问neutron metadata服务，需要禁用zero conf 1# echo "NOZEROCONF=yes" &gt;&gt; /etc/sysconfig/network 关闭开启启动服务 123# chkconfig iptables off# chkconfig iptables6 off# chkconfig postfix off 关闭selinux 1# sed -i 's/=enforcing/=disabled/g' /etc/selinux/config 之后关机 1# proeroff 最后处理 清理虚拟机登陆及日志信息等 1# virt-sysprep -d centos-6.9 undefine虚拟机 1# virsh undefine centos-6.9 转换和压缩镜像 1virt-sparsify --convert qcow2 --compress centos-6.9.raw centos-6.9.qcow2 之后即可上传使用。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>镜像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言的字节对齐探究]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2FC%E8%AF%AD%E8%A8%80%E7%9A%84%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90%E6%8E%A2%E7%A9%B6.html</url>
    <content type="text"><![CDATA[环境如下（下文所有涉及类型长度都是以此环境为依据）： 12345# uname -a2.6.32-696.el6.i686 #1 SMP Tue Mar 21 18:53:30 UTC 2017 i686 i686 i386 GNU/Linux# gcc -v …………gcc version 4.4.7 20120313 (Red Hat 4.4.7-18) (GCC) 32位类型所占字节数 类型 长度（字节） char 1 short 2 int 3 float 4 double 8 缺省对齐在C语言中，结构是一种复合数据类型，其构成元素既可以是基本数据类型（如int、long、float等）的变量，也可以是一些复合数据类型（如数组、结构、联合等）的数据单元。 在结构中，各个成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构的地址相同。 结构内，编译器在默认情况下使结构成员按其类型长度对齐，即int类型成员对齐长度为4，double类型成员对齐长度为8。成员起始偏移（相对结构地址）必须是对齐长度或其倍数；不满足，则需要在前一个成员后添加空字节使其满足。 结构外（指作为整体单独计算或作为其他结构的成员计算），以结构内部最大成员类型长度对齐。 先对结构内对齐方式进行验证，代码如下（代码段1）： 123456789101112131415161718192021222324252627282930313233#include &lt;stdio.h&gt;#define PRINTF(intValue) printf(#intValue" is %d \n", (intValue));#define OFFSET_M(struct, member) ((char *)&amp;((struct *)0)-&gt;member - (char *)0)typedef struct &#123; char a; double b; short c;&#125;struct_a;typedef struct &#123; char a; short b; double c;&#125;struct_b;int main(void)&#123; PRINTF(OFFSET_M(struct_a, a)) PRINTF(OFFSET_M(struct_a, b)) PRINTF(OFFSET_M(struct_a, c)) PRINTF(sizeof(struct_a)) PRINTF(OFFSET_M(struct_b, a)) PRINTF(OFFSET_M(struct_b, b)) PRINTF(OFFSET_M(struct_b, c)) PRINTF(sizeof(struct_b)) return 0;&#125; 编译之后运行结果如下： 123456789# gcc test.c ; ./a.out OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 4 OFFSET_M(struct_a, c) is 12 sizeof(struct_a) is 16 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 4 sizeof(struct_b) is 12 运行结果与我们预想的不一样。struct_a中b的偏移应该为8，而不应该为4！ 造成此结果的原因为：gcc编译器进行了一些优化，在32位系统中默认对齐长度为4字节，64位系统中默认对齐字节为8字节，因此对验证有影响。 为了消除此影响，代码转到线上编译器上运行。 123http://c.yxz.me/或http://codepad.org/ 运行结果为： 12345678OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 8 OFFSET_M(struct_a, c) is 16 sizeof(struct_a) is 24 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 8 sizeof(struct_b) is 16 与预期相符： struct_a，成员b在偏移8字节处对齐，a补齐7字节，b后跟c(2个字节)，8 + 8 + 2 = 18 ，整体以成员最大长度8字节对齐，则最后再补齐6字节，结构体共占 24 字节。 struct_b，比较好分析，a占2字节（补齐1个），b占6字节（补齐4字节），c占用8字节，共16字节。 对结构外对齐方式验证，代码如下（代码段2）： 12345678910111213141516171819202122232425262728#include &lt;stdio.h&gt;#define PRINTF(intValue) printf(#intValue" is %d \n", (intValue));#define OFFSET_M(struct, member) ((char *)&amp;((struct *)0)-&gt;member - (char *)0)typedef struct &#123; char a; double b; short c;&#125;struct_a;typedef struct &#123; char a; struct_a b;&#125;struct_b;int main(void)&#123; PRINTF(sizeof(struct_a)) PRINTF(OFFSET_M(struct_b, a)) PRINTF(OFFSET_M(struct_b, b)) PRINTF(sizeof(struct_b)) return 0;&#125; 运行结果如下： 1234sizeof(struct_a) is 24 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 8 sizeof(struct_b) is 32 与预期相符。 改变缺省对齐现有两种方式改变缺省对齐方式： 使用伪指令 #pragma pack (n)，按照n个字节对齐。#pragma pack ()，取消自定义字节对齐方式。 __attribute((aligned (n)))，让所作用的结构成员以n字节对齐。如果结构中有成员的长度大于n，则按照最大成员的长度来对齐。 下面开始验证。修改代码段1为 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#define PRINTF(intValue) printf(#intValue" is %d \n", (intValue));#define OFFSET_M(struct, member) ((char *)&amp;((struct *)0)-&gt;member - (char *)0)#pragma pack(2)typedef struct &#123; char a; double b; short c;&#125;struct_a;typedef struct &#123; char a; short b; double c;&#125;struct_b;#pragma pack()int main(void)&#123; PRINTF(OFFSET_M(struct_a, a)) PRINTF(OFFSET_M(struct_a, b)) PRINTF(OFFSET_M(struct_a, c)) PRINTF(sizeof(struct_a)) PRINTF(OFFSET_M(struct_b, a)) PRINTF(OFFSET_M(struct_b, b)) PRINTF(OFFSET_M(struct_b, c)) PRINTF(sizeof(struct_b)) return 0;&#125; 通过以上方式，分别改为2、4、8、16字节对齐，运行结果为： 123456789101112131415161718192021222324252627282930313233343536373839#2字节对齐OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 2 OFFSET_M(struct_a, c) is 10 sizeof(struct_a) is 12 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 4 sizeof(struct_b) is 12 #4字节对齐OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 4 OFFSET_M(struct_a, c) is 12 sizeof(struct_a) is 16 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 4 sizeof(struct_b) is 12 #8字节对齐OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 8 OFFSET_M(struct_a, c) is 16 sizeof(struct_a) is 24 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 8 sizeof(struct_b) is 16 #16字节对齐OFFSET_M(struct_a, a) is 0 OFFSET_M(struct_a, b) is 8 OFFSET_M(struct_a, c) is 16 sizeof(struct_a) is 24 OFFSET_M(struct_b, a) is 0 OFFSET_M(struct_b, b) is 2 OFFSET_M(struct_b, c) is 8 sizeof(struct_b) is 16 分析运行结果，可得以下结论 内部，配置对齐字节与成员缺省对齐字节相比，选择较小的。配置大于内部成员缺省最大对齐字节无意义。 外部（或整体），配置对齐字节与内部成员缺省最大对齐字节，也选择较小的。 GCC编译代码（无自定义对齐）也可得相同结论。 三者同时存在时，也是按照“最小规则”处理。 参考资料C语言中的数据对齐 C语言的字节对齐及#pragma pack的使用 内存对界]]></content>
      <tags>
        <tag>C语言</tag>
        <tag>数据对齐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELF文件简述]]></title>
    <url>%2FELF%E6%96%87%E4%BB%B6%E7%AE%80%E8%BF%B0.html</url>
    <content type="text"><![CDATA[ELF(Executable and Linkable Format)是一种用于二进制文件、可执行文件、目标代码、共享库和核心转储的格式文件，是UNIX系统实验室（USL）作为应用程序二进制接口（Application Binary Interface，ABI）而开发和发布的，也是Linux的主要可执行文件格式。 ELF文件类型ELF 格式的文件可归为如下4类： ELF文件类型 说明 实例 可重定位文件 (Relocatable File) 这类文件包含了代码和数据，可以被用来链接成可执行文件或共享目标文件，静态链接库也可以归为这一类。 Linux的.o文件和.ko（内核）文件；Windows的.obj。 可执行文件 (Executable File) 这类文件包含了可以直接执行的程序，一般无扩展名。 Linux的/bin/bash；Windows的.exe。 共享目标文件 (Shared Object File) 这种文件包含了代码和数据，可以在以下两种情况下使用。一种是链接器可以使用这种文件跟其他的可重定位文件和共享目标文件链接，产生新的目标文件。第二种是动态链接器可以将几个这种共享目标文件与可执行文件结合，作为进程映像的一部分来运行。 Linux的.so；Windows的.dll。 核心转储文件 (Core Dump File) 当进程意外终止时，系统可以将该进程的地址空间内容及终止时的一些其他信息转储到核心转储文件。 Linux的core dump 第四种类型可以忽略。可以用’file’命令来查看文件类型。 123456# file 8021q.ko 8021q.ko: ELF 32-bit LSB relocatable, Intel 80386, version 1 (SYSV), not stripped# file /bin/mv/bin/mv: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped# file /lib/libc-2.12.so /lib/libc-2.12.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (GNU/Linux), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, not stripped ELF文件结构可以用“readelf”命令来查看ELF文件的信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# readelf -a SimpleSection.oELF Header: Magic: 7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 Class: ELF32 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: REL (Relocatable file) Machine: Intel 80386 Version: 0x1 Entry point address: 0x0 Start of program headers: 0 (bytes into file) Start of section headers: 272 (bytes into file) Flags: 0x0 Size of this header: 52 (bytes) Size of program headers: 0 (bytes) Number of program headers: 0 Size of section headers: 40 (bytes) Number of section headers: 11 Section header string table index: 8Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al [ 0] NULL 00000000 000000 000000 00 0 0 0 [ 1] .text PROGBITS 00000000 000034 000050 00 AX 0 0 4 [ 2] .rel.text REL 00000000 000420 000028 08 9 1 4 [ 3] .data PROGBITS 00000000 000084 000008 00 WA 0 0 4 [ 4] .bss NOBITS 00000000 00008c 000004 00 WA 0 0 4 [ 5] .rodata PROGBITS 00000000 00008c 000004 00 A 0 0 1 [ 6] .comment PROGBITS 00000000 000090 00002e 01 MS 0 0 1 [ 7] .note.GNU-stack PROGBITS 00000000 0000be 000000 00 0 0 1 [ 8] .shstrtab STRTAB 00000000 0000be 000051 00 0 0 1 [ 9] .symtab SYMTAB 00000000 0002c8 0000f0 10 10 10 4 [10] .strtab STRTAB 00000000 0003b8 000066 00 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings) I (info), L (link order), G (group), x (unknown) O (extra OS processing required) o (OS specific), p (processor specific)There are no section groups in this file.There are no program headers in this file.Relocation section '.rel.text' at offset 0x420 contains 5 entries: Offset Info Type Sym.Value Sym. Name00000010 00000501 R_386_32 00000000 .rodata00000015 00000d02 R_386_PC32 00000000 printf0000002e 00000301 R_386_32 00000000 .data00000033 00000401 R_386_32 00000000 .bss00000046 00000c02 R_386_PC32 00000000 func1There are no unwind sections in this file.Symbol table '.symtab' contains 15 entries: Num: Value Size Type Bind Vis Ndx Name 0: 00000000 0 NOTYPE LOCAL DEFAULT UND 1: 00000000 0 FILE LOCAL DEFAULT ABS SimpleSection.c 2: 00000000 0 SECTION LOCAL DEFAULT 1 3: 00000000 0 SECTION LOCAL DEFAULT 3 4: 00000000 0 SECTION LOCAL DEFAULT 4 5: 00000000 0 SECTION LOCAL DEFAULT 5 6: 00000004 4 OBJECT LOCAL DEFAULT 3 static_var.1243 7: 00000000 4 OBJECT LOCAL DEFAULT 4 static_var2.1244 8: 00000000 0 SECTION LOCAL DEFAULT 7 9: 00000000 0 SECTION LOCAL DEFAULT 6 10: 00000000 4 OBJECT GLOBAL DEFAULT 3 global_init_var 11: 00000004 4 OBJECT GLOBAL DEFAULT COM global_uninit_var 12: 00000000 27 FUNC GLOBAL DEFAULT 1 func1 13: 00000000 0 NOTYPE GLOBAL DEFAULT UND printf 14: 0000001b 53 FUNC GLOBAL DEFAULT 1 main SimpleSection.o为gcc编译（未链接）的一个可执行文件，源码为 123456789101112131415161718192021222324/** SimpleSection.c* Linux:* gcc -c SimpleSection.c*/int printf(const char* format, ...);int global_init_var = 84;int global_uninit_var;void func1(int i)&#123; printf(&quot;%d\n&quot;, i);&#125;int main(void)&#123; static int static_var = 85; static int static_var2; int a = 1; int b; func1(static_var + static_var2 + a + b); return a;&#125; ELF目标文件格式的最前部是ELF头部（ELF Header），这是确定的。由于目标文件既要参与程序链接又要参与程序执行。出于方便性和效率考虑，目标文件格式提供了两种并行视图，分别反映了这些活动的不同需求。 ELF头部（ELF Header） 包含了描述整个文件的基本属性，比如ELF文件版本、目标机器型号、程序入口地址等。 节区头部（Section Headers） 可选。包含描述文件节区的信息，每个节区在表中都有一项，每一项给出诸如节区名称、节区大小这类信息。用于链接的目标文件必须包含节区头部。 程序头部（Program Headers） 也称作segments，可选，告诉系统如何创建进程映像。 用来构造进程映像的目标文件必须具有程序头部表，可重定位文件不需要这个表。 section和segment的区别： section称为节，是指在汇编源码中经由关键字section或segment修饰、逻辑划分的指令或数据区域。 segment称为段，是根据目标文件中属性相同的多个section合并后的section集合，这个集合称为segment。我们平时所说的可执行程序内存空间中的代码段和数据段就是指的segment。 section主要提供给Linker使用， 而segment提供给Loader用。Linker需要关心.text、.rel.text、.data、.rodata等，因为Linker需要做relocation，而Loader只需要知道Read/Write/Execute的属性 executable的ELF文件可以没有section，但必须有segment。ELF文件中间部分是共用的（也就是代码段、数据段等），如shared objects就可以同时拥有Program header table和Section Header Table，这样load完后还可以relocate。 这样设定之后，使得Loader需要做的工作大大减少了，一定程度上提高了程序加载的效率。 （摘自Section和Segment的区别 和 ELF文件中section与segment的区别） 在大多数情况下，这两者都被混为一谈。 除了 ELF 头部表以外， 其他节区和段都没有规定的顺序。 ELF数据表示ELF数据应用自定义的一组数据类型。根据不同的体系结构选择不同的类型。 ELF文件有关的数据结构都在/usr/include/elf.h 中定义（也可以在内核代码中搜索 elf.h 文件，然后根据不同的架构进行选择）。下表列出的仅是x86架构。 自定义类型 原始类型 长度（字节） Elf32_Half uint16_t 2 Elf32_Word uint32_t 4 Elf32_Sword int32_t 4 Elf32_Xword uint64_t 8 Elf32_Sxword int64_t 8 Elf32_Addr uint32_t 4 Elf32_Off uint32_t 4 Elf32_Section uint16_t 2 Elf32_Versym Elf32_Half 2 64位体系结构有对应的自定义类型。 ELF Hearder包含整个文件的基本属性信息。这些信息独立于处理器，也独立于文件中的其余内容。用以下的数据结构表示： 12345678910111213141516171819#define EI_NIDENT (16)typedef struct&#123; unsigned char e_ident[EI_NIDENT]; /* 魔数及其他信息 */ Elf32_Half e_type; /* ELF文件类型 */ Elf32_Half e_machine; /* ELF文件的CPU属性（架构） */ Elf32_Word e_version; /* ELF文件版本号 */ Elf32_Addr e_entry; /* 入口点（虚拟地址） */ Elf32_Off e_phoff; /* 程序头表的偏移地址 */ Elf32_Off e_shoff; /* 节区头表的偏移地址 */ Elf32_Word e_flags; /* ELF标志位 */ Elf32_Half e_ehsize; /* ELF文件头本身的大小 */ Elf32_Half e_phentsize; /* 程序头表中的每一项大小 */ Elf32_Half e_phnum; /* 程序头表中的项目数 */ Elf32_Half e_shentsize; /* 节区头表中的每一项大小 */ Elf32_Half e_shnum; /* 节区头表的项目数 */ Elf32_Half e_shstrndx; /* 字符串表所在节区头表的位置下标 */&#125; Elf32_Ehdr; 下面对主要字段的含义进行解读： e_ident 1234567ELF Header: Magic: 7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 Class: ELF32 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 第一行“Magic”的16个字节被ELF标准规定用来标识ELF文件的平台属性，每一个字节代表不同的含义。 前4个字节是所有ELF文件都必须相同的标识码，第一个字节对应ASCII字符里的DEL控制符，后面3个字节是ELF这3个字母的ASCII码。这4个字节被称为ELF文件的魔数。 .out格式最开始两个字节为0x01、0x07；PE/COFF文件最开始两个字节为0x4d、0x5a，即ASCII字符MZ。这种魔数用来确认文件的类型，操作系统加载可执行文件的时候会确认魔数是否正确，不正确会拒绝加载。 第5个字节用来标识ELF的文件类：0代表不确认；1代表是32位；2代表是64位。 第6个字节是字节序：0代表不确认；1代表小端；2代表大端。 第7个字节规定ELF文件的主版本号，一般是1，因为ELF标准自1.2版之后就再也没有更新了。 后面的9个字节ELF标准没有定义，一般填0，有些平台会使用这9个字节作为扩展标志。 12345魔数的由来UNIX早年是在PDP小型机上诞生的，当时的系统在加载一个可执行文件后直接从文件的当一个字节开始执行，一般在文件的最开始放置一条跳转（jump）指令，这条指令负责跳过接下来的7个机器字的文件头到可执行文件的真正入口。而0x01 0x07这两个字节正好是当时PDP-11的机器的跳转7个机器字的指令。为了跟以前的系统保持兼容，这条跳转指令被当做魔数被保留至今。ELF文件标准历史20世纪90年代，一些厂商联合成立了一个委员会，起草并发布了一个ELF文件格式标准供公开使用，并且希望所有人能够遵循这项标准并且从中获益。1993年，委员会发布了ELF文件标准。1995年，委员会发布了ELF 1.2标准，自此委员会完成了自己的使命，不就就解散了。所以ELF文件格式标准的最新版本为1.2。 e_type ELF文件类型。系统通过这个常量判断ELF文件的类型，而不是通过文件的扩展名。相关常量以“ET_”开头。 常量 值 含义 ET_REL 1 可重定位文件 ET_EXEC 2 可执行文件 ET_DYN 3 共享目标文件 ET_CORE 4 转储文件 e_machine ELF文件格式被设计成可以在多个平台下使用。但这并不表示同一个文件可以在不同的平台下使用，而是表示不同平台下的ELF文件都遵循同一套ELF标准。此字段表示该文件的平台属性。相关的常量以“EM_”开头。 常量 值 含义 EM_M32 1 AT&amp;T WE 32100 EM_SPARC 2 SUN SPARC EM_386 3 Intel 80386 EM_68K 4 Motorola m68k family EM_88K 5 Motorola m88k family EM_860 7 Intel 80860 EM_MIPS 8 MIPS R3000 big-endian 节区头部表（Section Headers Table）ELF文件中有很多各种各样的段，这个段表（Section Header Table）就是保存这些段的基本属性的结构。段表是ELF文件中除了文件头以外最重要的结构，它描述了ELF的各个段的信息，比如每个段的段名、段的长度、在文件中的偏移、读写权限及段的其他属性。也就是说，ELF文件的短结构就是由段表决定的。编译器、链接器和装载器都是依靠段表来定位和访问各个段的属性的。段表在文件中的位置由ELF文件头的“e_shoff”决定。 可用readelf工具查看ELF文件的段（objdump -h也可以查看，但是此命令只显示ELF文件中关键的段（.code、.data、.bss等），而忽略其他辅助性的段，比如：符号表、字符串表、段名字符串表、重定位表等）。 1234567891011121314151617181920# readelf -S SimpleSection.o There are 11 section headers, starting at offset 0x110:Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al [ 0] NULL 00000000 000000 000000 00 0 0 0 [ 1] .text PROGBITS 00000000 000034 000050 00 AX 0 0 4 [ 2] .rel.text REL 00000000 000420 000028 08 9 1 4 [ 3] .data PROGBITS 00000000 000084 000008 00 WA 0 0 4 [ 4] .bss NOBITS 00000000 00008c 000004 00 WA 0 0 4 [ 5] .rodata PROGBITS 00000000 00008c 000004 00 A 0 0 1 [ 6] .comment PROGBITS 00000000 000090 00002e 01 MS 0 0 1 [ 7] .note.GNU-stack PROGBITS 00000000 0000be 000000 00 0 0 1 [ 8] .shstrtab STRTAB 00000000 0000be 000051 00 0 0 1 [ 9] .symtab SYMTAB 00000000 0002c8 0000f0 10 10 10 4 [10] .strtab STRTAB 00000000 0003b8 000066 00 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings) I (info), L (link order), G (group), x (unknown) O (extra OS processing required) o (OS specific), p (processor specific) 段表是一个以“Elf32_Shdr”结构体为元素的数组。元素个数等于段的个数。“Elf32_Shdr”又被称为段描述符（Section Descriptor），其结构如下 12345678910111213typedef struct&#123; Elf32_Word sh_name; /* Section name (string tbl index) */ Elf32_Word sh_type; /* Section type */ Elf32_Word sh_flags; /* Section flags */ Elf32_Addr sh_addr; /* Section virtual addr at execution */ Elf32_Off sh_offset; /* Section file offset */ Elf32_Word sh_size; /* Section size in bytes */ Elf32_Word sh_link; /* Link to another section */ Elf32_Word sh_info; /* Additional section information */ Elf32_Word sh_addralign; /* Section alignment */ Elf32_Word sh_entsize; /* Entry size if section holds table */&#125; Elf32_Shdr; 其各个成员含义如下： 成员 描述 sh_name Section name段名。段名是字符串类型，位于一个叫做“.shstrtab”的字符串表。sh_name是段名在“.shstrtab”中的偏移 sh_type Section type 段的类型 sh_flags Section flag 段的标志位 sh_addr Section Address 段虚拟地址。如果该段可以被加载，则sh_addr为该段被加载后在进程地址空间中的虚拟地址 sh_offset Section Offset 段偏移。如果该段存在于文件中，则表示该段在文件中的偏移 sh_size Section Size 段的长度 sh_link Section Link and Section Information 段链接信息 sh_addralign Section Address Alignment 段地址对齐。有些段对段地址对齐有要求，假设有个段刚开始的位置包含了有个double变量，因为x86系统要求浮点数的存储地址必须是本身的整数倍，也就是说保存double变量的地址必须是8字节的整数倍。这样对一个段来说，它的sh_addr必须是8的整数倍。由于地址对齐的数量都是2的指数倍。sh_addralign表示地址对齐数量中的指数，即sh_addralign = 3表示对齐为2的3次方倍。0 或 1表示没有对齐要求。 sh_entsize Section Entry Size 项的长度。有些段包含了一些固定大小的项，比如符号表，它包含的每个符号所占的大小是一样的。对于这种段，sh_entsize表示每个项的大小。0表示该段不包含固定大小的项 段的名字对于编译器、链接器是有意义的，但是对于操作系统来说并没有实质的意义，对于操作系统来说，一个段该如何处理取决于它的属性和权限，即由段的类型和段的标志位这两个成员决定。 所有段的位置及长度如下图所示 由于对齐的原因，深色部分表示间隔。 段的类型（sh_type） 段的名字只是在链接和编译过程中有意义，但它不能真正表示段的类型。对于编译器和链接器来说，主要决定段的属性的是段的类型（sh_type）和段的标志位（sh_flags）。段的类型相关常量以SHT_开头，列举如下 常量 值 含义 SHT_NULL 0 无效段 SHT_PROGBITS 1 程序段、代码段、数据段都是这种类型 SHT_SYMTAB 2 表示该段的内容为符号表 SHT_STRTAB 3 表示该段的内容为字符串表 SHT_RELA 4 重定位表。该段包含了重定位信息。 SHT_HASH 5 符号表的哈希表 SHT_DYNAMIC 6 动态链接信息 SHT_NOTE 7 提示性信息 SHT_NOBITS 8 表示该段在文件中没内容，比如.bss段 SHT_REL 9 该段包含了重定位信息。 SHT_SHLIB 10 保留 SHT_DYNSYM 11 动态链接的符号表。 段的标志位（sh_flag） 段的标志位表示该段在进程虚拟地址空间中的属性。相关常量以SHF_开头。 常量 值 含义 SHF_WRITE 1 &lt;&lt; 0 表示该段在进程空间中可写 SHF_ALLOC 1 &lt;&lt; 1 表示该段在进程空间中须要分配空间。有些包含指示或控制信息的段不需要在进程空间中被分配空间，它们一般不会有这个标志。像代码段、数据段和.bss段都会有这个标志位 SHF_EXECINSTR 1 &lt;&lt; 2 表示该段在进程空间中可以被执行，一般指代码段 对于系统保留段，下表列举了它们的属性。 Name sh_type sh_flag .bss SHT_NOBITS SHF_ALLOC + SHF_WRITE .comment SHT_PROGBITS NONE .data SHT_PROGBITS SHF_ALLOC + SHF_WRITE .data1 SHT_PROGBITS SHF_ALLOC + SHF_WRITE .debug SHT_PROGBITS NONE .dynamic SHT_DYNAMIC SHF_ALLOC + SHF_WRITE 在有些系统下.dynamic段可能是只读的，所以无SHF_WRITE标志位 .hash SHT_HASH SHF_ALLOC .line SHT_PROGBITS NONE .note SHT_NOTE NONE .rodata SHT_PROGBITS SHF_ALLOC .rodata1 SHT_PROGBITS SHF_ALLOC .shstrtab SHT_STRTAB NONE .strtab SHT_STRTAB 如果该ELF文件中有可装载的段需要用到该字符串表，那么该字符串表也将被装载到进程空间，则有SHF_ALLOC标志位 .symtab SHT_STRTAB 同字符串表 .text SHT_PROGBITS SHF_ALLOC + SHF_EXECINSTR 段的链接信息（sh_link、sh_info） 如果段的类型是与链接相关的（不论是动态链接或静态链接），比如重定位表、符号表等，那么sh_link和sh_info这两个成员所包含的意义如下表所示，对于其他类型的段，这两个成员没有意义。 sh_type sh_link sh_info SHT_DYNAMIC 该段所使用的字符串表在段表中的下标 0 SHT_HASH 该段所使用的符号表在段表中的下标 0 SHT_REL、SHT_RELA 该段所使用的相应符号表在段表中的下标 该重定位表所作用的段在段表中的下标 SHT_SYMTAB、SHT_DYNSYM 操作系统相关 操作系统相关 程序头部表（Program Header Table）可执行文件或者共享目标文件的程序头部是一个结构数组，每个结构描述了一个段或者系统准备程序执行所必需的其它信息。目标文件的“段”包含一个或者多个“节区”， 也就是“段内容(Segment Contents)”。程序头部仅对于可执行文件和共享目标文件 有意义。 可执行目标文件在 ELF 头部的 e_phentsize 和 e_phnum 成员中给出其自身程序头部 的大小。程序头部的数据结构如下： 1234567891011typedef struct&#123; Elf32_Word p_type; /* Segment type */ Elf32_Off p_offset; /* Segment file offset */ Elf32_Addr p_vaddr; /* Segment virtual address */ Elf32_Addr p_paddr; /* Segment physical address */ Elf32_Word p_filesz; /* Segment size in file */ Elf32_Word p_memsz; /* Segment size in memory */ Elf32_Word p_flags; /* Segment flags */ Elf32_Word p_align; /* Segment alignment */&#125; Elf32_Phdr; 各字段含义如下 成员 含义 p_type 数组元素描述的段的类型，或者如何解释此数组元素的信息。 p_flags 段相关的标志。 p_offset 从文件头到该段第一个字节的偏移。 p_vaddr 段的第一个字节将被放到内存中的虚拟地址。 p_paddr 仅用于与物理地址相关的系统中。因为 System V 忽略所有应用程序的物理地址信息，此字段对与可执行文件和共享目标文件而言具体内容是指定的。 p_filesz 段在文件映像中所占的字节数。 p_memsz 段在内存映像中占用的字节数。 p_align 可加载的进程段的 p_vaddr 和 p_offset 取值必须合适，相对于对页面大小的取模而言。此成员给出段在文件中和内存中如何 对齐。数值 0 和 1 表示不需要对齐。指数型。 p_type包括以下常量，一般以“PT_”开头 常量 值 描述 PT_NULL 0 元素未使用 PT_LOAD 1 段可加载。段的大小由p_filesz 和 p_memsz描述。文件中的字节被映射到内存段开始处。p_memsz大于p_filesz，“剩余”的字节要清零。p_filesz不能大于p_memsz。可加载的段在程序头部表格中根据p_vaddr成员按升序排列。 PT_DYNAMIC 2 动态链接信息 PT_INTERP 3 元素给出一个NULL结尾的字符串的位置和长度，该字符串将被当做解释器调用。这种段类型仅对与可执行文件有意义。在一个文件中不能出现一次以上。如果存在这种类型的段，它必须在所有可加载段项目的前面。 PT_NOTE 4 元素给出附加信息的位置和大小 PT_SHLIB 5 保留 PT_PHDR 6 元素若存在，则给出了程序头部表自身的大小和位置，既包括在文件中也包括在内存中的信息。此类型的段在文件中不能出现一次以上。并且只有程序头部表是程序的内存映像一部分时才起作用。如果存在此类型段，则必须在所有可加载段炫目的前面。 PT_TLS 7 线程本地存储段 p_flags包括以下常量，一般以“PF_”开头 常量 值 描述 PF_X 1 &lt;&lt; 0 段可执行 PF_W 1 &lt;&lt; 1 段可写 PF_R 1 &lt;&lt; 2 段可读 字符串表（String Table）ELF文件中用到了很多字符串，比如段名、变量名等。因为字符串的长度往往是不定的，所以用固定的结构来表示它比较困难。一种很常见的做法是把字符串集中起来存放到一个表，然后使用字符串在表中的偏移来引用字符串。比如下面这个字符串表。 偏移 +0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +0 \0 h e l l o w o r l +10 d \0 M y v a r i a b +20 l e \0 那么偏移与它们对应的字符串如下表 偏移 字符串 0 空字符串 1 helloworld 6 world 12 Myvariable 通过这种方法，在ELF文件中引用字符串只须给出一个数字下标即可，不用考虑字符串长度的问题。一般字符串表在ELF文件中也以段的形式保存，常见的段名为“.strtab”或”.shstrtab”。这两个字符串表分别为字符串表(String Table)和段表字符串表(Section Header String Table)。字符串表用来保存普通的字符串，比如符号的名字；段表字符串表用来保存段表中用到的字符串，最常见的就是段名(sh_name)。 ELF文件头中的“e_shstrndx”是E1f32_Ehdr的最后一个成员，它是”Section header string table index”的缩写。我们知道段表字符串表本身也是ELF文件中的一个普通的段，知道它的名字往往叫做“.shstrtab”。那么这个“e_shstrndx”就表示“.shstrtab”在段表中的下标，即段表字符串表在段表中标。可以通过“readelf -S”验证。 符号表（Symbol Table）链接过程的本质就是要把多个不同的目标文件之间相互“粘”到一起。为了使不同目标文件之间能够相互粘合，这些目标文件之间必须有固定的规则才行。在链接中，目标文件之间相互粘合实际上是目标文件之间对地址的引用，即对函数和变量的地址的引用。比如目标文件B要用到了目标文件A中的函数“foo”，那么我们就称目标文件A定义(Define)了函数”foo”，称目标文件B引用(Reference)了目标文件A中的函数”foo”。这两个概念也同样适用于变量。每个函数或变量都有自已独特的名字，才能避免链接过程中不同变量和函数之间的混淆。在链接中，将函数和变量统称为符号(Symbol)，函数名或变量名就是符号名(Symbol Name)。 符号可以被看作是链接中的粘合剂，整个链接过程正是基于符号才能够完成。链接过程中很关键的一部分就是符号的管理，每一个目标文件都会有一个相应的符号表(Symbol Table)。这个表里面记录了目标文件中所用到的所有符号。每个定义的符号有一个对应的值，叫做符号值(Symbol Value)，对于变量和函数来说，符号值就是它们的地址。除了函数和变量之外，还存在其他几种不常用到的符号。将符号表中所有的符号进行分类，它们有可能是下面这些类型中的一种： 定义在本目标文件的全局符号，可以被其他目标文件引用。比如SimpleSection.o里面”funcl”、”main”和”global_init_var”。 在本目标文件引用的全局符号，却没有定义在本目标文件，这一般叫做外部符号(External SymEaol)。比如SimpleSection.o里面的”printf”。 段名，这种符号往往由编译器产生，它的值就是该段的起始地址。比如SimpleSection.o里面的”.text”. “.data”等。 局部符号，这类符号只在编译单元内部可见。比如5imple5ection.o里面的”static_var”和”static_var2”。调试器可以使用这些符号来分析程序或崩溃时的核心转储文件。这些局部符号对于链接过程没有作用，链接器往往也忽略它们。 行号信息，即目标文件指令与源代码中代码行的对应关系，它也是可选的。 最值得关注的就是全局符号，即上面分类中的第一类和第二类。因为链接过程只关心全局符号的相互“粘合”，局部符号、段名、行号等都是次要的，它们对于其他目标文件来说是“不可见”的，在链接过程也是无关紧要的。可以使用很多工共来查看ELF文件的符号表，比如readelf、objdump、nm等，比如使用”nm”来查看SimpleSection.o的符号结果如下。 12345678# nm SimpleSection.o 00000000 T func100000000 D global_init_var00000004 C global_uninit_var0000001b T main U printf00000004 d static_var.124300000000 b static_var2.1244 符号表结构ELF文件中的符号表往往是文件中的一个段，段名一般叫“.symtab”。符号表结构如下 123456789typedef struct&#123; Elf32_Word st_name; /* Symbol name (string tbl index) */ Elf32_Addr st_value; /* Symbol value */ Elf32_Word st_size; /* Symbol size */ unsigned char st_info; /* Symbol type and binding */ unsigned char st_other; /* Symbol visibility */ Elf32_Section st_shndx; /* Section index */&#125; Elf32_Sym; 每个Elf32_Sym结构对应一个符号。数组第一个元素为“未定义”符号。结构成员解析如下 成员 描述 st_name 符号名。包含了该符号名在字符串表中的下标 st_value 符号相对应的值。这个值跟符号有关，绝对值或地址，不同符号对应的值含义不同 st_size 符号大小，对于包含数据的符号，这个值是该数据类型的大小。 st_info 符号类型和绑定信息 st_other 保留 st_shndx 符号所在的段 符号类型和绑定信息（st_info） 低4位表示符号的类型（Symbol Type），高28位表示符号绑定信息（Symbol Binding）。 类型 常量 值 说明 STT_OBJECT 1 该符号是个数据对象，比如变量、数组等 STT_FUNC 2 该符号是个函数或其他可执行代码 STT_SECTION 3 该符号表示一个段，这种符号必须是STB_LOCAL的 STT_FILE 4 该符号表示文件名，一般都是该目标文件所对应的原文件名，它一定是STB_LOCAL的，并且它的st_shndx一定是SHN_ABS 绑定信息 常量 值 说明 STB_LOCAL 0 局部符号 STB_GLOBAL 1 全局符号 STB_WEAK 2 弱引用 符号所在段（st_shndx） 符号定义在本目标文件中，这个成员表示符号所在的段在段表中的下标；符号不在本目标文件，或对于有些特殊符号，此字段的值如下 宏定义 值 说明 SHN_ABS 0xfff1 该符号包含了一个绝对值。文件名符号属于此类型。 SHN_COMMON 0xfff2 该符号是一个“COMMON块”类型的符号，一般来说，未初始化的全局符号定义就是此类型。 SHN_UNDEF 0 该符号未定义。表示该符号在本目标文件被引用，但定义在其他目标文件。 符号值（st_value） 具体按照下面几种情况区别对待 在目标文件中，如果是符号的定义并且该符号不是“COMMON块”类型的（即st_shndx不为COMMON），则st_value表示该符号在段中的偏移。即符号所对应的函数或变量位于由st_shndx指定的段，偏移st_value的位置。也是目标文件中定义全局变量的符号的最常见的情况。 在目标文件中，如果是符号是“COMMON块”类型的，则st_value表示该符号的对齐属性。 在可执行文件中，st_value表示符号的虚拟地址。 重定位表To be continued. 参考资料可执行文件（ELF）格式的理解 ELF文件格式分析 ELF文件标准 ELF文件格式详解]]></content>
      <tags>
        <tag>ELF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github网页之评论]]></title>
    <url>%2FHexo%2Fhexo%E6%90%AD%E5%BB%BAgithub%E7%BD%91%E9%A1%B5%E4%B9%8B%E8%AF%84%E8%AE%BA.html</url>
    <content type="text"><![CDATA[hexo搭建的博客下，有多种评论系统可供选择：Disqus、多说和友言等。但是由于被墙或者评论系统关闭，已不可再用。本介绍基于 GitHub Issues开发的评论系统——gitment。 OAuth ApplicationOAuth Application是一种认证机制，可参考简述 OAuth 2.0 的运作流程。 使用gitmen第一步就是需要注册一个新的 OAuth Application。 Application name，可以随便填写。 Homepage URL，网站主页，可以填写github给的网址（以io结尾）。 Application description，应用描述，可不填。 Authorization callback URL，应用调用返回页。可和 Homepage URL一样，最好填写私有网址。 注册之后得到一个 client ID 和一个 client secret，这两个是之后要用到的值。也可以通过以下方式查询 1github settings --&gt;&gt; Developer settings --&gt;&gt; OAuth Apps gitment配置gitment创作者列出的配置方法比较简单，本文以next主题为例进行详细介绍。 配置主题配置文件。 在主题配置文件_config.yaml中添加以下代码 123456789# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: true githubID: your github name repo: your repository name ClientID: -- ClientSecret: -- lazy: true githubID，github 账号的 name。配置id不生效，会报错。获取方式，之前的文章已经介绍。 repo，仓库名，用来存放评论。gitment以github issues为基础，所以此仓库可以是属性为public的任意仓库。 ClientID、ClientSecret，OAuth Application相关。 lazy，懒加载。进入文章后，评论全部显示（false）或者仅显示一个按钮（true）。 懒加载按钮显示设置 主题languages是一些文字的各种语言版本。在en.yml文件中添加 1gitmentbutton: Show comments from Gitment 在zh-Hans.yml文件中添加 1gitmentbutton: 显示 Gitment 评论 其他语言课根据个人需求进行配置。 按钮事件 在主题layout/_partials/comments.swig文件中添加代码，以使文件最后类似 123456789101112&#123;% elseif theme.valine.appid and theme.valine.appkey %&#125; &lt;div id="vcomments"&gt;&lt;/div&gt; &#123;% elseif theme.gitment.enable %&#125; &#123;% if theme.gitment.lazy %&#125; &lt;div onclick="ShowGitment()" id="gitment-display-button"&gt;&#123;&#123; __('gitmentbutton') &#125;&#125;&lt;/div&gt; &lt;div id="gitment-container" style="display:none"&gt;&lt;/div&gt; &#123;% else %&#125; &lt;div id="gitment-container"&gt;&lt;/div&gt; &#123;% endif %&#125; &#123;% endif %&#125; &lt;/div&gt;&#123;% endif %&#125; 文件中增加了“elseif theme.gitment.enable” 事件域。 增加对应页面生成JS代码 在主题layout/_third-party/comments/目录中添加文件gitment.swig，内容为 12345678910111213141516171819202122232425262728293031323334353637&#123;% if theme.gitment.enable %&#125; &#123;% set owner = theme.gitment.githubID %&#125; &#123;% set repo = theme.gitment.repo %&#125; &#123;% set cid = theme.gitment.ClientID %&#125; &#123;% set cs = theme.gitment.ClientSecret %&#125; &lt;link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"&gt; &lt;script src="https://imsun.github.io/gitment/dist/gitment.browser.js"&gt;&lt;/script&gt; &#123;% if not theme.gitment.lazy %&#125; &lt;script type="text/javascript"&gt; var gitment = new Gitment(&#123; id: window.location.pathname, owner: '&#123;&#123;owner&#125;&#125;', repo: '&#123;&#123;repo&#125;&#125;', oauth: &#123; client_id: '&#123;&#123;cid&#125;&#125;', client_secret: '&#123;&#123;cs&#125;&#125;', &#125;&#125;); gitment.render('gitment-container'); &lt;/script&gt; &#123;% else %&#125; &lt;script type="text/javascript"&gt; function ShowGitment()&#123; document.getElementById("gitment-display-button").style.display = "none"; document.getElementById("gitment-container").style.display = "block"; var gitment = new Gitment(&#123; id: window.location.pathname, owner: '&#123;&#123;owner&#125;&#125;', repo: '&#123;&#123;repo&#125;&#125;', oauth: &#123; client_id: '&#123;&#123;cid&#125;&#125;', client_secret: '&#123;&#123;cs&#125;&#125;', &#125;&#125;); gitment.render('gitment-container'); &#125; &lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125; 文件中使用了两个线上文件，可以把路径复制，在浏览器（谷歌可用）中打开来查看。 添加完文件后，在同目录index.swig文件中添加 1&#123;% include 'gitment.swig' %&#125; 以引入新创建的文件。 设置CSS样式 在主题source/css/_common/components/third-party/目录下添加gitment.styl文件，设置button的样式 12345678910111213#gitment-display-button&#123; display: inline-block; padding: 0 15px; color: #0a9caf; cursor: pointer; font-size: 14px; border: 1px solid #0a9caf; border-radius: 4px; &#125; #gitment-display-button:hover&#123; color: #fff; background: #0a9caf; &#125; 在目录third-party.styl中添加 1@import "gitment"; 以引入新创建的文件。 至此，评论配置完成。 调试点 文件中空格、结尾空行最好保持不变。 本地只能调试是否有页面错误，其他功能最好线上调试。 调试引用的线上js代码时，可以下载到本地进行修改。代码引用本地修改后的文件。例如替换为 1src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/test.js test.js为下载到本地加入调试信息的文件。 最易出错的地方应该是字段置换错误（layout/_third-party/comments/gitment.swig文件调用js时，字段已经置换）。可进入浏览器调试模式（F12）进行调试。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github网页之搜索引擎收录]]></title>
    <url>%2FHexo%2Fhexo%E6%90%AD%E5%BB%BAgithub%E7%BD%91%E9%A1%B5%E4%B9%8B%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%94%B6%E5%BD%95.html</url>
    <content type="text"><![CDATA[确认收录确认网站是否被某搜索引擎收录的方法为，在其搜索框输入 1site:website_url 例如：在百度搜索引擎上确认baidu.com是否被收录 1site:baidu.com 站点验证验证此站点是否归你所有。 谷歌验证进入 谷歌搜索引擎站点验证，验证流程比较智能，输入网址之后，登录域名提供商，谷歌会自动添加一条 TXT记录，之后即可完成验证。 百度验证进入站点管理，添加网站，最后会提供验证方式。 文件验证 下载验证文件，将其放置于所配置域名的根目录下。 HTML标签验证 将给出的一段代码发在网站首页的HTML代码的标签与标签之间。 CNAME验证 将 baidu_key.site_url 使用CNAME解析到zz.baidu.com。baidu_key为搜索引擎自动分配，site_url为需要验证的网址。 如果是购买的域名，建议用CNAME验证。但是CNAME验证有些问题，描述不正确。应该是将提供的一串字符（website_url之前的部分）解析到提供的网址。 之后即可验证成功。 站点地图站点地图是一个页面，上面放置了网站上需要搜索引擎抓取的所有页面的链接。网站地图可以方便搜索引擎蜘蛛抓取网站页面，通过抓取网站页面，清晰了解网站的架构，为搜索引擎蜘蛛指路，增加网站重要内容页面的收录。 安装hexo插件获得站点地图功能 12# npm install hexo-generator-sitemap --save# npm install hexo-generator-baidu-sitemap --save 在全局配置文件中增加如下配置 12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 生成静态文件及部署 1# hexo g 之后public目录下，sitemap.xml和baidusitemap.xml这两个文件就是生成的站点地图。可以看一下，其中的列出了网站中所有的文章链接。由于百度屏蔽了github，所以最好购买一个域名。更改地图url，可以更改全局配置文件“url”字段。 1# hexo d 部署至线上。之后可以通过访问 your_site_url/sitemap.xml 和 your_site_url/baidusitemap.xml来确定是否存在站点地图。 谷歌收录将生成的sitemap.xml提交到谷歌站长。 1“抓取” --&gt;&gt; “站点地图” --&gt;&gt; “添加站点地图” 百度收录百度提供如下方式对站点进行收录。 自动提交 主动推送 实时推送，通过百度引擎提供的目录格式进行推送。 自动推送 将百度引擎提供的一段JS代码插入到网站页面中，安装完成后即可实现链接自动推送功能。 如果用的主题是“next”，在主题配置文件中设置 “baidu_push” 为 “True”即可，不用做其他更改。 sitemap 提交站点地图。 手动提交这个应该是最费事的方式，估计…没人会用。 参考资料github+hexo提交到百度谷歌搜索引擎]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github网页之搜索]]></title>
    <url>%2FHexo%2Fhexo%E6%90%AD%E5%BB%BAgithub%E7%BD%91%E9%A1%B5%E4%B9%8B%E6%90%9C%E7%B4%A2.html</url>
    <content type="text"><![CDATA[随着站内文章数量增加，即使是作者本人查询也是比较困难。而“站内搜索”功能提供了一条便捷之路。 hexo建站支持多种站内搜索方式，现对几种常用的方式进行简要介绍。 Local Search添加百度/谷歌/本地 自定义站点内容搜索。 此方式比较简单，本站即用此方式。 安装 hexo-generator-searchdb，在站点的更目录下，执行 1# npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true Swiftype参考 next主题配置之Swiftype。 Algolia参考 next主题配置之Algolia。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github网页之统计]]></title>
    <url>%2FHexo%2Fhexo%E6%90%AD%E5%BB%BAgithub%E7%BD%91%E9%A1%B5%E4%B9%8B%E7%BB%9F%E8%AE%A1.html</url>
    <content type="text"><![CDATA[网站统计“不蒜子”是提供统计网站访问量的第三方插件。 对于使用了“next”主题的GitHub网页，由于主题源码已经支持“不蒜子”，所以在主题配置文件中进行如下更改即可。 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class="fa fa-user"&gt;&lt;/i&gt; 访问人数 site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class="fa fa-eye"&gt;&lt;/i&gt; 总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: false page_pv_header: &lt;i class="fa fa-file-o"&gt;&lt;/i&gt;浏览 page_pv_footer: 次 文章统计单独文章的统计量需要第三方插件“LeanCloud”来支持，“next”主题配置文件中“leancloud_visitors”即是此功能的配置域。 由配置而知，其需要两个数值：id 和 key。以下步骤给出如何获取这两个值。 “LeanCloud” 中创建应用，应用名随意。 点击应用进入，在“存储”选项的“数据”下创建Class，Class名为“Counter”（权限默认即可，若之后无计数可调整权限）。 “设置”选项下的“应用Key”中， App ID 和 App Key 即是所需数值。 将其值复制到主题配置文件，并将功能打开即可。 其他主题可参考 “使用LeanCloud平台为Hexo博客添加文章浏览量统计组件”。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github网页之基础]]></title>
    <url>%2FHexo%2Fhexo%E6%90%AD%E5%BB%BAgithub%E7%BD%91%E9%A1%B5%E4%B9%8B%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[此网页基于hexo和github，在搭建过程中及之后遇到许多问题，特此在这进行一下总结，以免其他人遇到相关问题。 1操作系统： Centos 7 github仓库创建项目创建一个项目，项目名为： github的用户名.github.io，github页面默认以此命名方式提供。 123创建项目流程：右上角头像 --&gt;&gt; Your profile --&gt;&gt; Repositories --&gt;&gt; "New" 按钮。查看github用户名流程：右上角头像 --&gt;&gt; settings，Name 即是。 项目设置开启”GitHub Pages“功能，github会自动创建一个页面。创建的页面地址在设置页面上也会显示，“Your site is published at” 之后即是。 123进入创建的项目：右上角头像 --&gt;&gt; Your profile ，选择上一步创建的项目进入。项目设置：settings --&gt;&gt; GitHub Pages，开启。 高级设置这一步是对页面代码进行备份，防止更换环境时“从头再来”。 在上面创建的项目中创建一个分支，分支名为“hexo”（分支名随意，此为方便起见）。 1项目页面会看到当前分支名，点击会有输入框，此输入框有创建功能。 设置“hexo”分支为默认分支。 1项目页面 --&gt;&gt; “branches” 至此，项目存在两个分支： master，用来存放静态页面，即hexo部署。 hexo，用来存放网页的原始文件。 hexo环境前提：需要安装npm，可参看其GitHub源码README.md，根据不同系统选择不同命令。 12# sudo yum install -y epel-release# sudo yum install -y nodejs npm 源码安装可参考 此处，也可参考官网安装方式。若遇到权限问题，可参考 此处。 全局安装hexo-cli指令。1# npm install hexo-cli -g 之后可查看hexo命令版本。 1# hexo -v 初始化hexo。将上面创建的项目拉取到本地。git clone 命令。进入拉到本地的项目目录。 1# hexo init 之后可以利用hexo进行本地调试。生成静态网页，命令如下 1# hexo g 创建进程，模拟网页服务器，命令如下 1# hexo s 根据提供的网址即可访问。”localhost” 可改为centos IP地址。 hexo配置github前提：本地环境可以提交代码，即需要配置好ssh key 、GitHub user name和email。 打开hexo全局设置文件“_config.yml”，配置“deploy”部分。 1234deploy: type: git repository: 创建的github项目地址，即git clone时，后接的地址 branch: master deploy是hexo部署页面时，静态网页提交的代码仓库及分支。 页面的标题、描述、作者和语言等都可以在此文件中修改，文件不大，可仔细看一下各字段功能含义。 安装插件： 1# npm install hexo-deployer-git --save 此插件可保证成功部署到github。 调试部署1# hexo new 'hello hexo' 创建的文件默认放置在source_posts目录下。可编辑文件，添加一些内容或不修改。 12# hexo g# hexo d 选项“d” 即部署。部署之后可能需要等待几分钟，然后再次查看github提供的网页，即可看到已经存在自己的提交。 主题hexo所有主题放置在themes文件夹下。默认主题为“landscape”，界面比较简单。 在hexo配置文件中，“theme”即是修改主题的字段。hexo主题列出了hexo支持的所有主题。下面以最流行的next主题为例。 Next官网提供了hexo使用方面的文档。 注意： 下载的源码需放置在“theme/next”目录下，可参考默认主题。 git clone下载的最新主题源码，需要将主题源码中.git文件夹改名，并将修改记录在一个文件中以防自己遗忘。 改名是因为本地源码需要进行备份，主题存在.git会与自己的项目目录下的.git冲突。 当需要更新主题时，可以将其改回，更新之后再次更改即可。 修改完主题后可在本地调试，调试没问题之后进行部署。 环境更换正常修改流程：部署 1# hexo clean; hexo g; hexo d 备份 1# hexo clean; git add -A; git commit -m "backup to git"; git push origin hexo 环境更换的重新操作 下载项目源码。 配置git基本环境。 安装nodejs和npm。 安装hexo-cli即可。 注意：不在需要初始化。初始化是创建一些必要文件，而这些文件都已经备份。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>hexo备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内核编译详解]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%91%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[内核主要通过Makefile把整个内核里的文件联系起来进行编译，最后得到内核镜像文件vmlinux。顶层Makefile文件多达1500行，本文仅分析其中比较重要的代码以概括内核编译流程。 123456789include $(srctree)/arch/$(SRCARCH)/Makefile…………init-y := init/drivers-y := drivers/ sound/ firmware/net-y := net/libs-y := lib/core-y := usr/…………core-y += kernel/ mm/ fs/ ipc/ security/ crypto/ block/ 类似init-y形式，是编译过程的核心，告知顶层Makefile哪些目录需要编译。之后进入相应目录进行编译，完成之后，再链接为built-in.o目标文件。例如，init-y编译完成后会在init目录下形成built-in.o文件。 include \$(srctree)/arch/\$(SRCARCH)/Makefile 表明需要把目录 \$(srctree)/arch/\$(SRCARCH)/ 编译进内核。如果在编译前，配置顶层Makefile的ARCH=arm，表明把arch/arm目录纳入内核编译。可根据需要调整编译架构。 一言概之，顶层Makefile指定需要编译的目录。 以x86架构为例，进入arch/x86目录后编译器就要依靠Makefile编译。 123456789101112131415…………head-y := arch/x86/kernel/head_$(BITS).ohead-y += arch/x86/kernel/head$(BITS).ohead-y += arch/x86/kernel/head.ohead-y += arch/x86/kernel/init_task.olibs-y += arch/x86/lib/# See arch/x86/Kbuild for content of core part of the kernelcore-y += arch/x86/# drivers-y are linked after core-ydrivers-$(CONFIG_MATH_EMULATION) += arch/x86/math-emu/drivers-$(CONFIG_PCI) += arch/x86/pci/………… \$(\)形式，根据.config是否有配该选项生效来决定用y或者n或者m代替$(**)。y或n表示是否编译进内核，而m表示以模块形式进行编译。 最后会继续回到根目录进行编译： 123456789101112131415init-y := $(patsubst %/, %/built-in.o, $(init-y))core-y := $(patsubst %/, %/built-in.o, $(core-y))drivers-y := $(patsubst %/, %/built-in.o, $(drivers-y))net-y := $(patsubst %/, %/built-in.o, $(net-y))libs-y1 := $(patsubst %/, %/lib.a, $(libs-y))libs-y2 := $(patsubst %/, %/built-in.o, $(libs-y))libs-y := $(libs-y1) $(libs-y2)…………vmlinux-init := $(head-y) $(init-y)vmlinux-main := $(core-y) $(libs-y) $(drivers-y) $(net-y)vmlinux-all := $(vmlinux-init) $(vmlinux-main)vmlinux-lds := arch/$(SRCARCH)/kernel/vmlinux.lds…………# vmlinux image - including updated kernel symbolsvmlinux: $(vmlinux-lds) $(vmlinux-init) $(vmlinux-main) vmlinux.o init-y := \$(patsubst %/, %/built-in.o, $(init-y)) 形式是把init-y变为 /init/built-in.o形式。vmlinux-*为vmlinux构成文件，所列出的最后一行会将这些文件链接为vmlinux镜像文件。]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>编译</tag>
        <tag>内核</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言关键字]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2FC%E8%AF%AD%E8%A8%80%E5%85%B3%E9%94%AE%E5%AD%97.html</url>
    <content type="text"><![CDATA[ANSI C标准C语言共有32个关键字，9种控制语句，程序书写形式自由，区分大小写。把高级语言的基本结构和语句与低级语言的实用性结合起来。 C 语言可以像汇编语言一样对位、字节和地址进行操作，而这三者是计算机最基本的工作单元。 关键字如下： auto break case char const continue default do double else enum extern float for goto if int long register return short signed sizeof static struct switch typedef union unsigned void volatile while 1999年12月16日，ISO推出了C99标准，该标准新增了5个C语言关键字： inline、restrict、_Bool、_Complex和_Imaginary。 2011年12月8日，ISO发布C语言的新标准C11，该标准新增了7个C语言关键字： _Alignas、_Alignof、_Atomic、_Static_assert、_Noreturn、_Thread_local和_Generic。 根据关键字的作用，可以将关键字分为数据类型关键字和流程控制关键字两大类。 数据类型关键字数据基本类型（5个） void ：声明函数无返回值或无参数，声明无类型指针，显式丢弃运算结果。 定义函数时，以void为形参，表示函数没有参数；为空时，表示函数需要不确定个参数。 对于C语言，C99标准只定义了如下两种可接受的函数原型： int main ( void ) int main ( int argc, char *argv[] ) char ：字符型类型数据，属于整型数据的一种。 int ：整型数据，通常为编译器指定的机器字长。 float ：单精度浮点型数据，属于浮点数据的一种。 double ：双精度浮点型数据，属于浮点数据的一种。 类型修饰关键字（4个） short ：修饰int，短整型数据，可省略被修饰的int。 long ：修饰int，长整形数据，可省略被修饰的int。 signed ：修饰整型数据，有符号数据类型。 unsigned ：修饰整型数据，无符号数据类型。 复杂类型关键字（5个） struct ：结构体声明。 union ：共用体声明。 enum ：枚举声明。 typedef ：声明类型别名。 sizeof ：得到特定类型或特定类型变量的大小。 存储级别关键字（6个） auto ：指定为自动变量，由编译器自动分配及释放。通常在栈上分配。 static ：指定为静态变量，分配在静态变量区，修饰函数时，指定函数作用域为文件内部。 定义全局静态变量。全局静态变量有以下特点： 在全局数据区内分配内存。 如果没有初始化，其默认值为0。 该变量在本文件内从定义开始到文件结束可见。 定义局部静态变量。局部静态变量有以下特点： 该变量在全局数据区分配内存。 如果不显示初始化，那么将被隐式初始化为0。 它始终驻留在全局数据区，直到程序运行结束。 其作用域为局部作用域，当定义它的函数或语句块结束时，其作用域随之结束。 定义静态函数。静态函数有以下特点： 静态函数只能在本源文件中使用。 在文件作用域中声明的inline函数默认为static。 register ：指定为寄存器变量，建议编译器将变量存储到寄存器中使用，也可以修饰函数形参，建议编译器通过寄存器而不是堆栈传递参数。 extern ：指定对应变量为外部变量，即标示变量或者函数的定义在别的文件中，提示编译器遇到此变量和函数时在其他模块中寻找其定义。 const ：与volatile合称“cv特性”，指定变量不可被当前线程/进程改变（但有可能被系统或其他线程/进程改变）。 volatile ：与const合称“cv特性”，指定变量的值有可能会被系统或其他进程/线程改变，强制编译器每次从内存中取得该变量的值。 易变性 不可优化性 顺序性 关于volatile可参考C/C++ Volatile关键词深度剖析。 流程控制关键字跳转结构（4个） return ：用在函数体中，返回特定值（或者是void值，即不返回值）。 continue ：结束当前循环，开始下一轮循环。 break ：跳出当前循环或switch结构。 goto ：无条件跳转语句。 分支结构（5个） if ：条件语句，后面不需要放分号。 else ：条件语句否定分支（与if连用）。 switch ：开关语句（多重分支语句）。 case ：开关语句中的分支标记。 default ：开关语句中的“其他”分支，可选。 循环结构（3个） for：一种循环语句。 do ：循环语句的循环体。 while ：循环语句的循环条件。 以上循环语句，当循环条件表达式为真则继续循环，为假则跳出循环。]]></content>
      <tags>
        <tag>C语言</tag>
        <tag>关键字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC编译过程详解]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2FGCC%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[GCC简介GCC（GNU Compiler Collection，GNU编译器套件），是由 GNU 开发的编程语言编译器。GCC原本作为GNU操作系统的官方编译器，现已被大多数类Unix操作系统（如Linux、BSD、Mac OS X等）采纳为标准的编译器，GCC同样适用于微软的Windows。GCC由自由软件基金会以GPL协议发布。 GCC 原名为 GNU C 语言编译器（GNU C Compiler），因为它原本只能处理 C语言。GCC 很快地扩展，变得可处理 C++。后来又扩展能够支持更多编程语言，如Fortran、Pascal、Objective-C、Java、Ada、Go以及各类处理器架构上的汇编语言等，所以改名GNU编译器套件（GNU Compiler Collection）。 编译器的工作是将源代码(通常使用高级语言编写)翻译成目标代码(通常是低级的目标代码或者机器语言)，在现代编译器的实现中，这个工作一般是分为两个阶段来实现： 编译器的前端接受输入的源代码，经过词法、语法和语义分析等等得到源程序的某种中间表示方式。 编译器的后端将前端处理生成的中间表示方式进行一些优化，并最终生成在目标机器上可运行的代码。 GCC 设计中有两个重要的目标： 硬件无关性：在构建支持不同硬件平台的编译器时，它的代码能够最大程度的被复用。 要生成高质量的可执行代码，这就需要对代码进行集中的优化。 为了实现这两个目标，GCC 内部使用了一种硬件平台无关的语言，它能对实际的体系结构做一种抽象，这个中间语言为 RTL(Register Transfer Language)。 GCC的工作流程GCC是一个驱动程序，它接受并解释命令行参数，根据对命令行参数分析的结果决定下一步动作，GCC提供了多种选项以达到控制GCC编译过程的目的，可以在 GCC 手册中查找这些编译选项的详细信息。 GCC的使用是比较简单的，但是要深入到其内部去了解编译流程，情况就比较复杂了。有两个比较好的方法解析GCC： 阅读源码，对感兴趣的函数可以跟踪过去看一看，阅读代码看起来可怕，但其实代码中会有很多注释说明它的功能，使得我们的阅读变得更简单一些，这种方法便于从整体上把握GCC。 调试GCC，就是使用调试器来跟踪 GCC 的编译过程，这样可以看清 GCC 编译的实际流程，也可以追踪我们感兴趣的细节部分。 GCC的基本用法1gcc [options] infile 其中options就是编译器所需要的参数，infile给出相关的文件名称。 下表列出一些常用的参数说明： 参数 说明 -E 预处理后即停止，不进行编译。 -S 编译后即停止，不进行汇编。 -c 编译或汇编源文件，但是不作连接。 -o file 指定输出文件为file，该选项不在乎GCC产生什么输出。 GCC的基本规则gcc所遵循的部分约定规则： .c为后缀的文件，C语言源代码文件； .a为后缀的文件，是由目标文件构成的档案库文件； .C，.cc或.cxx 为后缀的文件，是C++源代码文件且必须要经过预处理； .h为后缀的文件，是程序所包含的头文件； .i 为后缀的文件，是C源代码文件且不应该对其执行预处理； .ii为后缀的文件，是C++源代码文件且不应该对其执行预处理； .m为后缀的文件，是Objective-C源代码文件； .mm为后缀的文件，是Objective-C++源代码文件； .o为后缀的文件，是编译后的目标文件； .s为后缀的文件，是汇编语言源代码文件； .S为后缀的文件，是经过预处理的汇编语言源代码文件。 GCC的编译过程GCC的编译过程可以分为以下四个阶段：预处理（或预编译）、编译、汇编、链接，如下图所示： 以下面代码为例： 1234567#include &lt;stdio.h&gt;int main()&#123; printf("Hello World\n"); return 0;&#125; include两种方式： #include&lt;&gt; 引用的是编译器的类库路径里面的头文件。 #include” “ 引用的是程序目录的相对路径中的头文件。 gcc命令只是后台程序的包装，它会根据不同的参数要求去调用预编译编译程序cc1、汇编器as、链接器ld。 预编译123# gcc –E test.c –o test.i或者# cpp -o test.i test.c 以下为test.i部分内容： 1234567891011121314151617# 1 "test.c"# 1 "&lt;built-in&gt;"# 1 "&lt;command-line&gt;"# 1 "/usr/include/stdc-predef.h" 1 3 4# 1 "&lt;command-line&gt;" 2# 1 "test.c"# 1 "/usr/include/stdio.h" 1 3 4# 27 "/usr/include/stdio.h" 3 4# 1 "/usr/include/features.h" 1 3 4…………# 2 "test.c" 2int main()&#123; printf("Hello World\n"); return 0;&#125; 预处理过程主要处理那些源代码中以#开始的预处理指令，主要处理规则如下： 将所有的#define删除，并且展开所有的宏定义； 处理所有条件编译指令，如#if，#ifdef等； 处理#include预处理指令，将被包含的文件插入到该预处理指令的位置。该过程递归进行，及被包含的文件可能还包含其他文件。 删除所有的注释//和 /**/； 添加行号和文件标识，如#2 “hello.c” 2,以便于编译时编译器产生调试用的行号信息及用于编译时产生编译错误或警告时能够显示行号信息； 保留所有的#pragma编译器指令，因为编译器须要使用它们； 经过预编译后的.i文件不包含任何宏定义，因为所有的宏都已经被展开，并且包含的文件也已经被插入到.i文件中。所以当无法判断宏定义是否正确或头文件包含是否正确使，可以查看预编译后的文件来确定问题。 编译编译过程就是把预处理完的文件进行一系列词法分析、语法分析、语义分析及优化后生成相应的汇编代码文件。这个过程是整个程序构建的核心部分，也是最复杂的部分之一。 123# gcc -S test.i -o test.s或者# ccl -o test.s test.i 以下为test.s部分内容： 1234567891011121314151617181920212223242526 .file &quot;test.c&quot; .section .rodata.LC0: .string &quot;Hello World&quot; .text .globl main .type main, @functionmain:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl $.LC0, %edi call puts movl $0, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size main, .-main .ident &quot;GCC: (GNU) 4.8.5 20150623 (Red Hat 4.8.5-16)&quot; .section .note.GNU-stack,&quot;&quot;,@progbits 汇编汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可。 123# gcc -c test.c -o test.o或者# as -o test.o test.s test.o的内容为机器码，不能以文本形式方便的呈现(不过可以利用 objdump -S file 查看源码反汇编)。利用hexdump 查看如下： 123456789101112131415161718192021# hexdump test.o 0000000 457f 464c 0102 0001 0000 0000 0000 00000000010 0001 003e 0001 0000 0000 0000 0000 00000000020 0000 0000 0000 0000 0298 0000 0000 00000000030 0000 0000 0040 0000 0000 0040 000d 000a0000040 4855 e589 00bf 0000 e800 0000 0000 00b80000050 0000 5d00 48c3 6c65 6f6c 5720 726f 646c0000060 0000 4347 3a43 2820 4e47 2955 3420 382e0000070 352e 3220 3130 3035 3236 2033 5228 64650000080 4820 7461 3420 382e 352e 312d 2936 00000000090 0014 0000 0000 0000 7a01 0052 7801 011000000a0 0c1b 0807 0190 0000 001c 0000 001c 000000000b0 0000 0000 0015 0000 4100 100e 0286 0d4300000c0 5006 070c 0008 0000 2e00 7973 746d 626100000d0 2e00 7473 7472 6261 2e00 6873 7473 747200000e0 6261 2e00 6572 616c 742e 7865 0074 642e00000f0 7461 0061 622e 7373 2e00 6f72 6164 61740000100 2e00 6f63 6d6d 6e65 0074 6e2e 746f 2e650000110 4e47 2d55 7473 6361 006b 722e 6c65 2e610000120 6865 665f 6172 656d 0000 0000 0000 00000000130 0000 0000 0000 0000 0000 0000 0000 0000 链接链接器ld将各个目标文件组装在一起，解决符号依赖，库依赖关系，并生成可执行文件。如下形式： 1ld –static crt1.o crti.o crtbeginT.o test.o –start-group –lgcc –lgcc_eh –lc-end-group crtend.o crtn.o (省略了文件的路径名) 123# gcc -o test test.o或者# ld -o test test.o test程序调用了printf 函数，这个函数是标准C库中的一个函数，它保存在一个名为printf.o 的文件中，这个文件必须以某种方式合并到test.o的程序中。 链接器ld负责处理这种合并。结果得到test可执行文件，可以被加载到内存中由系统执行。 小结以上过程可以参考下图： 参考资料GCC编译过程分解 gcc编译过程简述 GCC-百度百科 GCC中文手册]]></content>
      <tags>
        <tag>GCC</tag>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 升级内核]]></title>
    <url>%2FCentos-%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8.html</url>
    <content type="text"><![CDATA[Linux 只是个内核。在正常操作期间，内核负责执行两个重要任务： 作为硬件和系统上运行的软件之间的接口。 尽可能高效地管理系统资源。 为此，内核通过内置的驱动程序或以后可作为模块安装的驱动程序与硬件通信。 随着新的设备和技术定期出来，如果我们想充分利用它们，保持最新的内核就很重要。此外，更新内核将帮助我们利用新的内核函数，并保护自己免受先前版本中发现的漏洞的攻击。 包管理工具升级检查已安装的内核版本让我们安装了一个发行版，它包含了一个特定版本的内核。为了展示当前系统中已安装的版本，我们可以： 1# uname -sr 12s 打印出内核名；r 打印出内核release版本; Linux官网可看到当前最新的内核版本。如果你当前使用的版本接近它的生命周期结束，那么在该日期后将不会提供更多的 bug 修复。关于更多信息，请参阅内核发布页。 在 CentOS 中升级内核大多数现代发行版提供了一种使用 yum 等包管理系统和官方支持的仓库升级内核的方法。这会使内核升级到仓库中可用的最新版本，而非官网中可用的最新版本。不幸的是，Red Hat 只允许使用前者升级内核。 与 Red Hat 不同，CentOS 允许使用 ELRepo，这是一个第三方仓库，可以将内核升级到最新版本。要在 CentOS 7 上启用 ELRepo 仓库，请运行： 12# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 12此方法基于ELRepo项目，因此需要导入此项目的KEY;release-7.0 是指 centos7 或 RH7，具体可参考 http://www.elrepo.org。 仓库启用后，你可以使用下面的命令列出可用的内核相关包： 1# yum --disablerepo="*" --enablerepo="elrepo-kernel" list available 12禁用ELRepo项目中所有存储库，之后使能名为“elrepo-kernel”的存储库。其他存储库有“elrepo-extras”、“elrepo-testing”。 接下来，安装最新的主线稳定内核： 1# yum --enablerepo=elrepo-kernel install kernel-ml 12kernel-ml 指 Linux mainline（主线）分支；kernel-lt 指 Linux longterm（长期维护）分支； 最后，重启机器并选择最新内核，接着运行下面的命令检查最新内核版本： 1# uname -sr 设置 GRUB 默认的内核版本打开并编辑 /etc/default/grub 并设置 GRUB_DEFAULT=0。意思是 GRUB 初始化页面的第一个内核将作为默认内核。 1234567GRUB_TIMEOUT=5GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"GRUB_DEFAULT=0GRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT="console"GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet"GRUB_DISABLE_RECOVERY="true" 接下来运行命令来重新创建内核配置： 1# grub2-mkconfig -o /boot/grub2/grub.cfg 重启并验证最新的内核已作为默认内核。 源码升级检查已安装的内核版本1# uname -sr 下载内核包并解压123# wget -c http://www.kernel.org/pub/linux/kernel/v3.0/linux-3.2.2.tar.bz2# tar jxvf linux-3.2.2.tar.bz2# cd linux-3.2.2 配置内核并安装清除环境变量1# make mrproper 删除所有的编译生成文件， 还有内核配置文件， 再加上各种备份文件。或者仅仅使用 1# make clean 删除大多数的编译生成文件， 但是会保留内核的配置文件.config， 还有足够的编译支持来建立扩展模块。 编译配置表配置时，以下方式选其一即可。 当前系统拷贝选取/boot/config-XXX文件，拷贝到代码目录。 1# cp /boot/config-3.10.0-693.2.2.el7.x86_64 .config defconfig命令1# make defconfig 获取当前系统参数并写入.config文件。之后可使用“make menuconfig”进行定制化。 问答式配置1# make localmodconfig 可一直回车选择默认配置。 命令界面配置1# make oldconfig 自动载入既有的.config配置文件，并且只有在遇到先前没有设定过的选项时，才会要求手动设定。 交互模式1# make menuconfig 找到以下选中选项并选中networking support → networking options → network packet filtering framework(netfilter) Core netfilter configuration 勾中”Netfilter connection tracking support” -m state相关模块是依赖它的，不选则没有。 将netbios name service protocal support(new) 编译成模块,不然后面升级iptables后启动时会出错。 勾中“Netfilter Xtables support (required for ip_tables)” IP: Netfilter Configuration 将 “IPv4 connection tracking support (require for NAT)” 编译成模块。 勾中IP tables support (required for filtering/masq/NAT) 。 将 “Full NAT” 下的 “MASQUERADE target support” 和 “REDIRECT target support” 编译成模块 其它模块可以根据自己的需要进行选择,若不懂可以参考内核配置手册。 编译内核生成内核文件 1# make bzImage 编译模块 1# make modules 或者直接 make 。 make = make bzImage + make modules。 安装模块 1# make modules_install 安装内核 1# make install 此步会： 重新制作内核映像文件，mkinitramfs -o /boot/initrd.img-XXX 更新制作的内核映像文件，update-initramfs -c -k XXX 自动修改系统引导配置，产生或更新boot/grub/grub.cfg启动文件，文件中增加了新内核版本号的启动项，update-grub2 之后重启即可。 重启之后内核未改变，则编辑 /etc/grub.conf 文件，将 default=1 改为 default=0。 内核包(YUM)升级获取源从下面三个地址从获取想要的内核： 官方源 香港源 scientific源，根据需要选择不同的源，例如，http://ftp.scientificlinux.org/linux/scientific/7.0/x86_64/updates/security/。 安装可下载到本地或者直接在线安装： 1# yum install -y http://hkg.mirror.rackspace.com/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.5.2-1.el7.elrepo.x86_64.rpm 修改文件升级完内核，自动按最新内核启动，修改/etc/default/grub，GRUB_DEFAULT=0。 重新编译内核启动文件，以后升级完内核也要执行一次。 1# grub2-mkconfig -o /boot/grub2/grub.cfg 1Tip: Centos 6.x 仅修改 /etc/grub.conf文件 default 即可。此文件中列出了所有的内核(title),顺序由 0 开始。 删除旧内核 列出当前所用内核 1# uname -sr 列出系统所有内核 1# rpm -qa | grep kernel Debian/ Ubuntu Linux 用户，使用： 1# dpkg --list 'linux-image*' 删除内核 1# rpm -e kernel-XXX 或 1# yum autoremove kernel-XXX Debian/ Ubuntu Linux 用户，使用： 1# apt-get remove kernel-XXX 参考资料Debian、CentOS 升级内核至当前最新稳定版 如何在 CentOS 7 中安装或升级最新的内核 CentOS Linux 升级内核步骤、方法 安全删除linux旧内核的方法]]></content>
      <tags>
        <tag>Centos</tag>
        <tag>内核升级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 进程]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-%E8%BF%9B%E7%A8%8B.html</url>
    <content type="text"><![CDATA[进程是任何多道程序设计操作系统中的基本概念。 进程、轻量级进程和线程进程是程序执行的一个实例，是充分描述程序已经执行到何种程度的数据结构的汇集。 从内核观点看，进程的目的就是担当分配系统资源（CPU时间、内存等）的实体。 拥有很多相对独立执行流的用户程序共享应用程序的大部分数据结构。一个进程由几个用户线程(或简单地说，线程)组成，每个线程都代表进程的一个执行流。大部分多线程应用程序都是用pthread(POSIX thread)库的标准库函数集编写的。 Linux内核的早期版本没有提供多线程应用的支持。从内核观点看，多线程应用程序仅仅是一个普通进程。多线程应用程序多个执行流的创建、处理、调度整个都是在用户态进行的(通常使用POSIX兼容的pthread库)。 但是，这种多线程应用程序的实现方式不那么令人满意。程序的多个线程，一个线程阻塞时，另外的线程也会阻塞。 Linux使用轻量级进程(lightwetght process)对多线程应用程序提供更好的支持。两个轻量级进程基本上可以共享一些资源，诸如地址空间、打开的文件等等。只要其中一个修改共享资源，另一个就立即查看这种修改。当然，当两个线程访问共享资源时就必须同步它们自己。 实现多线程应用程序的一个简单方式就是把轻量级进程与每个线程关联起来。这样，线程之间就可以通过简单地共享同一内存地址空间、同一打开文件集等来访问相同的应用程序数据结构集。同时，每个线程都可以由内核独立调度，以便一个睡眠的同时另一个仍然是可运行的。POSIX兼容的pthread库使用Linux轻量级进程有3个例子，它们是LinuxThreads， Native Posix Thread Library(NPTL)和IBM的下一代Posix线程包NGPT(Next Generation Posix Threading Package)。 POSIX兼容的多线程应用程序由支持“线程组”的内核来处理。在Linux中，一个线程组基本上就是实现了多线程应用的一组轻量级进程，对于像getpid()，kill()，和_exit()这样的一些系统调用，它像一个组织，起整体的作用。 进程描述符为了管理进程，内核必须对每个进程所做的事情进行清楚的描述。例如，内核必须知道进程的优先级，它是正在CPU上运行还是因某些事件而被阻塞，给它分配了什么样的地址空间，允许它访问哪个文件等等。这正是进程描述符(process descriptor)的作用——进程描述符都是task_struct类型结构，它的字段包含了与一个进程相关的所有信息。它不仅包含了很多进程属性的字段，而且一些字段还包括了指向其他数据结构的指针，依此类推。 进程状态进程描述符中的state字段描述了进程当前所处的状态。它由一组标志组成，其中每个标志描述一种可能的进程状态，且状态是互斥的。 标志 描述 可运行状态(TASK_RUNNING) 进程要么在CPU上执行，要么准备执行。 可中断的等待状态(TASK_INTERRUPTIBLE) 进程被挂起(睡眠)，直到某个条件变为真。产生一个硬件中断，释放进程正等待的系统资源，或传递一个信号都是可以唤醒进程的条件(把进程的状态放回到TASK_RUNNING。 不可中断的等待状态(TASK_UNINTERRUPTIBLE) 与可中断的等待状态类似，但是，把信号传递到睡眠进程不能改变它的状态。这种状态在一些特定的情况下(进程必须等待，直到一个不能被中断的事件发生)是很有用的。例如，当进程打开一个设备文件，其相应的设备驱动程序开始探测相应的硬件设备时会用到这种状态。探测完成以前，设备驱动程序不能被中断，否则，硬件设备会处于不可预知的状态。 暂停状态(TASK_STOPPED) 进程的执行被暂停。当进程接收到SIGSTOP， SIGTSTP， SIGTTIN或SIGTTOU信号后，进入暂停状态。 跟踪状态(TASK_TRACED) 进程的执行已由debugger程序暂停。当一个进程被另一个进程监控时(例如 debugger执行ptrace()系统调用监控一个测试程序)，任何信号都可以把这个进程置于TASK_TRACED状态。 僵死状态(EXIT_ZOMBIE) 可存放在exit_state字段中。进程的执行被终止，但是，父进程还没有发布wait4()或waitpid()系统调用来返回有关死亡进程的信息。发布wait()类系统调用前，内核不能丢弃包含在死进程描述符中的数据，因为父进程可能还需要它。 僵死撤消状态(EXIT_DEAD) 可存放在exit_state字段中。最终状态：由于父进程刚发出wait4()或waitpid()系统调用，因而进程由系统删除。为了防止其他执行线程在同一个进程上也执行wait()类系统调用(这是一种竞争条件)，而把进程的状态由僵死(EXIT_ZOMBIE)状态改为僵死撤消状态 (EXIT_DEAD)。 内核使用set_task_state和set_current_state宏分别设置指定进程的状态和当前执行进程的状态。此外，这些宏确保编译程序或CPU控制单元不把赋值操作与其他指令混合。混合指令的顺序有时会导致灾难性的后果。 标识一个进程能被独立调度的每个执行上下文都必须拥有它自己的进程描述符。因此，即使共享内核大部分数据结构的轻量级进程，也有它们自己的task_struct结构。 进程和进程描述符之间有非常严格的一一对应关系，这使得用32位进程描述符地址标识进程成为一种方便的方式。进程描述符指针指向这些地址，内核对进程的大部分引用是通过进程描述符指针进行的。 另一方面，类Unix操作系统允许用户使用一个叫做进程标识符process ID(或PID)的数来标识进程，PID存放在进程描述符的pid字段中。PID被顺序编号。不过，PID的值有一个上限，当内核使用的PID达到这个上限值的时候就必须开始循环使用已闲置的小PID号。系统管理员可以通过往/proc/sys/kernel/pid_max这个文件中写入一个值来更改PID的上限值。 由于循环使用PID编号，内核必须通过管理一个pidmap-array位图来表示当前已分配的PID号和闲置的PID号。因为一个页框包含32768个位，所以在32位体系结构中pidmap-array位图存放在一个单独的页中。然而，在64位体系结构中，当内核分配了超过当前位图大小的PID号时，需要为PID位图增加更多的页。系统会一直保存这些页不被释放。 Linux把不同的PID与系统中每个进程或轻量级进程相关联(多处理器系统上稍有例外)。这种方式能提供最大的灵活性，因为系统中每个执行上下文都可以被唯一地识别。 另一方面，Unix程序员希望同一组中的线程有共同的PID。例如，把指定PID的信号发送给组中的所有线程。事实上，POSIX 1003.1c标准规定一个多线程应用程序中的所有线程都必须有相同的PID。 遵照这个标准，Linux引入线程组的表示。一个线程组中的所有线程使用和该线程组的领头线程(thread group leader)相同的PID，也就是该组中第一个轻量级进程的PID，它被存入进程描述符的tgid字段中。getpid()系统调用（sys_getpid()）返回当前进程的tgid值而不是pid的值，因此，一个多线程应用的所有线程共享相同的PID。线程组的领头线程其tgid的值与pid的值相同，因而getpid()系统调用对这类进程所起的作用和一般进程是一样的。 从进程的PID中有效地导出它的描述符指针，效率至关重要，因为像kill()这样的很多系统调用使用PID表示所操作的进程。 进程描述符处理进程是动态实体，其生命周期范围从几毫秒到几个月。因此，内核必须能够同时处理很多进程，并把进程描述符存放在动态内存中，而不是放在永久分配给内核的内存区(线性地址在3GB之上)。 对每个进程来说，Linux都把两个不同的数据结构紧凑地存放在一个单独为进程分配的存储区域内：一个是内核态的进程堆栈，另一个是紧挨进程描述符的小数据结构thread_info，叫做线程描述符。这块存储区域的大小通常为8192个字节(两个页框)。考虑到效率的因素，内核让这8K空间占据连续的两个页框并让第一个页框的起始地址是213的倍数。当几乎没有可用的动态内存空间时，就会很难找到这样的两个连续页框，因为空闲空间可能存在大量碎片。因此，在80x86体系结构中，在编译时可以进行设置，以使内核栈和线程描述符跨越一个单独的页框(4096个字节)。 内核态的进程访问处于内核数据段的栈，这个栈不同于用户态的进程所用的栈。因为内核控制路径使用很少的栈，因此只需要几千个字节的内核态堆栈。所以，对栈和thread_info结构来说，8KB足够了。不过，当使用一个页框存放内核态堆栈和thread_info结构时，内核要采用一些额外的栈以防止中断和异常的深度嵌弃而引起的溢出。 下图显示了在2页(8KB)内存区中存放两种数据结构的方式。线程描述符驻留于这个内存区的开始，而栈从末端向下增长。该图还显示了分别通过task和thread_info字段使thread_info结构与task_struct结构互相关联。 esp寄存器是CPU栈指针，用来存放栈顶单元的地址。在80x86系统中，栈起始于末端，并朝这个内存区开始的方向增长。从用户态刚切换到内核态以后，进程的内核栈总是空的，因此，esp寄存器指向这个栈的顶端。 一旦数据写入堆栈，esp的值就递减。因为thread_info结构是52个字节长，因此内核栈能扩展到8140个字节。 C语言使用下列的联合结构方便地表示一个进程的线程描述符和内核栈： 1234union thread_union &#123; struct thread_info thread_info; unsigned long stack[THREAD_SIZE/sizeof(long)];/*2048 or 1024*/&#125;; 如上图所示，thread_info结构从0x015f 0000地址处开始存放，而栈从0x015f c000地址处开始存放。esp寄存器的值指向地址为0x012f a878的当前栈顶。 内核使用alloc_thread_info和free_thread_info宏分配和释放存储thread_info结构和内核栈的内存区。 标识当前进程从效率的观点来看，thread_info结构与内核态堆栈之间的紧密结合提供的主要好处是：内核很容易从esp寄存器的值获得当前在CPU上正在运行进程的thread_ info结构的地址。如果thread_union结构长度是8K(213字节)，则内核屏蔽掉esp的低13位有效位就可以获得thread_info结构的基地址。而如果thread_union结构长度是4K，内核需要屏蔽掉esp的低12位有效位。这项工作由current_thread_ info()函数来完成，它产生如下一些汇编指令： 123movl $0xffffe000， %ecx /*或者是用于4K堆栈的Oxfffff000*/andl %esp， %ecxmovl %ecx， p 这三条指令执行以后，p就包含在执行指令的CPU上运行的进程的thread_info结构的指针。 进程最常用的是进程描述符的地址而不是thread_info结构的地址。为了获得当前在CPU上运行进程的描述符指针，内核要调用current宏，该宏本质上等价于current_thread_info()-&gt;task，它产生如下汇编语言指令： 123movl $0xffffe000， %ecx /*或者是用于4K堆栈的Oxfffff000*/andl %esp，%ecxmovl (%ecx)，p 因为task字段在thread_info结构中的偏移量为0，所以执行完这三条指令之后，p就包含在CPU上运行进程的描述符指针。 current宏经常作为进程描述符字段的前缀出现在内核代码中，例如，current-&gt;pid返回在CPU上正在执行的进程的PID。 用栈存放进程描述符的另一个优点体现在多处理器系统上：如前所述，对于每个硬件处理器，仅通过检查栈就可以获得当前正确的进程。早先的Linux版本没有把内核栈与进程描述符存放在一起，而是强制引入全局静态变量current来标识正在运行进程的描述符。在多处理器系统上，有必要把current定义为一个数组，每一个元素对应一个可用CPU。 进程链表每个task_struct结构都包含一个list_head类型的tasks字段，这个类型的prev和next字段分别指向前面和后面的task_struct元素。 进程链表的头是init_task描述符，它是所谓的0进程(process 0)或swapper进程的进程描述符。init_task的tasks.prev字段指向链表中最后插入的进程描述符的tasks字段。 SET_LINKS和REMOVE_LINKS宏分别用于从进程链表中插入和删除一个进程描述符。这些宏考虑了进程间的父子关系。 还有一个很有用的宏就是for_each_process，它的功能是扫描整个进程链表，其定义如下： 1234define next_task(p) list_entry((p)-&gt;tasks.next， struct task_struct， tasks)define prev_task(p) list_entry((p)-&gt;tasks.prev， struct task_struct， tasks)define for_each_process(p) \ for (p = &amp;init_task ; (p = next_task(p)) != &amp;init_task ; ) 这个宏是循环控制语句，内核开发者利用它提供循环。注意init_task进程描述符是如何起到链表头作用的。这个宏从指向init_task的指针开始，把指针移到下一个任务，然后继续，直到又到init_task为止。在每一次循环时，传递给这个宏的参变量中存放的是当前被扫描进程描述符的地址，这与list_entry宏的返回值一样。 TASK_RUNNING状态的进程链表当内核寻找一个新进程在CPU上运行时，必须只考虑可运行进程(即处在TASK_RUNNING状态的进程)。 早先的Linux版本把所有的可运行进程都放在同一个叫做运行队列(runqueue)的链表中，由于维持链表中的进程按优先级排序开销过大，因此，早期的调度程序不得不为选择“最佳”可运行进程而扫描整个队列。 Linux2.6实现的运行队列有所不同。其目的是让调度程序能在固定的时间内选出“最佳”可运行进程，与队列中可运行的进程数无关。 提高调度程序运行速度的诀窍是建立多个可运行进程链表，每种进程优先权对应一个不同的链表。每个task_struct描述符包含一个list_head类型的字段run_list。如果进程的优先权等于k(其取值范围是0到139)，run_list字段把该进程链人优先权为k的可运行进程的链表中。此外，在多处理器系统中，每个CPU都有它自己的运行队列，即它自己的进程链表集。这是一个通过使数据结构更复杂来改善性能的典型例子：调度程序的操作效率的确更高了，但运行队列的链表却为此而被拆分成140个不同的队列！ 内核必须为系统中每个运行队列保存大量的数据，不过运行队列的主要数据结构还是组成运行队列的进程描述符链表，所有这些链表都由一个单独的prio_array_t数据结构来实现，其字段说明如下表所示： 类型 字段 描述 int nr_active 链表中进程描述符的数量 unsigned long [5] bitmap 优先权位图：当且仅当某个优先权的进程链表不为空时设置相应的位标志 struct list_head [140] queue 140个优先权队列的头结点 enqueue_task(p,array)函数把进程描述符插入某个运行队列的链表，其代码本质上等同于： 12345678static void enqueue_task(struct task_struct *p, prio_array_t *array)&#123; sched_info_queued(p); list_add_tail(&amp;p-&gt;run_list, array-&gt;queue + p-&gt;prio); __set_bit(p-&gt;prio, array-&gt;bitmap); array-&gt;nr_active++; p-&gt;array = array;&#125; 进程描述符的prio字段存放进程的动态优先权，而array字段是一个指针，指向当前运行队列的prio_array_t数据结构。类似地，dequeue_task(p,array)函数从运行队列的链表中删除一个进程的描述符。 进程间的关系程序创建的进程具有父/子关系。如果一个进程创建多个子进程时，则子进程之间具有兄弟关系。在进程描述符中引入几个字段来表示这些关系，表示给定进程P的这些字段列在下表中。进程0和进程1是由内核创建的。稍后我们将看到，进程1 (init)是所有进程的祖先。 字段名 说明 real_parent 指向创建了P的进程的描述符，如果P的父进程不再存在，就指向进程1 (init)的描述符(因此，如果用户运行一个后台进程而且退出了shell，后台进程就会成为init的子进程) parent 指向P的当前父进程(这种进程的子进程终止时，必须向父进程发信号)。它的值通常与real_parent一致，但偶尔也可以不同，例如，当另一个进程发出监控P的ptrace()系统调用请求时 children 链表的头部，链表中的所有元素都是P创建的子进程 sibling 指向兄弟进程链表中的下一个元素或前一个元素的指针，这些兄弟进程的父进程都是P 上图显示了一组进程间的亲属关系。进程P0接连创建了P1,P2和P3。进程P3又创建了P4。 特别要说明的是，进程之间还存在其他关系：一个进程可能是一个进程组或登录会话的领头进程，也可能是一个线程组的领头进程，它还可能跟踪其他进程的执行。下表列出了进程描述符中的一些字段，这些字段建立起了进程P和其他进程之间的关系。 字段名 说明 group_leader P所在进程组的领头进程的描述符指针 signal-&gt;pgrp P所在进程组的领头进程的PID tgid P所在线程组的领头进程的PID signal-&gt;session P的登录会话领头进程的PID ptrace_children 链表的头，该链表包含所有被debugge程序跟踪的P的子进程 ptrace_list 指向所跟踪进程其实际父进程链表的前一个和下一个元素(用于P被跟踪的时候) pidhash表及链表在几种情况下，内核必须能从进程的PID导出对应的进程描述符指针。例如，为kill()系统调用提供服务时就会发生这种情况：当进程P1希望向另一个进程P2发送一个信号时，P1调用kill()系统调用，其参数为P2的PID，内核从这个PID导出其对应的进程描述符，然后从P2的进程描述符中取出记录挂起信号的数据结构指针。 顺序扫描进程链表并检查进程描述符的pid字段是可行但相当低效的。为了加速查找，引入了4个散列表。需要4个散列表是因为进程描述符包含了表示不同类型PID的字段(见下表)，而且每种类型的PID需要它自己的散列表。 Hash表的类型 字段名 说明 PIDTYPE_PID pid 进程的PID PIDTYPE_TGID tgid 线程组领头进程的PID PIDTYPE_PGID pgrp 进程组领头进程的PID PIDTYPE_SID session 会话领头进程的PID 内核初始化期间动态地为4个散列表分配空间，并把它们的地址存入pid_hash数组。一个散列表的长度依赖于可用RAM的容量，例如：一个系统拥有512MB的RAM，那么每个散列表就被存在4个页框中，可以拥有2048个表项。 用pid_hashfn宏把PID转化为表索引，pid_hashfn宏展开为： 1#define pid_hashfn(nr) hash_long((unsigned long)nr, pidhash_shift) 变量pidhash_shift用来存放表索引的长度(以位为单位的长度，在我们的例子里是11位)。很多散列函数都使用hash_long()，在32位体系结构中它基本等价于： 12345678910#define GOLDEN_RATIO_PRIME 0x9e370001UL#define BITS_PER_LONG 32static inline unsigned long hash_long(unsigned long val, unsigned int bits)&#123; unsigned long hash = val; /* On some cpus multiply is faster, on others gcc will do shifts */ hash *= GOLDEN_RATIO_PRIME; /* High bits are more random, so use them. */ return hash &gt;&gt; (BITS_PER_LONG - bits);&#125; 因为在我们的例子中pidhash_shift等于11，所以pid_hashfn的取值范围是0到211-1=2047。 散列(hash)函数并不总能确保PID与表的索引一一对应。两个不同的PID散列(hash)到相同的表索引称为冲突(colliding)。 1魔数常量：常量0x9e370001究竟是怎么得出的？这种散列函数是基于表索引乘以一个适当的大数，于是结果溢出，就把留在32位变量中的值作为模数操作的结果。Knuth建议，要得到满意的结果，这个大乘数就应当是接近黄金比例的2^32的一个素数(32位是80x86寄存器的大小)。这里，0x9e370001就是接近2^32的一个素数。 Linux利用链表来处理冲突的PID：每一个表项是由冲突的进程描述符组成的双向链表。 具有链表的散列法比从PID到表索引的线性转换更优越，这是因为在任何给定的实例中，系统中的进程数总是远远小于32768(所允许的进程PID的最大数)。如果在任何给定的实例中大部分表项都不使用的话，那么把表定义为32768项会是一种存储浪费。 由于需要跟踪进程间的关系，PID散列表中使用的数据结构非常复杂。看一个例子：假设内核必须回收一个指定线程组中的所有进程，这意味着这些进程的tgid的值是相同的，都等于一个给定值。如果根据线程组号查找散列表，只能返回一个进程描述符，就是线程组领头进程的描述符。为了能快速返回组中其他所有进程，内核就必须为每个线程组保留一个进程链表。在查找给定登录会话或进程组的进程时也会有同样的情形。 PID散列表的数据结构解决了所有这些难题，因为它们可以为包含在一个散列表中的任何PID号定义进程链表。最主要的数据结构是四个pid结构的数组，它在进程描述符的pids字段中，下表显示pid结构的字段。 类型 名称 描述 int nr pid的数值 struct hlist_node pid_chain 链接散列表的下一个和前一个元素 struct list_head pid_list 每个pid的进程链表头 上图给出了PIDTYPE_TGID类型散列表的例子。pid_hash数组的第二个元素存放散列表的地址，也就是用hlist_head结构的数组表示链表的头。在散列表第71项为起点形成的链表中，有两个PID号为246和4351的进程描述符(双箭头线表示一对向前和向后的指针)。PID的值存放在pid结构的nr字段中，而pid结构在进程描述符中。(顺便提一下，由于线程组的号和它的首创者的PID相同，因此这些PID值也存在进程描述符的pid字段中。)我们考虑线程组4351的PID链表:散列表中的进程描述符的pid_list字段中存放链表的头，同时每个PID链表中指向前一个元素和后一个元素的指针也存放在每个链表元素的pid_list字段中。 下面是处理PID散列表的函数和宏： 名称 描述 #define do_each_task_pid(who, type, task) #define while_each_task_pid(who, type, task) 标记do-while循环的开始和结束，循环作用在PID值等于nr的PID链表上，链表的类型由参数type给出，task参数指向当前被扫描的元素的进程描述符。 find_task_by_pid_type(int type, int nr) 在type类型的散列表中查找PID等于nr的进程。该函数返回所匹配的进程描述符指针，若没有匹配的进程，函数返回NULL。 #define find_task_by_pid(nr) 与find_task_by_pid_type(int type, int nr)相同。 attach_pid(task_t *task, enum pid_type type, int nr) 把task指向的PID等于nr的进程描述符插人type类型的散列表中。如果一个PID等于nr的进程描述符已经在散列表中，这个函数就只把task插入已有的PID进程链表中。 detach_pid(task_t *task, enum pid_type type) 从type类型的PID进程链表中删除task所指向的进程描述符。如果删除后PID进程链表没有变为空，则函数终止，否则，该函数还要从type类型的散列表中删除进程描述符。最后，如果PID的值没有出现在任何其他的散列表中，为了这个值能够被反复使用，该函数还必须清除PID位图中的相应位。 next_thread(const task_t *p) 返回PIDTYPE_TGID类型的散列表链表中task指示的下一个轻量级进程的进程描述符。由于散列链表是循环的，若应用于传统的进程，那么该宏返回进程本身的描述符地址。 如何组织进程运行队列链表把处于TASK_RUNNING状态的所有进程组织在一起。没有为处于TASK_STOPPED, EXIT_ZOMBIE或EXIT_DEAD状态的进程建立专门的链表。由于对处于暂停、僵死、死亡状态进程的访问比较简单，或者通过PID,或者通过特定父进程的子进程链表，所以不必对这三种状态进程分组。 等待队列等待队列在内核中有很多用途，尤其用在中断处理、进程同步及定时。进程必须经常等待某些事件的发生，例如，等待一个磁盘操作的终止，等待释放系统资源，或等待时间经过固定的间隔。等待队列实现了在事件上的条件等待:希望等待特定事件的进程把自己放进合适的等待队列，并放弃控制权。因此，等待队列表示一组睡眠的进程，当某一条件变为真时，由内核唤醒它们。 等待队列由双向链表实现，其元素包括指向进程描述符的指针。每个等待队列都有一个等待队列头(wait queue head)，等待队列头是一个类型为wait_queue_head_t的数据结构： 12345struct __wait_queue_head &#123; spinlock_t lock; struct list_head task_list;&#125;;typedef struct __wait_queue_head wait_queue_head_t; 因为等待队列是由中断处理程序和主要内核函数修改的，因此必须对其双向链表进行保护以免对其进行同时访问而导致不可预测的后果。同步是通过等待队列头中的lock自旋锁达到的。task_list字段是等待进程链表的头。 等待队列链表中的元素类型为wait_queue_t： 12345678struct __wait_queue &#123; unsigned int flags;#define WQ_FLAG_EXCLUSIVE 0x01 struct task_struct * task; wait_queue_func_t func; struct list_head task_list;&#125;;typedef struct __wait_queue wait_queue_t; 等待队列链表中的每个元素代表一个睡眠进程，该进程等待某一事件的发生;它的描述符地址存放在task字段中。task_list字段中包含的是指针，由这个指针把一个元素链接到等待相同事件的进程链表中。 然而，要唤醒等待队列中所有睡眠的进程有时并不方便。例如，如果两个或多个进程正在等待互斥访问某一要释放的资源，仅唤醒等待队列中的一个进程才有意义。这个进程占有资源，而其他进程继续睡眠。(这就避免了所谓“雷鸣般兽群”问题，即唤醒多个进程只为了竟争一个资源，而这个资源只能有一个进程访问，结果是其他进程必须再次回去睡眠。) 因此，有两种睡眠进程:互斥进程(等待队列元素的flags字段为1)由内核有选择地唤醒，而非互斥进程(falgs值为0)总是由内核在事件发生时唤醒。等待访问临界资源的进程就是互斥进程的典型例子。等待相关事件的进程是非互斥的。例如，我们考虑等待磁盘传输结束的一组进程:一但磁盘传输完成，所有等待的进程都会被唤醒。正如我们将在下面所看到的那样，等待队列元素的fun。字段用来表示等待队列中睡眠进程应该用什么方式唤醒。 等待队列操作DECLARE_WAIT_QUEUE_HEAD(name)宏静态地声明一个叫name的等待队列的头变量并对该变量的lock和task_list字段进行初始化。函数init_waitqueue_head()可以初始化动态分配的等待队列的头变量。 函数init_waitqueue_entry(wait_queue_t q, struct task_struct p)初始化wait_queue_t结构的变量q： 123q-&gt;flags = 0;q-&gt;task = p;q-&gt;func = default_wake_function; 非互斥进程p将由default_wake_function()唤醒： 12345int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)&#123; task_t *p = curr-&gt;task; return try_to_wake_up(p, mode, sync);&#125; 也可以选择DEFINE_WAIT宏声明一个wait_queue_t类型的新变量，并用CPU上运行的当前进程的描述符和唤醒函数autoremove_wake_function()的地址初始化这个新变量。这个函数调用default_wake_function()来唤醒睡眠进程，然后从等待队列的链表中删除对应的元素(每个等待队列链表中的一个元素其实就是指向睡眠进程描述符的指针)。最后，内核开发者可以通过init_waitqueue_func_entry()函数来自定义唤醒函数，该函数负责初始化等待队列的元素。 一旦定义了一个元素，必须把它插人等待队列。add_wait_queue()函数把一个非互斥进程插入等待队列链表的第一个位置。add_wait_queue_exclusive()函数把一个互斥进程插入等待队列链表的最后一个位置。remove_wait_queue()函数从等待队列链表中删除一个进程。waitqueue_active()函数检查一个给定的等待队列是否为空。 要等待特定条件的进程可以调用如下列表中的任何一个函数： sleep_on()对当前进程进行操作： 12345678910111213141516171819202122232425#define SLEEP_ON_VAR \ unsigned long flags; \ wait_queue_t wait; \ init_waitqueue_entry(&amp;wait, current);#define SLEEP_ON_HEAD \ spin_lock_irqsave(&amp;q-&gt;lock,flags); \ __add_wait_queue(q, &amp;wait); \ spin_unlock(&amp;q-&gt;lock);#define SLEEP_ON_TAIL \ spin_lock_irq(&amp;q-&gt;lock); \ __remove_wait_queue(q, &amp;wait); \ spin_unlock_irqrestore(&amp;q-&gt;lock, flags);void fastcall __sched sleep_on(wait_queue_head_t *q)&#123; SLEEP_ON_VAR current-&gt;state = TASK_UNINTERRUPTIBLE; SLEEP_ON_HEAD schedule(); SLEEP_ON_TAIL&#125; 该函数把当前进程的状态设置为TASK_UNINTERRUPTIBLE，并把它插入到特定的等待队列。然后，它调用调度程序，而调度程序重新开始另一个程序的执行。当睡眠进程被唤醒时，调度程序重新开始执行sleep_on()函数，把该进程从等待队列中删除。 interruptible_sleep_on()函数sleep_on()函数是一样的，但此函数把当前进程状态设置为TASK_INTERRUPTIBLE，因此，接受一个信号就可以唤醒当前进程。 12345678910void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)&#123; SLEEP_ON_VAR current-&gt;state = TASK_INTERRUPTIBLE; SLEEP_ON_HEAD schedule(); SLEEP_ON_TAIL&#125; sleep_on_timeout()和interruptible_sleep_on timeout()与前面函数类似，但它们允许调用者定义一个时间间隔，过了这个间隔以后，进程将由内核唤醒。为了做到这点，它们调用schedule_timeout()函数而不是schedule()函数。 在Linux 2.6中引入的prepare_to_wait(), prepare_to_wait_exclusive()和finish_wait()函数提供了另外一种途径来使当前进程在一个等待队列中睡眠。它们的典型应用如下： 1234567DEFINE_WAIT(wait);prepare_to_wait exclusive(&amp;wq, &amp;wait，TASK_INTERRUPTIBLE); /*wq是等待队列的头*/...if(!condition) schedule();finish_wait(&amp;wq,&amp;wait) 函数prepare_to_wait()和prepare_to_wait_ exclusive()用传递的第三个参数设置进程的状态，然后把等待队列元素的互斥标志flag分别设置为0(非互斥)或1(互斥)，最后，把等待元素wait插人到以wq为头的等待队列的链表中。 进程一但被唤醒就执行finish_wait()函数，它把进程的状态再次设置为TASK RUNNING(仅发生在调用schedule()之前，唤醒条件变为真的情况下)，并从等待队列中删除等待元素(除非这个工作已经由唤醒函数完成)。 wait_event和wait_event_interruptible宏使它们的调用进程在等待队列上睡眠，一直到修改了给定条件为止。例如，宏wait_event(wq,condition)本质上实现下面的功能： 12345678910111213141516171819#define __wait_event(wq, condition) \do &#123; \ DEFINE_WAIT(__wait); \ \ for (;;) &#123; \ prepare_to_wait(&amp;wq, &amp;__wait, TASK_UNINTERRUPTIBLE); \ if (condition) \ break; \ schedule(); \ &#125; \ finish_wait(&amp;wq, &amp;__wait); \&#125; while (0)#define wait_event(wq, condition) \do &#123; \ if (condition) \ break; \ __wait_event(wq, condition); \&#125; while (0) sleep_on()类函数在以下条件下不能使用，那就是必须测试条件并且当条件还没有得到验证时又紧接着让进程去睡眠;由于那些条件是众所周知的竞争条件产生的根源，所以不鼓励这样使用。此外，为了把一个互斥进程插人等待队列，内核必须使用prepare_to_wait_exclusive()函数[或者只是直接调用add_wait_queue_exclusive()]。所有其他的相关函数把进程当作非互斥进程来插人。最后，除非使用DEFINE_WAIT或finish_wait()，否则内核必须在唤醒等待进程后从等待队列中删除对应的等待队列元素。 内核通过下面的任何一个宏唤醒等待队列中的进程并把它们的状态置为TASK_RUNNING：wake_up、wake_up_nr、wake_up_all、wake_up_interruptible、wake_up_interruptible_nr、wake_up_interruptible_all、wake_up_interruptible_sync和wake_up_locked。从每个宏的名字我们可以明白其功能： 所有宏都考虑到处于TASK_INTERRUPTIBLE状态的睡眠进程；如果宏的名字中不含字符串”interruptible”，那么处于TASK_UNINTERRUPTIBLE状态的睡眠进程也被考虑到。 所有宏都唤醒具有请求状态的所有非互斥进程(参见上一项)。 名字中含有“nr”字符串的宏唤醒给定数的具有请求状态的互斥进程；这个数字是宏的一个参数。名字中含有“all”字符串的宏唤醒具有请求状态的所有互斥进程。最后，名字中不含“nr”或“all”字符串的宏只唤醒具有请求状态的一个互斥进程。 名字中不含有“sync”字符串的宏检查被唤醒进程的优先级是否高于系统中正在运行进程的优先级，并在必要时调用schedule()。这些检查并不是由名字中含有“sync”字符串的宏进行的，造成的结果是高优先级进程的执行稍有延迟。 wake_up_locked宏和wake_up宏相类似，仅有的不同是当wait_queue_head_t中的自旋锁已经被持有时要调用wake_up_locked。 例如，wake_up宏等价于下列代码片段： 12345678910void wake_up(wait_queue_head_t *q)&#123; struct list_head *tmp; wait_queue_t *curr; list_for_each(tmp, &amp;q-&gt;task_list)&#123; curr=list_entry(tmp, wait_queue_t，task_list) if (curr-&gt;func(curr, TASK_INTERRUPTIBLE|TASK_UNINTERRUPTIBLE, 0, NULL) &amp;&amp; curr-&gt;flags) break; &#125;&#125; list_for_each宏扫描双向链表q-&gt;task_list中的所有项，即等待队列中的所有进程。对每一项，list_entry宏都计算wait_queue_t变量对应的地址。这个变量的func字段存放唤醒函数的地址，它试图唤醒由等待队列元素的task字段标识的进程。如果一个进程已经被有效地唤醒(函数返回1)并且进程是互斥的(curr-&gt;flags等于1)，循环结束。因为所有的非互斥进程总是在双向链表的开始位置，而所有的互斥进程在双向链表的尾部，所以函数总是先唤醒非互斥进程然后再唤醒互斥进程，如果有进程存在的话(一个队列同时包含互斥和非互斥进程的情况是非常罕见的)。 进程资源限制每个进程都有一组相关的资源限制(resource limit)，限制指定了进程能使用的系统资源数量。这些限制避免用户过分使用系统资源(CPU、磁盘空间等)。 对当前进程的资源限制存放在current-&gt;signal-&gt;rlim字段，即进程的信号描述符的一个字段。该字段是类型为rlimit结构的数组，每个资源限制对应一个元素： 1234struct rlimit &#123; unsigned long rlim_cur; unsigned long rlim_max;&#125;; 字段名 说明 RLIMIT_CPU (0) 进程使用CPU的最长时间(以秒为单位)。如果进程超过了这个限制，内核就向它发一个SIGXCPU信号，然后如果进程还不终止，再发一个SIGKILL信号 RLIMIT_FSIZE 文件大小的最大值(以字节为单位)。如果进程试图把一个文件的大小扩充到大于这个值，内核就给这个进程发SIGXFSZ信号 RLIMIT_DATA 堆大小的最大值(以字节为单位)。在扩充进程的堆之前，内核检查这个值 RLIMIT_STACK 栈大小的最大值(以字节为单位)。内核在扩充进程的用户态堆栈之前检查这个值 RLIMIT_CORE 内存信息转储文件的大小(以字节为单位)。当一个进程异常终止时，内核在进程的当前目录下创建内存信息转储文件之前检查这个值。如果这个限制为0，那么，内核就不创建这个文件 RLIMIT_RSS (5) 进程所拥有的页框最大数(目前是非强制的) RLIMIT_NPROC 用户能拥有的进程最大数(参见本章“clone(), fork()及vfork()系统调用”一节) RLIMIT_NOFILE 打开文件描述符的最大数。当打开一个新文件或复制一个文件描述符时，内核检查这个值 RLIMIT_MEMLOCK 非交换内存的最大值(以字节为单位)。当进程试图通过mlock()或mlockall()系统调用锁住一个页框时，内核检查这个值 RLIMIT_AS 进程地址空间的最大数(以字节为单位)。当进程使用malloc()或相关函数扩大它的地址空间时，内核检查这个值 RLIMIT_LOCKS (10) 文件锁的最大值(目前是非强制的) RLIMIT_SIGPENDING 进程挂起信号的最大数 RLIMIT_MSGQUEUE POSIX消息队列中的最大字节数 RLIMIT_NICE RLIMIT_RTPRIO 最大实时优先级 rlim_cur 表示资源的当前限制，例如 current-&gt;signal-&gt;rlim[RLIMIT_CPU]，rlim_cur表示正运行进程所占用CPU时间的当前限制。 rlim_max字段是资源限制所允许的最大值。利用getrlimit()和setrlimit()系统调用，用户总能把一些资源的rlim_cur限制增加到rlim_max。然而，只有超级用户(或更确切地说，具有CAP_SYS_RESOURCE权能的用户)才能改变rlim_max字段，或把rlim_cur字段设置成大于相应rlim_max字段的一个值。 大多数资源限制包含值RLIMIT_INFINITY(0xffffffff)，它意味着没有对相应的资源施加用户限制(当然，由于内核设计上的限制，可用RAM、可用磁盘空间等，实际的限制还是存在的)。然而，系统管理员可以给一些资源选择施加更强的限制。只要用户注册进系统，内核就创建一个由超级用户拥有的进程，超级用户能调用setrlimit()以减少一个资源rlim_max和rlim_cur字段的值。随后，同一进程执行一个login shell, 该进程就变为由用户拥有。由用户创建的每个新进程都继承其父进程rlim数组的内容，因此，用户不能忽略系统强加的限制。 进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换(process switch)、任务切换(task switch)或上下文切换(context switch)。 硬件上下文每个进程有属于自己的地址空间，但共享CPU寄存器。因此，在恢复一个进程的执行之前，内核必须确保每个寄存器装入了挂起进程时的值。 进程恢复执行前必须装入寄存器的一组数据称为硬件上下文(hardware context)。硬件上下文是进程可执行上下文的一个子集，因为可执行上下文包含进程执行时需要的所有信息。在Linux中，进程硬件上下文的一部分存放在TSS段，而剩余部分存放在内核态堆栈中。 在下面的描述中，我们假定用prev局部变量表示切换出的进程的描述符，next表示切换进的进程的描述符。因此，我们把进程切换定义为这样的行为：保存prev硬件上下文，用next硬件上下文代替prev。因为进程切换经常发生，因此减少保存和装入硬件上下文所花费的时间是非常重要的。 早期的Linux版本利用80x86体系结构所提供的硬件支持，并通过far jmp指令(far jmp 指令既修改cs寄存器，也修改eip寄存器，而简单的jmp指令只修改eip寄存器)跳到next进程TSS描述符的选择符来执行进程切换。当执行这条指令时，CPU通过自动保存原来的硬件上下文，装人新的硬件上下文来执行硬件上下文切换。但基于以下原因，Linux 2.6使用软件执行进程切换： 通过一组mov指令逐步执行切换，这样能较好地控制所装入数据的合法性。尤其是，这使检查ds和es段寄存器的值成为可能，这些值有可能被恶意伪造。当用单独的far jmp指令时，不可能进行这类检查。 旧方法和新方法所需时间大致相同。然而，尽管当前的切换代码还有改进的余地，却不能对硬件上下文切换进行优化。 进程切换只发生在内核态。在执行进程切换之前，用户态进程使用的所有寄存器内容都已保存在内核态堆栈上，这也包括ss和esp这对寄存器的内容(存储用户态堆栈指针的地址)。 任务状态段80x86体系结构包括了一个特殊的段类型，叫任务状态段(Task State Segment ,TSS)来存放硬件上下文。尽管Linux并不使用硬件上下文切换，但是强制它为系统中每个不同的CPU创建一个TSS。这样做的两个主要理由为： 当80x86的一个CPU从用户态切换到内核态时，它就从TSS中获取内核态堆栈的地址。 当用户态进程试图通过in或out指令访问一个I/O端口时，CPU需要访问存放在TSS中的I/O许可权位图(Permission Bitmap)以检查该进程是否有访问端口的权力。 tss_struct结构描述TSS的格式。init_tss数组为系统上每个不同的CPU存放一个TSS。在每次进程切换时，内核都更新TSS的某些字段以便相应的CPU控制单元可以安全地检索到它需要的信息。因此，TSS反映了CPU上的当前进程的特权级，但不必为没有在运行的进程保留TSS。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546Processor.h (include\asm-i386)struct tss_struct &#123; unsigned short back_link,__blh; unsigned long esp0; unsigned short ss0,__ss0h; unsigned long esp1; unsigned short ss1,__ss1h; /* ss1 is used to cache MSR_IA32_SYSENTER_CS */ unsigned long esp2; unsigned short ss2,__ss2h; unsigned long __cr3; unsigned long eip; unsigned long eflags; unsigned long eax,ecx,edx,ebx; unsigned long esp; unsigned long ebp; unsigned long esi; unsigned long edi; unsigned short es, __esh; unsigned short cs, __csh; unsigned short ss, __ssh; unsigned short ds, __dsh; unsigned short fs, __fsh; unsigned short gs, __gsh; unsigned short ldt, __ldth; unsigned short trace, io_bitmap_base; /* * The extra 1 is there because the CPU will access an * additional byte beyond the end of the IO permission * bitmap. The extra byte must be all 1 bits, and must * be within the limit. */ unsigned long io_bitmap[IO_BITMAP_LONGS + 1]; /* * Cache the current maximum and the last task that used the bitmap: */ unsigned long io_bitmap_max; struct thread_struct *io_bitmap_owner; /* * pads the TSS to be cacheline-aligned (size is 0x100) */ unsigned long __cacheline_filler[35]; /* * .. and then another 0x100 bytes for emergency kernel stack */ unsigned long stack[64];&#125; __attribute__((packed)); 每个TSS有它自己8字节的任务状态段描述符(Task State Segment Descriptor, TSSD )。这个描述符包括指向TSS起始地址的32位Base字段，20位Limit字段。TSSD的S标志位被清0，以表示相应的TSS是系统段。 Type字段置为11或9以表示这个段实际上是一个TSS。在Intel的原始设计中，系统中的每个进程都应当指向自己的TSS；Type字段的第二个有效位叫做Busy位，如果进程正由CPU执行，则该位置1，否则置0。在Linux的设计中，每个CPU只有一个TSS,因此，Busy位总置为1。 由Linux创建的TSSD存放在全局描述符表(GDT)中，GDT的基地址存放在每个CPU的gdtr寄存器中。每个CPU的tr寄存器包含相应TSS的TSSD选择符，也包含了两个隐藏的非编程字段：TSSD的Base字段和Limit字段。这样，处理器就能直接对TSS寻址而不用从GDT中检索TSS的地址。 thread字段在每次进程切换时，被替换进程的硬件上下文必须保存在别处。不能像Intel原始设计那样把它保存在TSS中，因为Linux为每个处理器而不是为每个进程使用TSS。 因此，每个进程描述符包含一个类型为thread_struct的thread字段，只要进程被切换出去，内核就把其硬件上下文保存在这个结构中。随后我们会看到，这个数据结构包含的字段涉及大部分CPU寄存器，但不包括诸如eax,ebx等这些通用寄存器，它们的值保留在内核堆栈中。 执行进程切换进程切换可能只发生在精心定义的点：schedule()函数。从本质上说，每个进程切换由两步组成： 切换页全局目录以安装一个新的地址空间。 切换内核态堆栈和硬件上下文，因为硬件上下文提供了内核执行新进程所需要的所有信息，包含CPU寄存器。 switch_to宏schedule() -&gt; context_switch() -&gt; switch_to() 进程切换的第二步由switch_to宏执行。它是内核中与硬件关系最密切的例程之一，首先，该宏有三个参数，它们是prev，next和last。prev和next是局部变量prev和next的占位符，分别表示被替换进程和新进程描述符的地址在内存中的位置。 在任何进程切换中，涉及到三个进程而不是两个。假设内核决定暂停进程A而激活进程B。在schedule()函数中，prev指向A的描述符而next指向B的描述符。switch_to宏一但使A暂停，A的执行流就冻结。 随后，当内核想再次此激活A，就必须暂停另一个进程C(这通常不同于B)，于是就要用prev指向C而next指向A来执行另一个switch_to宏。当A恢复它的执行流时，就会找到它原来的内核栈，于是prev局部变量还是指向A的描述符而next指向B的描述符。此时，代表进程A执行的内核就失去了对C的任何引用。但是，事实表明这个引用对于完成进程切换是很有用的。 switch_to宏的最后一个参数是输出参数，它表示宏把进程C的描述符地址写在内存的什么位置了(这是在A恢复执行之后完成的)。在进程切换之前，宏把第一个输入参数prev(即在A的内核堆栈中分配的prev局部变量)表示的变量的内容存人CPU的eax寄存器。在完成进程切换，A已经恢复执行时，宏把CPU的eax寄存器的内容写入由第三个输出参数——last所指示的A在内存中的位置。因为CPU寄存器不会在切换点发生变化，所以C的描述符地址也存在内存的这个位置。在schedule()执行过程中，参数last指向A的局部变量prev，所以prev被C的地址覆盖。 下图显示了进程A，B，C内核堆栈的内容以及eax寄存器的内容。必须注意的是：图中显示的是在被eax寄存器的内容覆盖以前的prev局部变量的值。 由于switch_to宏采用扩展的内联汇编语言编码，所以可读性比较差。实际上这段代码通过特殊位置记数法使用寄存器，而实际使用的通用寄存器由编译器自由选择。我们将采用标准汇编语言而不是麻烦的内联汇编语言来描述switch_to宏在80x86微处理器上所完成的典型工作。 在eax和edx寄存器中分别保存prev和next的值： 12movl prev, %eaxmovl next, %edx 把eflags和ebp寄存器的内容保存在prev内核栈中。必须保存它们的原因是编译器认为在switch_to结束之前它们的值应当保持不变。 12pushflpushl %ebp 把esp的内容保存到prev-&gt;thread.esp中以使该字段指向prev内核栈的栈顶： 1movl %esp, 484(%eax) 484(%eax)操作数表示内存单元的地址为eax内容加上484。 把next-&gt;thread.esp装人esp。此时，内核开始在next的内核栈上操作，因此这条指令实际上完成了从prev到next的切换。由于进程描述符的地址和内核栈的地址紧挨着，所以改变内核栈意味着改变当前进程。 1movl 484(%edx)，%esp 把标记为1的地址(本节后面所示)存入prev-&gt;thread.eip。当被替换的进程重新恢复执行时，进程执行被标记为1的那条指令： 1movl $lf, 480(%eax) 宏把next-&gt;thread.eip的值(绝大多数情况下是一个被标记为1的地址)压入next的内核栈： 1pushl 480(%edx) 跳到__switch_to() C函数： 1jmp __switch_to 这里被进程B替换的进程A再次获得CPU:它执行一些保存eflags和ebp寄存器内容的指令，这两条指令的第一条指令被标记为1。 1231: popl %%ebp popfl 注意这些pop指令是怎样引用prev进程的内核栈的。当进程调度程序选择了prev作为新进程在CPU上运行时，将执行这些指令。于是，以prev作为第二个参数调用switch_to。因此，esp寄存器指向prev的内核栈。 拷贝eax寄存器(上面步骤1中被装载)的内容到switch_to宏的第三个参数last标识的内存区域中 1movl %eax, last 正如先前讨论的，eax寄存器指向刚被替换的进程的描述符(当前执行的schedule()函数重新使用了prev局部变量，于是汇编语言指令就是：movl %eax, prev)。 __switch_to()函数__switch_to()函数执行大多数开始于switch_to()宏的进程切换。这个函数作用于prev_p和next_p参数，分别表示前一个进程和新进程。这个函数的调用不同于一般函数的调用，因为__switch_to()从eax和edx取参数prev_p和next_p(fastcall，ecx、edx，我们在前面已看到这些参数就是保存在那里)，而不像大多数函数一样从栈中取参数。为了强迫函数从寄存器取它的参数，内核利用__attribute__和regparm关键字，这两个关键字是C语言非标准的扩展名，由gcc编译程序实现。在include/asm-1386/system.h头文件中，__switch_to()函数的声明如下： 12__switch_to(struct task_struct *prev, struct task_struct *next&#125;__attribute__(regparm(3)); 函数执行的步骤如下： 执行由__unlazy_fpu()宏产生的代码，以有选择地保存prev_p进程的FPU, MMX及XMM寄存器的内容。 1__unlazy_fpu(prev_p); 执行smp_processor_id()宏获得本地(local)CPU的下标，即执行代码的CPU。该宏从当前进程的thread_info结构的cpu字段获得下标并将它保存到cpu局部变量。 把next-&gt;thread.esp0装入对应于本地CPU的TSS的esp0字段；任何由sysenter汇编指令产生的从用户态到内核态的特权级转换将把这个地址拷贝到esp寄存器中： 1init_tss[cpu].esp0=next-&gt;thread.esp0; 把next_p进程使用的线程局部存储(TLS)段装入本地CPU的全局描述符表；三个段选择符保存在进程描述符内的tls_array数组中。 123cpu_gdt_table[cpu&#125;[6] = next-&gt;thread.tls_array[0];cpu_gdt_table[cpu][7] = next-&gt;thread.tls_array[1];cpu_gdt_table[cpu][8] = next-&gt;thread.tls_array[2]; 把fs和gs段寄存器的内容分别存放在prev_p-&gt;thread.fs和prev_p-&gt;thread.gs中，对应的汇编语言指令是： 12movl %fs，40(%esi)movl %gs，44(%esi) esi寄存器指向prev_p-&gt;thread结构。 如果fs或gs段寄存器已经被prev_p或next_p进程中的任意一个使用(也就是说如果它们有一个非0的值)，则将next_p进程的thread_struct描述符中保存的值装入这些寄存器中。这一步在逻辑上补充了前一步中执行的操作。主要的汇编语言指令如下： 12movl 40(%ebx)，%fsmovl 44(%ebx)，%gs ebx寄存器指向next_p-&gt;thread结构。代码实际上更复杂，因为当它检测到一个无效的段寄存器值时，CPU可能产生一个异常。代码采用一种“修正(fix-up)”途径来考虑这种可能性。 用next_p-&gt;thread.debugreg数组的内容装载dr0，…，dr7中的6个调试寄存器(x86调试器允许进程被硬件监控。最多可定义4个断点区域)。只有在next_p被挂起时正在使用调试寄存器(也就是说，next_p-&gt;thread.debugreg[7]字段不为0)，这种操作才能进行。这些寄存器不需要被保存，因为只有当一个调试器想要监控prev时prev_p-&gt;thread.debugreg才会被修改。 123456789if (next_p-&gt;thread.debugreg[7])&#123; loaddebug(&amp;next_p-&gt;thread, 0); loaddebug(&amp;next_p-&gt;thread, 1); loaddebug(&amp;next_p-&gt;thread, 2); loaddebug(&amp;next_p-&gt;thread, 3); /*没有4和5*/ loaddebug(&amp;next_p-&gt;thread, 6); loaddebug(&amp;next_p-&gt;thread, 7);&#125; 如果必要，更新TSS中的I/O位图。当next_p或prev_p有其自己的定制I/O权限位图时必须这么做： 12if (prev&#125;一&gt;thread.io_bitmap_ptr || next_p-&gt;thread.io_ bitmap_ptr)handle_io_bitmap(&amp;next_p-&gt;thread, &amp;init_tss[cpu]); 因为进程很少修改I/O权限位图，所以该位图在“懒”模式中被处理：当且仅当一个进程在当前时间片内实际访问I/O端口时，真实位图才被拷贝到本地CPU的TSS中。进程的定制I/O权限位图被保存在thread_info结构的io_bitmap_ptr字段指向的缓冲区中。handle_io_bitmap()函数为next_p进程设置本地CPU使用的TSS的io_bitmap字段如下： 如果next_p进程不拥有自己的I/O权限位图，则TSS的io_bitmap字段被设为0x8000。 如果next_p进程拥有自己的I/O权限位图，则TSS的io_bitmap字段被设为0x9000。 TSS的io_bitmap字段应当包含一个在TSS中的偏移量，其中存放实际位图。无论何时用户态进程试图访问一个1/O端口，0x8000和0x9000指向TSS界限之外并将因此引起“General protection”异常。 do_general_protection()异常处理程序将检查保存在io_bitmap字段的值;如果是0x8000,函数发送一个SIGSEGV信号给用户态进程;如果是0x9000,函数把进程位图(由thread_info结构中的io_bitmap_ptr字段指示)拷贝到本地CPU的TSS中，把io_bitmap字段设为实际位图的偏移(104)，并强制再一次执行有缺陷的汇编语言指令。 终止。__switch_to() C函数通过使用下列声明结束： 1return prev_p; ​ 由编译器产生的相应汇编语言指令是： 12movl %edi, %eaxret ​ prev_p参数(现在在edi中)被拷贝到eax，因为缺省情况下任何C函数的返回值被传递给eax寄存器。注意eax的值因此在调用__switch_to()的过程中被保护起来;这非常重要，因为调用switch_to宏时会假定eax总是用来存放将被替换的进程描述符的地址。 ​ 汇编语言指令ret把栈顶保存的返回地址装人eip程序计数器。不过，通过简单地跳转到__switch_to()函数来调用该函数。因此，ret汇编指令在栈中找到标号为1的指令的地址，其中标号为1的地址是由switch_to宏推入栈中的。如果因为next第一次执行而以前从未被挂起，__switch_to()就找到ret_from_fork()函数的起始地址(参见本章后面“clone(),fork()和vfork()系统调用一节”)。 创建进程Unix操作系统紧紧依赖进程创建来满足用户的需求。例如，只要用户输入一条命令，shell进程就创建一个新进程，新进程执行shell的另一个拷贝。 现代Unix内核通过引入三种不同的机制解决了这个问题： 写时复制技术允许父子进程读相同的物理页。只要两者中有一个试图写一个物理页，内核就把这个页的内容拷贝到一个新的物理页，并把这个新的物理页分配给正在写的进程。 轻量级进程允许父子进程共享每进程在内核的很多数据结构，如页表(也就是整个用户态地址空间)、打开文件表及信号处理。 vfork()系统调用创建的进程能共享其父进程的内存地址空间。为了防止父进程重写子进程需要的数据，阻塞父进程的执行，一直到子进程退出或执行一个新的程序为止。 clone()、fork()及vfork()系统调用在Linux中，轻量级进程是由名为clone()的函数创建的，这个函数使用下列参数： 参数 描述 fn 指定一个由新进程执行的函数。当这个函数返回时，子进程终止。函数返回一个整数，表示子进程的退出代码。 arg 指向传递给fn()函数的数据。 flags 各种各样的信息。低字节指定子进程结束时发送到父进程的信号代码，通常选择SIGCHLD信号。剩余的3个字节给clone标志组用于编码。 child_stack 表示把用户态堆栈指针赋给子进程的esp寄存器。调用进程(指调用clone()的 父进程)应该总是为子进程分配新的堆栈。 tls 表示线程局部存储段(TLS)数据结构的地址，该结构是为新轻量级进程定义的。只有在CLONE_SETTLS标志被设置时才有意义。 ptid 表示父进程的用户态变量地址，该父进程具有与新轻量级进程相同的PID。只有在CLONE_PARENT_SETTID标志被设置时才有意义。 ctid 表示新轻量级进程的用户态变量地址，该进程具有这一类进程的PID。只有在CLONE_CHILD_ SETTID标志被设置时才有意义。 clone标志： 标志名称 说明 CLONE_VM 共享内存描述符和所有的页表 CLONE_FS 共享根目录和当前工作目录所在的表，以及用于屏蔽新文件初始许可权的位掩码值(所谓文件的umask ) CLONE_FILES 共享打开文件表 CLONE_SIGHAND 共享信号处理程序的表、阻塞信号表和挂起信号表。如果这个标志为true,就必须设置CLONE_VM标志 CLONE_PTRACE 如果父进程被跟踪，那么，子进程也被跟踪。尤其是，debugger程序可能希望以自己作为父进程来跟踪子进程，在这种情况下，内核把该标志强置为1 CLONE_VFORK 在发出vfork()系统调用时设置(参见本节后面) CLONE_PARENT 设置子进程的父进程(进程描述符中的parent和real_parent字段)为调用进程的父进程 CLONE_THREAD 把子进程插入到父进程的同一线程组中，并迫使子进程共享父进程的信号描述符。因此也设置子进程的tgid字段和group_leader字段。如果这个标志位为true，就必须设置CLONE_SIGRAND标志 CLONE_NEWNS 当clone需要自己的命名空间时(即它自己的已挂载文件系统视图)设置这个标志。不能同时设置CLONE_NEWNS和CLONE_FS CLONE_SYSVSEM 共享System V IPC取消信号量的操作 CLONE_SETTLS 为轻量级进程创建新的线程局部存储段(TLS)，该段由参数tls所指向的结构进行描述 CLONE_PARENT_SETTID 把子进程的PID写入由ptid参数所指向的父进程的用户态变量 CLONE_CHILD_CLEARTID 如果该标志被设置，则内核建立一种触发机制，用在子进程要退出或要开始执行新程序时。在这些情况下，内核将清除由参数ctid所指向的用户态变量，并唤醒等待这个事件的任何进程 CLONE_DETACHED 遗留标志.内核会忽略它 CLONE_UNTRACED 内核设置这个标志以使CLONE_PTRACE标志失去作用(用来禁止内核线程跟踪进程，参见本章稍后的“内核线程”一节) CLONE_CHILD_SETTID 把子进程的PID写入由ctid参数所指向的子进程的用户态变量中 CLONE_STOPPED 强迫子进程开始于TASK_STOPPED状态 实际上，clone()是在C语言库中定义的一个封装(wrapper)函数，它负责建立新轻量级进程的堆栈并且调用对编程者隐藏的clone()系统调用。实现clone()系统调用的sys_clone()服务例程没有fn和arg参数。实际上，封装函数把fn指针存放在子进程堆栈的某个位置处，该位置就是该封装函数本身返回地址存放的位置。arg指针正好存放在子进程堆栈中fn的下面。当封装函数结束时，CPU从堆栈中取出返回地址，然后执行fn(arg)函数。 传统的fork()系统调用在Linux中是用clone()实现的，其中clone()的flags参数指定为SIGCHLD信号及所有清0的clone标志，而它的child_stack参数是父进程当前的堆栈指针。因此，父进程和子进程暂时共享同一个用户态堆栈。但是，要感谢写时复制机制，通常只要父子进程中有一个试图去改变栈，则立即各自得到用户态堆栈的一份拷贝。 前一节描述的vfork()系统调用在Linux中也是用clone()实现的，其中clone()的参数flags指定为SIGCHLD信号和CLONE_VM及CLONE_VFORK标志，clone()的参数child_ stack等于父进程当前的栈指针。 do_fork()函数do_fork()函数负责处理clone(),fork()和vfork()系统调用，执行时使用下列参数： 参数 描述 clone_flags 与clone()的参数flags相同 stack_start 与clone()的参数child_stack相同 regs 指向通用寄存器值的指针，通用寄用器的值是在从用户态切换到内核态时被保存到内核态堆栈中的 stack_size 未使用(总是被设置为O) parent_tidptr child_tidptr 与clone()中的对应参数ptid和ctid相同 do_fork()利用辅助函数copy_process()来创建进程描述符以及子进程执行所需要的所有其他内核数据结构。下面是do_fork()执行的主要步骤： 通过查找pidmap_array位图，为子进程分配新的PID。 检查父进程的ptrace字段(current-&gt;ptrace)：如果它的值不等于0，说明有另外一个进程正在跟踪父进程，因而，do_fork()检查debugger程序是否自己想跟踪子进程(独立于由父进程指定的CLONE_PTRACE标志的值)。在这种情况下，如果子进程不是内核线程(CLONE_UNTRACED标志被清0)，那么do_fork()函数设置CLONE_PTRACE标志。 调用copy_process()复制进程描述符。如果所有必须的资源都是可用的，该函数返回刚创建的task_struct描述符的地址。这是创建过程的关键步骤。 如果设置了CLONE_STOPPED标志，或者必须跟踪子进程，即在p-&gt;ptrace中设置了PT_PTRACED标志，那么子进程的状态被设置成TASK_STOPPED，并为子进程增加挂起的SIGSTOP信号。在另外一个进程(不妨假设是跟踪进程或是父进程)把子进程的状态恢复为TASK_RUNNING之前(通常是通过发送SIGCONT信号)，子进程将一直保持TASK_STOPPED状态。 如果没有设置CLONE_STOPPED标志，则调用wake_up_new_task()函数以执行下述操作： 调整父进程和子进程的调度参数。 如果子进程将和父进程运行在同一个CPU上(当内核创建新进程时，父进程可能被转移到另一个CPU上执行)，而且父进程和子进程不能共享同一组页表(CLONE_VM标志被清0)，那么，就把子进程插入父进程运行队列，插入时让子进程恰好在父进程前面，因此而迫使子进程先于父进程运行。如果子进程刷新其地址空间，并在创建之后执行新程序，那么这种简单的处理会产生较好的性能。而如果我们让父进程先运行，那么写时复制机制将会执行一系列不必要的页面复制。 否则，如果子进程与父进程运行在不同的CPU上，或者父进程和子进程共享同一组页表(CLONE_VM标志被设置)，就把子进程插入父进程运行队列的队尾。 如果CLONE_STOPPED标志被设置，则把子进程置为TASK_STOPPED状态。 如果父进程被跟踪，则把子进程的PID存入current的ptrace_message字段并调用ptrace_notify()。ptrace_notify()使当前进程停止运行，并向当前进程的父进程发送SIGCHLD信号。子进程的祖父进程是跟踪父进程的debugger进程。SIGCHLD信号通知debugger进程：current已经创建了一个子进程，可以通过查找current-&gt;ptrace_message字段获得子进程的PID。 如果设置了CLONE_VFORK标志，则把父进程插入等待队列，并挂起父进程直到子进程释放自己的内存地址空间(也就是说，直到子进程结束或执行新的程序)。 结束并返回子进程的PID。 copy_process()函数copy_process()创建进程描述符以及子进程执行所需要的所有其他数据结构。它的参数与do_fork()的参数相同，外加子进程的PID。下面描述copy_process()的最重要的步骤： 检查参数clone_flags所传递标志的一致性。在下列情况下，它返回错误代号： CLONE_NEWNS和CLONE_FS标志都被设置。 CLONE_THREAD标志被设置，但CLONE_SIGRAND标志被清0(同一线程组中的轻量级进程必须共享信号)。 CLONE_SIGHAND标志被设置，但CLONE_VM被清0(共享信号处理程序的轻量级进程也必须共享内存描述符)。 通过调用security_task_create()以及稍后调用的security_task_alloc()执行所有附加的安全检查。Linux 2.6提供扩展安全性的钩子函数，与传统Unix相比，它具有更加强壮的安全模型。 调用dup_task_struct()为子进程获取进程描述符。该函数执行如下操作（参数为current）： 如果需要，则在当前进程中调用__unlazy_fpu()，把FPU,MMX和SSE/SSE2寄存器的内容保存到父进程的thread_info结构中。稍后，dup_task_struct()将把这些值复制到子进程的thread_info结构中。 执行alloc_task_struct()宏，为新进程获取进程描述符(task_struct结构)，并将描述符地址保存在tsk局部变量中。 执行alloc_thread_info宏以获取一块空闲内存区，用来存放新进程的thread_info结构和内核栈，并将这块内存区字段的地址存在局部变量ti中。这块内存区字段的大小是8KB或4KB。 将current进程描述符的内容复制到tsk所指向的task_struct结构中，然后把tsk-&gt;thread_ info置为ti。 把current进程的thread_info描述符的内容复制到ti所指向的结构中，然后把ti-&gt;task置为tsk。 把新进程描述符的使用计数器(tsk-&gt;usage)置为2，用来表示进程描述符正在被使用而且其相应的进程处于活动状态(进程状态即不是EXIT_ ZOMBIE,也不是EXIT_DEAD)。 返回新进程的进程描述符指针(tsk)。 检查存放在current-&gt;signal-&gt;rlim[RLIMIT_NPROC].rlim_cur变量中的值是否小于或等于用户所拥有的进程数。如果是，则返回错误码，除非进程没有root权限。该函数从每用户数据结构user_struct中获取用户所拥有的进程数。通过进程描述符user字段的指针可以找到这个数据结构。 递增user_struct结构的使用计数器(tsk-&gt;user-&gt;__count字段)和用户所拥有的进程的计数器(tsk-&gt;user-&gt;processes)。 检查系统中的进程数量(存放在nr_threads变量中)是否超过max_threads变量的值。这个变量的缺省值取决于系统内存容量的大小。总的原则是：所有thread_info描述符和内核栈所占用的空间不能超过物理内存大小的1/8。不过，系统管理员可以通过写/proc/sys/kernel/threads-max文件来改变这个值。 如果实现新进程的执行域和可执行格式的内核函数都包含在内核模块中，则递增它们的使用计数器。 设置与进程状态相关的几个关键字段： 把大内核锁计数器tsk-&gt;lock_depth初始化为-1。 把tsk-&gt;did_exec字段初始化为0；它记录了进程发出的execve()系统调用的次数。 更新从父进程复制到tsk-&gt;flags字段中的一些标志:首先清除PF_SUPERPRIV标志，该标志表示进程是否使用了某种超级用户权限。然后设置PF_FORKNOEXEC标志，它表示子进程还没有发出execve()系统调用。 把新进程的PID存人tsk-&gt;pid字段。 如果clone_flags参数中的CLONE_PARENT_SETTID标志被设置，就把子进程的PID复制到参数parent_tidptr指向的用户态变量中。 初始化子进程描述符中的list_head数据结构和自旋锁，并为与挂起信号、定时器及时间统计表相关的几个字段赋初值。 调用copy_semundo()，copy_files()，copy_fs()，copy_sighand()，copy_signal() , copy_mm()和copy namespace()来创建新的数据结构，并把父进程相应数据结构的值复制到新数据结构中，除非clone_flags参数指出它们有不同的值。 调用copy_thread()，用发出clone()系统调用时CPU寄存器的值(这些值已经被保存在父进程的内核栈中)来初始化子进程的内核栈。不过，copy_thread()把eax寄存器对应字段的值[这是fork()和clone()系统调用在子进程中的返回值]字段强行置为0。子进程描述符的thread.esp字段初始化为子进程内核栈的基地址，汇编语言函数(ret_from_fork())的地址存放在thread.eip字段中。如果父进程使用I/O权限位图，则子进程获取该位图的一个拷贝。最后，如果CLONE_SETTLS标志被设置，则子进程获取由clone()系统调用的参数tls指向的用户态数据结构所表示的TLS段(tls并不被传递给do_fork()和嵌套函数。在第十章会看到，通过拷贝系统调用的参数的值到某个CPU寄存器来把它们传递给内核；因此，这些值与其他寄存器一起被保存在内核态堆栈中。copy_thread()只查看esi的值在内核堆栈中对应的位置保存的地址)。 如果clone_flags参数的值被置为CLONE_CHILD_SETTID或CLONE_CHILD_CLEARTID,就把child_tidptr参数的值分别复制到tsk-&gt;setchid_tid或tsk-&gt;clear_child_tid字段。这些标志说明：必须改变子进程用户态地址空间的child_tidptr所指向的变量的值，不过实际的写操作要稍后再执行。 清除子进程thread_info结构的TIF_SYSCALL_TRACE标志，以使ret_from_fork()函数不会把系统调用结束的消息通知给调试进程。(因为对子进程的跟踪是由tsk-&gt;ptrace中的PTRACE_SYSCALL标志来控制的，所以子进程的系统调用跟踪不会被禁用。) 用clone_flags参数低位的信号数字编码初始化tsk-&gt;exit_signal字段，如果CLONE_THREAD标志被置位，就把tsk-&gt;exit_sinal字段初始化为-1。只有当线程组的最后一个成员(通常是线程组的领头)“死亡”，才会产生一个信号，以通知线程组的领头进程的父进程。 调用sched_fork()完成对新进程调度程序数据结构的初始化。该函数把新进程的状态设置为TASK_RUNNING，并把thread_info结构的preempt_count字段设置为1，从而禁止内核抢占。此外，为了保证公平的进程调度，该函数在父子进程之间共享父进程的时间片。 把新进程的thread_info结构的cpu字段设置为由smp_processor_id()所返回的本地CPU号。 初始化表示亲子关系的字段。尤其是，如果CLONE_PARENT或CLONE_THREAD被设置，就用curent-&gt;real_parent的值初始化tsk-&gt;real_parent和tsk-&gt;parent,因此，子进程的父进程似乎是当前进程的父进程。否则，把tsk-&gt;real_parent和tsk-&gt;parent置为当前进程。 如果不需要跟踪子进程(没有设置CLONE_PTRAC标志)，就把tsk-&gt;ptrace字段设置为O。tsk-&gt;ptrace字段会存放一些标志，而这些标志是在一个进程被另外一个进程跟踪时才会用到的。采用这种方式，即使当前进程被跟踪，子进程也不会被跟踪。 执行SET_LINKS宏，把新进程描述符插人进程链表。 如果子进程必须被跟踪(tsk-&gt;ptrace字段的PT_PTRACED标志被设置)，就把current-&gt;parent赋给tsk-&gt;parent，并将子进程插入调试程序的跟踪链表中。 调用attach_pid()把新进程描述符的PID插入pidhash[PIDTYPE_PID]散列表。 如果子进程是线程组的领头进程(CLONE_THREAD标志被清0)： 把tsk-&gt;tgid的初值置为tsk-&gt;pid。 把tsk-&gt;group_leader的初值置为tsk。 调用三次attach_pid()，把子进程分别插入PIDTYPE_TGID, PIDTYPE_PGID和PIDTYPE_SID类型的PID散列表。 否则，如果子进程属于它的父进程的线程组(CLONE_THREAD标志被设置)： 把tsk-&gt;tgid的初值置为tsk-&gt;current-&gt;tgid。 把tsk-&gt;group_leader的初值置为current-&gt;group_leader的值。 调用attach_pid()，把子进程插入PIDTYPE_TGID类型的散列表中(更具体地说，插入current-&gt;group_leader进程的每个PID链表)。 现在，新进程已经被加入进程集合:递增nr_threads变量的值。 递增total_forks变量以记录被创建的进程的数量。 终止并返回子进程描述符指针(tsk)。 现在，我们有了处于可运行状态的完整的子进程。但是，它还没有实际运行，调度程序要决定何时把CPU交给这个子进程。在以后的进程切换中，调度程序继续完善子进程:把子进程描述符thread字段的值装入几个CPU寄存器。特别是把thread.esp(即把子进程内核态堆栈的地址)装人esp寄存器，把函数ret_from_fork()的地址装人eip寄存器。这个汇编语言函数调用schedule_tail()函数(它依次调用finish_task_switch()来完成进程切换)，用存放在栈中的值再装载所有的寄存器，并强迫CPU返回到用户态。然后，在fork(),vfork()或clone()系统调用结束时，新进程将开始执行。系统调用的返回值放在eax寄存器中:返回给子进程的值是0，返回给父进程的值是子进程的PID。回顾copy_thread()对子进程的eax寄存器所执行的操作(copy_process()的第13步)，就能理解这是如何实现的。 除非fork系统调用返回0，否则，子进程将与父进程执行相同的代码(参见copy_process()的第13步)。应用程序的开发者可以按照Unix编程者熟悉的方式利用这一事实，在基于PID值的程序中插人一个条件语句使子进程与父进程有不同的行为。 内核线程传统的Unix系统把一些重要的任务委托给周期性执行的进程，这些任务包括刷新磁盘高速缓存，交换出不用的页框，维护网络连接等等。事实上，以严格线性的方式执行这些任务的确效率不高，如果把它们放在后台调度，不管是对它们的函数还是对终端用户进程都能得到较好的响应。因为一些系统进程只运行在内核态，所以现代操作系统把它们的函数委托给内核线程(kernel thread)，内核线程不受不必要的用户态上下文的拖累。在Linux中，内核线程在以下几方面不同于普通进程： 内核线程只运行在内核态，而普通进程既可以运行在内核态，也可以运行在用户态。 因为内核线程只运行在内核态，它们只使用大于PAGE_OFFSET的线性地址空间。另一方面，不管在用户态还是在内核态，普通进程可以用4GB的线性地址空间。 创建一个内核线程kernel_thread()函数创建一个新的内核线程，它接受的参数有：所要执行的内核函数的地址(fn)、要传递给函数的参数(arg)、一组clone标志(flags)。该函数本质上以下面的方式调用do_fork()： 12345678910111213141516171819int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)&#123; struct pt_regs regs; memset(&amp;regs, 0, sizeof(regs)); regs.ebx = (unsigned long) fn; regs.edx = (unsigned long) arg; regs.xds = __USER_DS; regs.xes = __USER_DS; regs.orig_eax = -1; regs.eip = (unsigned long) kernel_thread_helper; regs.xcs = __KERNEL_CS; regs.eflags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2; /* Ok, create the new process.. */ return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &amp;regs, 0, NULL, NULL);&#125; CLONE_VM标志避免复制调用进程的页表:由于新内核线程无论如何都不会访问用户态地址空间，所以这种复制无疑会造成时间和空间的浪费。CLONE_UNTRACED标志保证不会有任何进程跟踪新内核线程，即使调用进程被跟踪。 传递给do_fork()的参数regs表示内核栈的地址，copy_thread()函数将从这里找到为新线程初始化CPU寄存器的值。kernel_thread()函数在这个栈中保留寄存器值的目的是： 通过copy_thread()把ebx和edx分edx设置为参数fn和arg的值。 把eip寄存器的值设置为下面汇编语言代码段的地址： 12345movl %edx, %eaxpushl %edxcall *%ebxpushl %eaxcall do_exit 因此，新的内核线程开始执行fn(arg)函数，如果该函数结束，内核线程执行系统调用_exit()，并把fn()的返回值传递给它。 进程0所有进程的祖先叫做进程0，idle进程或因为历史的原因叫做swapper进程，它是在Linux的初始化阶段从无到有创建的一个内核线程。这个祖先进程使用下列静态分配的数据结构(所有其他进程的数据结构都是动态分配的)： 存放在init_task变量中的进程描述符，由INIT_TASK宏完成对它的初始化。 存放在init_thread_union变量中的thread_info描述符和内核堆栈，由INIT_THREAD_INFO宏完成对它们的初始化。 | 线程 | 描述 || ———– | —————————————- || keventd(事件) | 执行keventd_wq工作队列中的函数。 || kapmd | 处理与高级电源管理(APM)相关的事件。 || kswapd | 执行内存回收，在第十七章“周期回收”一节将进行描述。 || pdflush | 刷新“脏”缓冲区中的内容到磁盘以回收内存，在第十五章“pdflush内核线程”一 || kblockd | 执行kblockd_workqueue工作队列中的函数。周期性地激活块设备驱动程序。 || ksoftirqd | 运行tasklet。系统中每个CPU都有这样一个内核线程。 | 主内核页全局目录存放在swapper_pg_dir中。 start_kernel()函数初始化内核需要的所有数据结构，激活中断，创建另一个叫进程1的内核线程(一般叫做init进程)： 1kernel_thread(init, NULL, CLONE_FS | CLONE_SIGHAND); 新创建内核线程的PID为1，并与进程0共享每进程所有的内核数据结构。此外，当调度程序选择到它时，init进程开始执行init()函数。 创建init进程后，进程0执行cpu_idle()函数，该函数本质上是在开中断的情况下重复执行hlt汇编语言指令。只有当没有其他进程处于TASK_RUNNING状态时，调度程序才选择进程O。 在多处理器系统中，每个CPU都有一个进程0。只要打开机器电源，计算机的BIOS就启动某一个CPU，同时禁用其他CPU。运行在CPU 0上的swapper进程初始化内核数据结构，然后激活其他的CPU，并通过copy_process()函数创建另外的swapper进程，把0传递给新创建的swapper进程作为它们的新PID。此外，内核把适当的CPU索引赋给内核所创建的每个进程的thread_info描述符的cpu字段。 进程1由进程0创建的内核线程执行init()函数，init()依次完成内核初始化。init()调用execve()系统调用装入可执行程序init。结果，init内核线程变为一个普通进程，且拥有自己的每进程(per-process)内核数据结构。在系统关闭之前，init进程一直存活，因为它创建和监控在操作系统外层执行的所有进程的活动。 其他内核线程Linux使用很多其他内核线程。其中一些在初始化阶段创建，一直运行到系统关闭；而其他一些在内核必须执行一个任务时“按需”创建，这种任务在内核的执行上下文中得到很好的执行。 一些内核线程的例子(除了进程0和进程1)是： 线程 描述 keventd(也被称为事件) 执行keventd_wq工作队列中的函数。 kapmd 处理与高级电源管理(APM)相关的事件。 kswapd 执行内存回收，在第十七章“周期回收”一节将进行描述。 pdflush 刷新“脏”缓冲区中的内容到磁盘以回收内存，在第十五章“pdflush内核线程”一 kblockd 执行kblockd_workqueue工作队列中的函数。实质上，它周期性地激活块设备驱动程序。 ksoftirqd 运行tasklet。系统中每个CPU都有这样一个内核线程。 撤销进程很多进程终止了它们本该执行的代码，从这种意义上说，这些进程“死”了。当这种情况发生时，必须通知内核以便内核释放进程所拥有的资源，包括内存、打开文件及其他我们在本书中讲到的零碎东西，如信号量。 进程终止的一般方式是调用exit()库函数，该函数释放c函数库所分配的资源，执行编程者所注册的每个函数，并结束从系统回收进程的那个系统调用。exit()函数可能由编程者显式地插入。另外，C编译程序总是把exit()函数插入到main()函数的最后一条语句之后。 内核可以有选择地强迫整个线程组死掉。这发生在以下两种典型情况下： 当进程接收到一个不能处理或忽视的信号时。 当内核正在代表进程运行时在内核态产生一个不可恢复的CPU异常时。 进程终止在Linux 2.6中有两个终止用户态应用的系统调用： exit_grpup()系统调用，它终止整个线程组，即整个基于多线程的应用。do_group_exit()是实现这个系统调用的主要内核函数。这是C库函数exit()应该调用的系统调用。 exit()系统调用，它终止某一个线程，而不管该线程所属线程组中的所有其他进程。do_exit()是实现这个系统调用的主要内核函数。这是被诸如pthread_exit()的Linux线程库的函数所调用的系统调用。 do_group_exit()函数do_group_exit()函数杀死属于current线程组的所有进程。它接受进程终止代号作为参数，进程终止代号可能是系统调用exit_group()(正常结束)指定的一个值，也可能是内核提供的一个错误代号(异常结束)。该函数执行下述操作： 检查退出进程的SIGNAL_GROUP_EXIT标志是否不为0，如果不为0，说明内核已经开始为线程组执行退出的过程。在这种情况下，就把存放在current-&gt;signal-&gt;group_exit_code中的值当作退出码，然后跳转到第4步。 否则，设置进程的SIGNAL_GROUP_EXIT标志并把终止代号存放到current-&gt;signal-&gt;group_exit_code字段。 调用zap_other_threads()函数杀死current线程组中的其他进程(如果有的话)。为了完成这个步骤，函数扫描与current-&gt;tgid对应的PIDTYPE_TGID类型的散列表中的每个PID链表，向表中所有不同于current的进程发送SIGKILL信号，结果，所有这样的进程都将执行do_exit()函数，从而被杀死。 调用do_exit()函数，把进程的终止代号传递给它。do_exit()杀死进程而且不再返回。 do_exit()函数所有进程的终止都是由do_exit()函数来处理的，这个函数从内核数据结构中删除对终止进程的大部分引用。do_exit()函数接受进程的终止代号作为参数并执行下列操作： 把进程描述符的flag字段设置为PF_EXITING标志，以表示进程正在被删除。 如果需要，通过函数del_timer_sync()从动态定时器队列中删除进程描述符。 分别调用exit_mm(),exit_sem(),exit_files(),exit_fs(),exit_namespace()和exit_thread()函数从进程描述符中分离出与分页、信号量、文件系统、打开文件描述符、命名空间以及I/O权限位图相关的数据结构。如果没有其他进程共享这些数据结构，那么这些函数还删除所有这些数据结构中。 如果实现了被杀死进程的执行域和可执行格式的内核函数包含在内核模块中，则函数递减它们的使用计数器。 把进程描述符的exit_code字段设置成进程的终止代号，这个值要么是_exit()或exit_group()系统调用参数(正常终止)，要么是由内核提供的一个错误代号(异常终止)。 调用exit_notify()函数执行下面的操作： 更新父进程和子进程的亲属关系。如果同一线程组中有正在运行的进程，就让终止进程所创建的所有子进程都变成同一线程组中另外一个进程的子进程，否则让它们成为init的子进程。 检查被终止进程其进程描述符的exit_signal字段是否不等于-1，并检查进程是否是其所属进程组的最后一个成员(注意：正常进程都会具有这些条件，参见前面“clone(),fork()和vfork()系统调用”一节中对copy_process()的描述，第16步)。在这种情况下，函数通过给正被终止进程的父进程发送一个信号(通常是SIGCHLD)，以通知父进程子进程死亡。 否则，也就是exit_signal字段等于-1，或者线程组中还有其他进程，那么只要进程正在被跟踪，就向父进程发送一个SIGCHLD信号(在这种情况下，父进程是调试程序，因而，向它报告轻量级进程死亡的信息)。 如果进程描述符的exit_signal字段等于-1，而且进程没有被跟踪，就把进程描述符的exit_state字段置为EXIT_DEAD，然后调用release_task()回收进程的其他数据结构占用的内存，并递减进程描述符的使用计数器(见下一节)。使用记数器变为1(参见copy_process()函数的第3f步)，以使进程描述符本身正好不会被释放。 否则，如果进程描述符的exit_signal字段不等于-1，或进程正在被跟踪，就把exit_state字段置为EXIT_ZOMBIE。在下一节我们将看到如何处理僵死进程。 把进程描述符的flags字段设置为PF_DEAD标志。 调用schedule()函数选择一个新进程运行。调度程序忽略处于EXIT_ZOMBIE状态的进程，所以这种进程正好在schedule()中的宏switch_to被调用之后停止执行。正如在第七章我们将看到的:调度程序将检查被替换的僵死进程描述符的PF_DEAD标志并递减使用计数器，从而说明进程不再存活的事实。 进程删除Unix允许进程查询内核以获得其父进程的PID,或者其任何子进程的执行状态。例如，进程可以创建一个子进程来执行特定的任务，然后调用诸如wait()这样的一些库函数检查子进程是否终止。如果子进程已经终止，那么，它的终止代号将告诉父进程这个任务是否已成功地完成。 为了遵循这些设计选择，不允许Unix内核在进程一终止后就丢弃包含在进程描述符字段中的数据。只有父进程发出了与被终止的进程相关的wait()类系统调用之后，才允许这样做。这就是引入僵死状态的原因:尽管从技术上来说进程已死，但必须保存它的描述符，直到父进程得到通知。 如果父进程在子进程结束之前结束会发生什么情况呢?在这种情况下，系统中会到处是僵死的进程，而且它们的进程描述符永久占据着RAM。如前所述，必须强迫所有的孤儿进程成为init进程的子进程来解决这个问题。这样，init进程在用wait()类系统调用检查其合法的子进程终止时，就会撤消僵死的进程。 release_task()函数从僵死进程的描述符中分离出最后的数据结构；对僵死进程的处理有两种可能的方式： 如果父进程不需要接收来自子进程的信号，就调用do_exit()。 如果已经给父进程发送了一个信号，就调用wait4()或waitpid()系统调用。 在后一种情况下，函数还将回收进程描述符所占用的内存空间，而在前一种情况下，内存的回收将由进程调度程序来完成。该函数执行下述步骤： 递减终止进程拥有者的进程个数。这个值存放在本章前面提到的user_struct结构中(参见copy_process()的第4步)。 如果进程正在被跟踪，函数将它从调试程序的ptrace_children链表中删除，并让该进程重新属于初始的父进程。 调用__exit_signal()删除所有的挂起信号并释放进程的signal_struct描述符。如果该描述符不再被其他的轻量级进程使用，函数进一步删除这个数据结构。此外，函数调用exit_itimers()从进程中剥离掉所有的POSIX时间间隔定时器。 调用__exit_sighand()删除信号处理函数。 调用__unhash_process() ,该函数依次执行下面的操作： 变量nr_threads减1。 两次调用detach_pid()，分别从PIDTYPE_PID和PIDTYPE_TGID类型的PID散列表中删除进程描述符。 如果进程是线程组的领头进程，那么再调用两次detach_pid()，从PIDTYPE_PGID和PIDTYPE_SID类型的散列表中删除进程描述符。 用宏REMOVE_LINKS从进程链表中解除进程描述符的链接。 如果进程不是线程组的领头进程，领头进程处于僵死状态，而且进程是线程组的最后一个成员，则该函数向领头进程的父进程发送一个信号，通知它进程已死亡。 调用sched_exit()函数来调整父进程的时间片(这一步在逻辑上作为对copy_process()第17步的补充)。 调用put_task_struct()递减进程描述符的使用计数器，如果计数器变为0，则函数终止所有残留的对进程的引用。 递减进程所有者的user_struct数据结构的使用计数器(__count字段)(参见copy_process()的第5步)，如果使用计数器变为0，就释放该数据结构。 释放进程描述符以及thread_info描述符和内核态堆栈所占用的内存区域。]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 内存寻址]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FLinux-%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80.html</url>
    <content type="text"><![CDATA[内存地址分类逻辑地址：机器语言指令中用来指定一个操作数或一条指令的地址。每一个逻辑地址都由一个段（segment）和偏移量（offset或displacement）组成，偏移量指明了从段开始的地方到实际地址之间的距离。 线性地址（或 虚拟地址）：一个32位（或64位）无符号整数，在32位系统中可以用来表示高达4GB（0x0000 0000 —— 0xffff ffff）的地址，也就是高达 4 * 1024 * 1024 * 1024个内存单元（字节）。 物理地址（physical address）：芯片级内存单元寻址。与从微处理器的地址引脚发送到内存总线上的电信号相对应。物理地址由32位或36位（开启PAE）无符号整数表示。 内存管理单元（MMU）通过分段单元（segmentation unit）把逻辑地址转换成线性地址；然后，通过分页单元（paging unit）把线性地址转换成物理地址。分段单元和分页单元都是一种硬件电路。 硬件中的分段段选择符和段寄存器逻辑地址由两部分组成：段选择符和指定段内相对地址的偏移量。段选择符（Segment Selector）是一个16位长的字段，而偏移量是一个32位长的字段。 字段名 描述 索引 指定了放在GDT或LDT中的相应段描述符 TI TI（Table Indicator）标志，指明段描述符是在GDT中（TI=0）或在LDT中（TI=1） RPL 请求者特权级，当相应的段选择符装入到cs寄存器中时指示出CPU当前的特权级，它还可以用于在访问数据段时有选择地削弱处理器的特权级 处理器提供段寄存器来存放段选择符以保证查找段选择符的效率。这些段寄存器称为cs, ss, ds, es, fs和gs。程序可以把同一个段寄存器用于不同的目的：先将其值保存在内存中，用完后再恢复。6个段寄存器中3个有专门的用途： cs 代码段寄存器，指向包含程序指令的段。 ss 栈段寄存器，指向包含当前程序栈的段。 ds 数据段寄存器，指向包含静态数据或者全局数据段（初始化数据）。 其他3个段寄存器作一般用途，可以指向任意的数据段。cs寄存器还有一个很重要的功能：它含有一个 两位的字段，用以指明CPU的 当前特权级(Current Privilege Level, CPL)。0代表最高优先级——内核态，而3代表最低优先级——用户态。 段描述符每个段由一个 8字节（64 bit） 的段描述符（Segment Descriptor）表示，它描述了段的特征。段描述符放在全局描述符表（Global Descriptor Table, GDT）或局部描述符表（Local Descriptor Table, LDT）中。GDT在主存中的地址和大小存放在gdtr控制寄存器中，当前正被使用的LDT地址和大小放在ldtr控制寄存器中。 字段名 描述 基地址（Base） 包含段的首字节的线性地址 （32 bit） G 粒度标志；置0，则段大小以字节为单位，否则以4096字节的倍数计 Limit 最大段偏移量，段的长度（20 bit）。如果G被置为0，则一个段的大小在1个字节到1MB之间变化；否则，将在4KB到4GB之间变化 S 系统标志；置0，系统段，存储诸如LDT这种关键的数据结构，否则它是一个普通的代码段或数据段 Type 描述了段的类型特征和它的存取权限 DPL 描述符特权级（Descriptor Privilege Level）字段；用于限制对这个段的存取。表示访问这个段要求的CPU最小的优先级 P Segment-Present标志；为0表示段当前不在主存中。Linux总是把这个标志（第47位）设为1，因为它从来不把整个段交换到磁盘上去 D或B 取决于是代码段还是数据段 AVL 操作系统使用，但被Linux忽略 为加速逻辑地址到线性地址的转换，80x86处理器提供一种附加的非编程的寄存器（不能被编程者设置的寄存器），供6个可编程的段寄存器使用。每一个非编程的寄存器含有8个字节的段描述符，由相应的段寄存器中的段选择符来指定。每当一个段选择符被装入段寄存器时，相应的段描述符就由内存装入到对应的非编程CPU寄存器。之后，针对那个段的逻辑地址转换就可以不访问主存中的GDT或LDT，处理器只需直接引用存放段描述符的CPU寄存器即可。仅当段寄存器的内容改变时，才有必要访问GDT或LDT。 分段单元下图显示一个逻辑地址转换的详细过程，分段单元（segmentation unit）执行以下操作： 先检查段选择符的TI字段，以决定段描述符保存在哪一个描述符表中。GDT中，分段单元从gdtr寄存器得到GDT的线性基地址；LDT中，分段单元从ldtr寄存器得到LDT的线性基地址。 从段选择符的index字段计算段描述符的地址，index字段的值乘以8（一个段描述符的大小），这个结果与gdtr或ldtr寄存器中的内容相加。 把逻辑地址的偏移量与段描述符Base字段的值相加就得到了线性地址。 有了与段寄存器相关的不可编程寄存器，只有当段寄存器的内容被改变时才需要执行前两个操作。 Linux中的分段2.6版的Linux只有在x86结构下才需要分段。 运行在用户态的所有Linux进程都使用一对相同的段来对指令和数据寻址。这两个段就是所谓的用户代码段和用户数据段。类似地，运行在内核态的所有Linux进程都使用一对相同的段对指令和数据寻址：内核代码段和内核数据段。 下表显示了这4个重要段的段描述符字段的值： 段 Base G Limit S Type DPL D/B p 用户代码段 0x0000 0000 1 0xfffff 1 10 3 1 1 用户数据段 0x0000 0000 1 0xfffff 1 2 3 1 1 内核代码段 0x0000 0000 1 0xfffff 1 10 0 1 1 内核数据段 0x0000 0000 1 0xfffff 1 2 0 1 1 G为1，粒度为4KB，Limit为 0xfffff，则空间为 4GB 相应的段选择符由宏定义。 1__USER_CS、__USER_DS、__KERNEL_CS、__KERNEL_DS 为了对内核代码段寻址，内核只需把__KERNEL_CS宏产生的值装进cs段寄存器即可。 注意，与段相关的线性地址从0开始，达到223 - 1的寻址限长。这就意味着在用户态或内核态下的所有进程可以使用相同的逻辑地址。 所有段都从0x0000 0000 开始，那么，在Linux下逻辑地址与线性地址是一致的，即逻辑地址的偏移量字段的值与相应的线性地址的值总是一致的。 当对指向指令或者数据结构的指针进行保存时，内核不需要为其设置逻辑地址的段选择符，因为cs寄存器就含有当前的段选择符。例如，当内核调用一个函数时，它执行一条call汇编语言指令，该指令仅指定其逻辑地址的偏移量部分，而段选择符不用设置，它已经隐含在cs寄存器中了。因为“在内核态执行”的段只有一种，叫做代码段，由宏KERNEL_CS定义，所以只要当CPU切换到内核态时将KERNEL_CS装载进cs就足够了。同样的道理也适用于指向内核数据结构的指针(隐含地使用ds寄存器)以及指向用户数据结构的指针(内核显式地使用es寄存器)。 Linux GDT在单处理器系统中只有一个GDT，而在多处理器系统中每个CPU对应一个GDT。所有的GDT都存放在cpu_gdt_table数组中，而所有GDT的地址和它们的大小(当初始化gdtr寄存器时使用)被存放在cpu_gdt_descr数组中。这些符号都在文件arch/i386/kernel/head.S中被定义。 下图是GDT的布局示意图。每个GDT包含18个段描述符和14个空的，未使用的，或保留的项。插入未使用的项的目的是为了使经常一起访问的描述符能够处于同一个32字节的硬件高速缓存行中。 每一个GDT中包含的18个段描述符指同下列的段： 用户态和内核态下的代码段和数据段，共4个。 任务状态段（TSS），每个处理器有1个。每个TSS相应的线性地址空间都是内核数据段相应线性地址空间的一个小子集。所有的任务状态段都顺序地存放在init_tss数组中，值得特别说明的是，第n个CPU的TSS描述符的Base字段指向init_tss数组的第n个元素。G(粒度)标志被清0，而Limit字段置为0xeb, 因为TSS段是236字节长。Type字段置为9或11(可用的32位TSS)，且DPL置 为0，因为不允许用户态下的进程访问TSS段。 1个包括缺省局部描述符表的段，这个段通常被所有进程共享。 3个局部线程存储（Thread-Local Storage, TLS）段：这种机制允许多线程应用程序使用最多3个局部于线程的数据段。系统使用set_thread_area()和get_thread_area()分别为正在执行的进程创建和撤销一个TLS段。 与高级电源管理（APM）相关的3个段：由于BIOS代码使用段，所以当Linux APM驱动程序调用BIOS函数来获取或者设置APM设备的状态时，就可以使用自定义的代码段和数据段。 与支持即插即用（PnP）功能的BIOS服务程序相关的5个段。 被内核用来处理“双重错误”异常（处理一个异常时可能会引发另一个异常）的特殊TSS段。 系统中每个处理器都有一个GDT副本。除少数几种情况外，所有GDT的副本都存放相同的表项： 每个处理器都有它自己的TSS段。 GDT中只有少数项可能依赖于CPU正在执行的进程（LDT和TLS段描述符）。 在某些情况下，处理器可能临时修改GDT副本里的某个项，例如，当调用APM的BIOS例程时就会发生这种情况。 Linux LDT大多数用户态下的Linux程序不使用局部描述符表，因此内核就定义了一个缺省的LDT供大多数进程共享。缺省的局部描述符表存放在default_ldt数组中。它包含5个项，但内核仅仅有效地使用了其中的两个项：用于iBCS执行文件的调用门和Solaris/x86可执行文件的调用门。调用门是80x86微处理器提供的一种机制，用于在调用预定义函数时改变CPU的特权级（参考Intel文档以获取更多详情）。 硬件中的分页分页单元(paging unit)把线性地址转换成物理地址。其中的一个关键任务是把所请求的访问类型与线性地址的访问权限相比较，如果这次内存访问是无效的，就产生一个缺页异常。 为了效率起见，线性地址被分成以固定长度为单位的组，称为页（page）。页内部连续的线性地址被映射到连续的物理地址中。这样，内核可以指定一个页的物理地址和其存取权限，而不用指定页所包含的全部线性地址的存取权限。我们遵循通常习惯，使用术语“页”既指一组线性地址，又指包含在这组地址中的数据。 分页单元把所有的RAM分成固定长度的叶框（page frame）（也叫做物理页）。每一个叶框包含一个页，也就是说叶框的长度与一个页的长度一致。页框是主存的一部分，因此也是一个存储区域。区分一页和一个页框是很重要的，前者只是一个数据块，可以存放在任何页框或磁盘中。 把线性地址映射到物理地址的数据结构称为页表(page table )。页表存放在主存中，并在启用分页单元之前必须由内核对页表进行适当的初始化。 从80386开始，所有的80x86处理器都支持分页，它通过设置cr0寄存器的PG标志启用。当PG=0时，线性地址就被解释成物理地址。&lt;需要了解控制寄存器(cr0~cr3)的结构及作用&gt; 常规分页从80386起，Intel处理器的分页单元处理4KB的页。32位的线性地址被分成3个域： Directory（目录）：最高10位 Table（页表）：中间10位 Offset（偏移量）：最低12位 线性地址的转换分两步完成，每一步都基于一种转换表，第一种转换表称为页目录表(page directory)，第二种转换表称为页表(page table )。 页目录 及 页表都分别存放在1个页中（4KB），其中每个表项也都是4个字节。 使用这种二级模式的目的在于减少每个进程页表所需RAM的数量。如果使用简单的一级页表，那将需要高达220个表项(4GB/4KB = 220 ，也就是，在每项4个字节时，需要4MB RAM)来表示每个进程的页表(如果进程使用全部4GB线性地址空间)，即使一个进程并不使用那个范围内的所有地址。二级模式通过只为进程实际使用的那些“虚拟内存区”请求页表来减少内存容量。 每个活动进程必须有一个分配给它的页目录。不过，没有必要马上为进程的所有页表都分配RAM。只有在进程实际需要一个页表时才给该页表分配RAM会更为有效率。 正在使用的页目录的物理地址存放在控制寄存器cr3中。 页目录项和页表项有相同的结构，每项都包含下面的字段： 字段 描述 Present标志 置为1，所指的页（或页表）就在主存中；为0，则这一页不在主存，此时这个表项剩余的位可由操作系统用于自己的目的。如果只需一个地址转换所需的页表项或页目录项中Present标志被清0，那么分页单元就把该线性地址存放在控制寄存器cr2中，并产生14号异常：缺页异常。 包含页框物理地址最高20位的字段 由于每一个页框有4KB的容量，它的物理地址必须是4096的倍数，因此物理地址的最低12位总是为0。若这个字段指向一个页目录，相应的页框就含有一个页表，若指向一个页表，相应的页框就含有一页数据。 Accessed标志 每当分页单元对相应页框进行寻址时就设置这个标志。当选中的页被交换出去时，这一标志由操作系统使用。分页单元从来不重置这个标志，而是必须由操作系统去做。 Dirty标志 只应用于页表项中。每当对一个页框进行写操作时就设置这个标志。与Accessed标志一样，“当选中…………系统去做”。 Read/Write标志 含有页或页表的存取权限。 User/Supervisor标志 含有访问页或页表所需的特权级。 PCD和PWT标志 控制硬件高速缓存处理页或页表的方式。 Page Size标志 只应用于页目录项。置为1，则页目录指的是2MB或4MB的页框。 Global标志 只应用于页表项。这个标志是在Pentium Pro中引入的，用来防止常用页从TLB（俗称“快表”）高速缓存中刷新出去。只有在cr4寄存器的页全局启用（Page Global Enable, PGE）标志置位时这个标志才起作用。 扩展分页从Pentium模型开始，80x86微处理器引入了扩展分页（extended paging），它允许页框大小为4MB而不是4KB。扩展分页用于把大段连续的线性地址转换成相应的物理地址，在这些情况下，内核可以不用中间页表进行地址转换，从而节省内存并保留TLB项。 通过设置页目录项的Page Size标志启用扩展分页功能。分页单元吧32位线性地址分成两个字段： Directory：最高10位 Offset：其余22位 扩展分页和正常分页的目录项基本相同，除了： Page Size标志必须被设置。 32位物理地址字段只有最高10位是有意义的。这是因为每一个物理地址都是在以4MB为边界的地方开始的，故这个地址的最低22位为0。 通过设置cr4处理器寄存器的PSE标志能使扩展分页与常规分页共存。 硬件保护方案分页单元和分段单元的保护方案不同。尽管x86处理器允许一个段使用4种可能的特权级别，但与页和页表相关的特权级只有两个，因为特权由User/Supervisor标志所控制。若这个标志为0，只有当CPL小于3(这意味着对于Linux而言，处理器处于内核态)时才能对页寻址。若该标志为1，则总能对页寻址。 此外，与段的3种存取权限（读、写、执行）不同的是，页的存取权限只有两种（度、写）。如果页目录项或页表项的Read/Write标志等于0，说明相应的页表或页是只读的，否则是可读写的。 物理地址扩展（PAE）分页机制处理器所支持的RAM容量受连接到地址总线上的地址管脚数限制。早期Intel处理器从80386到Pentium使用32位物理地址。从理论上讲，这样的系统上可以安装高达4GB的RAM；而实际上，由于用户进程线性地址空间的需要，内核不能直接对1GB以上的RAM进行寻址。 然而，大型服务器需要大于4GB的RAM来同时运行数以千计的进程，所以必须扩展32位x86结构所支持的RAM容量。Intel通过在它的处理器上把管脚数从32增加到36已经满足了这些需求。寻址能力可达到236 = 64GB。不过，只有引入一种新的分页机制把32位线性地址转换为36位物理地址才能使用所增加的物理地址。 从Pentium Pro处理器开始，Intel引入一种叫做 物理地址扩展（Physical Address Extension, PAE）的机制。另外一种叫做页大小扩展[Page Size Extension (PSE-36)]的机制在Pentium 3处理器中引入，但是Linux并没有采用这种机制。 通过设置cr4控制寄存器中的物理地址扩展（PAE）标志激活PAE。页目录项中的页大小标志PS启用大尺寸页(在PAE启用时为2MB)。 Intel为了支持PAE改变了分页机制： 64GB的RAM被分为224个页框（4KB），页表项的物理地址字段从20位扩展到了24位。因为PAE页表项必须包含12个标志位(在前面已描述)和24个物理地址位，总数之和为36，页表项大小从32位变为64位增加了一倍。结果，一个4KB的页表包含512个表项而不是1024个表项。 引入一个叫做页目录指针表(Page Directory Pointer Table, PDPT)的页表新级别，它由4个64位表项组成。 cr3控制寄存器包含一个27位的页目录指针表(PDPT)基地址字段。因为PDPT存放在RAM的前4GB中，并在32字节(25)的倍数上对齐，因此27位足以表示这种表的基地址。 当把线性地址映射到4KB的页时(页目录项中的PS标志清0), 32位线性地址按下列方式解释： cr3：指向一个PDPT 位31-30：指向PDPT中4个项中的一个 位29-21：指向页目录中512个项目中的一个 位20-12：指向页表中512项中的一个 位11-0：4KB页中的偏移量 当把线性地址映射到2MB的页时(页目录项中的PS标志置为1), 32位线性地址按下列方式解释： cr3：指向一个PDPT 位31-30：指向PDPT中4个项中的一个 位29-21：指向页目录中512个项中的一个 位20-0：2MB页中的偏移量 总之，一旦cr3被设置，就可能寻址高达4GB RAM。如果我们希望对更多的RAM寻址，就必须在cr3中放置一个新值，或改变PDPT的内容。然而，使用PAE的主要问题是线性地址仍然是32位长。这就迫使内核编程人员用同一线性地址映射不同的RAM区。很明显，PAE并没有扩大进程的线性地址空间，因为它只处理物理地址。此外，只有内核能够修改进程的页表，所以在用户态下运行的进程不能使用大于4GB的物理地址空间。另一方面，PAE允许内核使用容量高达64GB的RAM，从而显著增加了系统中的进程数量。 64位系统中的分页 平台名称 页大小 寻址使用的位数 分页级别数 线性地址分级 alpha 8KB 43 3 10+10+10+13 ia64 4KB 39 3 9+9+9+12 ppc64 4KB 41 3 10+10+9+12 sh64 4KB 41 3 10+10+9+12 x86_64 4KB 48 4 9+9+9+9+12 转换后援缓冲器（TLB）x86处理器包含了一个称为转换后援缓冲器或TLB(Translation Lookaside Buffer)的高速缓存用于加快线性地址的转换。当一个线性地址被第一次使用时，通过慢速访问RAM中的页表计算出相应的物理地址。同时，物理地址被存放在一个TLB表项(TLB entry)中，以便以后对同一个线性地址的引用可以快速地得到转换。 在多处理系统中，每个CPU都有自己的TLB，叫做该CPU的本地TLB。 当CPU的cr3控制寄存器被修改时，硬件自动使本地TLB中的所有项都无效，这是因为新的一组页表被启用而TLB指向的是旧数据。 Linux中的分页Linux采用了一种同时适用于32位和64位系统的普通分页模型。从2.6.11版本开始，采用了四级分页模型。下图中展示的4种页表分别被为： 页全局目录(Page Global Directory ) 页上级目录(Page Upper Directory ) 页中级目录(Page Middle Directory ) 页表(Page Table) 对于没有启用物理地址扩展的32位系统，两级页表已经足够了。Linux通过使“页上级目录”位和“页中间目录”位全为0，从根本上取消了页上级目录和页中间目录字段。不过，页上级目录和页中间目录在指针序列中的位置被保留，以便同样的代码在32位系统和64位系统下都能使用。内核为页上级目录和页中间目录保留了一个位置，这是通过把它们的页目录项数设置为1，并把这两个目录项映射到页全局目录的一个适当的目录项而实现的。 启用了物理地址扩展（PAE）的32位系统使用了三级页表。Linux的页全局目录对应x86的页目录指针表(PDPT)，取消了页上级目录，页中间目录对应x86的页目录，Linux的页表对应x86的页表。 最后，64位系统使用二级还是四级分页取决于硬件对线性地址的位的划分。 Linux的进程处理很大程度上依赖于分页。事实上，线性地址到物理地址的自动转换使下面的设计目标变得可行： 给每一个进程分配一块不同的物理地址空间，这确保了可以有效地防止寻址错误。 区别页(即一组数据)和页框(即主存中的物理地址)之不同。这就允许存放在某个页框中的一个页，然后保存到磁盘上，以后重新装入这同一页时又可以被装在不同的页框中。这就是虚拟内存机制的基本要素。 每个进程有它自己的页全局目录和自己的页表集。当发生进程切换时，Linux把cr3控制寄存器的内存保存在前一个执行进程的描述符中，然后把下一个要执行进程的描述符的值装入cr3寄存器中。因此，当新进程重新开始在CPU上执行时，分页单元指向一组正确的页表。 物理内存布局可参考 地址空间布局 在初始化阶段，内核必须建立一个物理地址映射来指定哪些物理地址范围对内核可用而哪些不可用。 内核将下列页框记为保留： 在不可用的物理地址范围内的页框。 含有内核代码和已初始化的数据结构的页框。 保留页框中的页绝不能被动态分配或交换到磁盘上。 一般来说，Linux内核安装在RAM中从物理地址0x00100000开始的地方，也就是说，从第二个MB开始。所需页框总数依赖干内核的配置方案：典型的配置所得到的内核可以被安装在小于3MB的RAM中。 为什么内核没有安装在RAM第一个MB开始的地方?因为PC体系结构有几个独特的地方必须考虑到。例如： 页框0由BIOS使用，存放加电自检(Power-On Self-Test, POST)期间检查到的系统硬件配置。 物理地址从0x000a0000到0x000fffff的范围通常留给BIOS例程，并且映射ISA图形卡上的内部内存。这个区域就是所有IBM兼容PC上从640KB到1MB之间著名的洞：物理地址存在但被保留，对应的页框不能由操作系统使用。 第一个MB内的其他页框可能由特定计算机模型保留。例如，IBM Thinkpnd把0xa0页框映射到0x9f页框。 在启动过程的早期阶段，内核询问BIOS并了解物理内存的大小。在新近的计算机中，内核也调用BIOS过程建立一组物理地址范围和其对应的内存类型。 随后，内核执行machine_specific_memory_setup()函数，该函数建立物理地址映射。当然，如果这张表是可获取的，那是内核在BIOS列表的基础上构建的。否则，内核按保守的缺省设置构建这张表：从0x9f000(LOWMEMSIZE())到0x100000(HIGH_MEMORY)号的所有页框都标记为保留。 开始 结束 类型 0x0000 0000 0x0009 ffff Usable 0x000f 0000 0x000f ffff Reserved 0x0010 0000 0x07fe ffff Usable 0x07ff 0000 0x07ff 2ffff ACPI data 0x07ff 3000 0x07ff ffff ACPI NVS 0xffff 0000 0xffff ffff Reserved 上表显示了具有128MB(0x0800 0000) RAM计算机的典型配置。从0x07ff 0000到0x07ff 2fff 的物理地址范围中存有加电自检(POST)阶段由BIOS写入的系统硬件设备信息。在初始化阶段，内核把这些信息拷贝到一个合适的内核数据结构中，然后认为这些页框是可用的。相反，从0x07ff3000到0x07ff ffff的物理地址范围被映射到硬件设备的ROM芯片。从0xffff 0000开始的物理地址范围标记为保留，因为它由硬件映射到BIOS的ROM芯片。注意BIOS也许并不提供一些物理地址范围的信息(在上述表中，范围是0x000a 0000到0x000e ffff)。为安全可靠起见，Linux假定这样的范围是不可用的。 内核可能不会见到BIOS报告的所有物理内存：例如，如果未使用PAE支持来编译，即使有更大的物理内存可供使用，内核也只能寻址4GB大小的RAM。setup_memory()函数在machine_specific_memory_setup()执行后被调用：它分析物理内存区域表并初始化一些变量来描述内核的物理内存布局。 为了避免把内核装入一组不连续的页框里，Linux更愿跳过RAM的第一个MB。明确地说，Linux用PC体系结构未保留的页框来动态存放所分配的页。下图显示了Linux怎样填充前3MB的RAM： 符号_text对应于物理地址0x0010 0000 （16MB），表示内核代码第一个字节的地址。内核代码的结束位代由另外一个类似的符号_etext表示。内核数据分为两组：初始化过的数据的和没有初始化的数据。初始化过的数据在_etext后开始，在_edata处结束。紧接着是未初始化的数据并以_end结束。 图中出现的符号并没有在Linux源代码中定义，它们是编译内核时产生的（可以在System.map文件中找到这些符号，System.map是编译内核以后所创建的）。 进程页表进程的线性地址空间分成两部分： 从0x0000 0000——0xbfff ffff的线性地址，无论进程运行在用户态还是内核态都可以寻址（0—3GB）。 从0xc000 0000——0xffff ffff的线性地址，只有内核的进程才能寻址。 进程运行在用户态时，所产生的线性地址小于0xc000 0000，而运行在内核态时，执行内核代码，所产生的地址大于等于0xc000 0000。但是，在某些情况下，内核为了检索或存放数据必须访问用户态线性地址空间。 宏PAGE_OFFSET产生的值是0xc000 0000，这就是进程在线性地址空间中的偏移量，也是内核生存空间的开始之处。 内核页表内核维持着一组自己使用的页表，驻留在所谓的主内核页全局目录(master kernel Page Global Directory)中。系统初始化后，这组页表还从未被任何进程或任何内核线程直接使用；更确切地说，主内核页全局目录的最高目录项部分作为参考模型，为系统中每个普通进程对应的页全局目录项提供参考模型。 内核初始化自己的页表，这个过程分为两个阶段。事实上，内核映像刚刚被装入内存后，CPU仍然运行于实模式，所以分页功能没有被启用。 第一个阶段，内核创建一个有限的地址空间，包括内核的代码段和数据段、初始页表和用于存放动态数据结构的共128KB大小的空间。这个最小限度的地址空间仅够将内核装入RAM和对其初始化的核心数据结构。 第二个阶段，内核充分利用剩余的RAM并适当地建立分页表。下一节解释这个方案是怎样实施的。 临时内核页表临时页全局目录是在内核编译过程中静态地初始化的，而临时页表是由startup_32()汇编语言函数(定义于arch/i386/kernel/head.S)初始化的。不再过多提及页上级目录和页中间目录，因为它们相当于页全局目录项。在这个阶段PAE支持并未激活。 临时页全局目录放在swapper_pg_dir变量中。临时页表在pg0变量处开始存放，紧接在内核未初始化的数据段(_end符号)后面。为简单起见，我们假设内核使用的段、临时页表和128KB的内存范围能容纳于RAM前8MB空间里。为了映射RAM前8MB的空间，需要用到两个页表。 分页第一个阶段的目标是允许在实模式下和保护模式下都能很容易地对这8MB寻址。因此，内核必须创建一个映射，把从0x0000 0000到0x007f ffff的线性地址和从0xc000 0000到0xc07f ffff的线性地址映射到从0x0000 0000到0x007f ffff的物理地址。换句话说，内核在初始化的第一阶段，可以通过与物理地址相同的线性地址或者通过从0xc000 0000开始的8MB线性地址对RAM的前8MB进行寻址。 内核通过把swapper_pg_dir所有项都填充为0来创建期望的映射，不过，0、1、0x300(十进制768)和0x301(十进制769)这四项除外。后两项包含了从0xc000 0000到0xc07f ffff间的所有线性地址。0、1、0x300和0x301按以下方式初始化： 0项和0x300项的地址字段置为pg0的物理地址，而1项和0x301项的地址字段 置为紧随pg0后的页框的物理地址。 把这四个项中的Present、Read/Write和User/Supervisor标志置位。 把这四个项中的Accessed、Dirty、PCD、PWD和Page Size标志清0。 汇编语言函数startup_32()也启用分页单元，通过向cr3控制寄存器装入swapper_pg_dir的地址及设置cr0控制寄存器的PG标志来达到这一目的。下面是等价的代码片段： 12345movl $swapper_pg_dir-0xc0000000,%eaxmovl %eax,%cr3 /*设置页表指针*/movl %cr0,%eaxorl $0x80000000,%eaxmovl %eax,%cr0 /*设置分页(PG)位“/]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>深入理解Linux内核</tag>
        <tag>内存寻址</tag>
        <tag>分段</tag>
        <tag>分页</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理器发展的两种处理模式：实模式和保护模式]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2F%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%A4%84%E7%90%86%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[Intel从80286开始引入保护模式，并与之前已存在的实模式并存。作为CPU运行的两种工作方式，具体有何区别，本文对其做简要说明。 实模式80286之前，处理器的内部数据总线、外部数据总线及位宽都是16位，但地址总线（物理引脚）为20位。因此它的可访问物理地址空间为 1MB（220字节）。 但是，由于其数据总线及位宽都是16位，最大只能代表有64KB的空间，小于地址总线能代表的1MB空间。为了解决这个问题，设计人员通过用16位的段寄存器内容左移4位，并和16位的段内偏移相加组成的20位代表1MB空间：物理地址 = (段寄存器内容 &lt;&lt; 4) + 段内偏移。 通过这种方式能表示的内存地址最大范围即为：0xfff &lt;&lt; 4 + 0xffff = 0x10 ffef。已大于1MB，为使地址正常访问，对已得地址对1MB取模，此方法称为 wrap-around。 现代处理器地址总线数已远超20，但是为了向下兼容，系统刚加电时依旧使用实模式。 保护模式实模式带来了一些问题，主要是物理地址可见带来的无法保证地址空间的安全性问题：不区分系统程序及用户程序，用户可随意更改处于物理内存的敏感数据，极易导致系统崩溃。因此引入保护模式。 以现在32位处理器为例：内部数据总线、外部数据总线、位宽及地址总线都为32位。 处于保护模式时，寻址方式发生些许改变：由逻辑地址转换为物理地址（开启分页时，中间需通过线性地址进行转换，本文不开启分页）。 逻辑地址由 16位段选择符和32位偏移量组成。段选择符格式如下图： 字段名 描述 索引 指定了放在GDT或LDT中的相应段描述符的入口 TI TI（Table Indicator）标志，指明段描述符是在GDT中（TI=0）或在LDT中（TI=1） RPL 请求者特权级，当相应的段选择符装入到cs寄存器中时指示出CPU当前的特权级，它还可以用于在访问数据段时有选择地削弱处理器的特权级 索引指向存放在GDT（Global Descriptor Table，全局描述符表）或LDT（Local Descriptor Table，局部描述符表）中的段描述符，而段描述符中会存放段基地址，将得到的段基地址与逻辑地址中的段偏移量相加即得物理内存地址。 总结实模式中内存被划分成段，每个段的大小为64KB，而这样的段地址可以用16位来表示。内存段的处理是通过和段寄存器相关联的内部机制来处理的，这些段寄存器（CS、DS、 SS和ES）的内容形成了物理地址的一部分。具体来说，最终的物理地址是由16位的段寄存器和16位的段内偏移地址组成的。 在保护模式下，段是通过一系列被称之为“描述符表”的表所定义的。段寄存器存储的段选择符，指向GDT或LDT中的位置。]]></content>
      <categories>
        <category>处理器工作方式</category>
      </categories>
      <tags>
        <tag>实模式</tag>
        <tag>保护模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPU 寄存器]]></title>
    <url>%2FLinux%E5%86%85%E6%A0%B8%2FCPU-%E5%AF%84%E5%AD%98%E5%99%A8.html</url>
    <content type="text"><![CDATA[寄存器是中央处理器内的组成部分，是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和地址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。 寄存器是集成电路中非常重要的一种存储单元，通常由D触发器组成。在集成电路设计中，寄存器可分为电路内部使用的寄存器和充当内外部接口的寄存器这两类。内部寄存器不能被外部电路或软件访问，只是为内部电路的实现存储功能或满足电路的时序要求。而接口寄存器可以同时被内部电路和外部电路或软件访问，CPU中的寄存器就是其中一种，作为软硬件的接口，为广泛的通用编程用户所熟知。 寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。寄存器是内存阶层中的最顶端，也是系统获得操作资料的最快速途径。寄存器通常都是以他们可以保存的位元数量来估量，举例来说，一个“8位元寄存器”或“32位元寄存器”。 16位寄存器以8086（16位处理器，14个寄存器）为例。按其用途可分为： 通用寄存器（8个）： 数据寄存器 AH&amp;AL=AX(accumulator)：累加寄存器，常用于运算；在乘除等指令中指定用来存放操作数，另外,所有的I/O指令都使用这一寄存器与外界设备传送数据。 BH&amp;BL=BX(base)：基址寄存器，常用于地址索引。 CH&amp;CL=CX(count)：计数寄存器，常用于计数，常用于保存计算值。如在移位指令,循环(loop)和串处理指令中用作隐含的计数器。 DH&amp;DL=DX(data)：数据寄存器，常用于数据传递。 这4个16位的寄存器可以分为高8位：AH，BH，CH，DH。以及低8位：AL，BL，CL，DL。这2组8位寄存器可以分别寻址，并单独使用。 指针寄存器和变址寄存器： SP（Stack Pointer）：堆栈指针，与SS配合使用，可指向目前的堆栈位置。 BP（Base Pointer）：基址指针寄存器，可用作SS的一个相对基址位置。 SI（Source Index）：源变址寄存器，可用来存放相对于DS段之源变址指针。 DI（Destination Index）：目的变址寄存器，可用来存放相对于ES 段之目的变址指针。 这4个16位寄存器只能按16位进行存取操作，主要用来形成操作数的地址，用于堆栈操作和变址运算中计算操作数的有效地址。 指令指针（1个）： 指令指针（Instruction Pointer，IP）是一个16位专用寄存器，它指向当前需要取出的指令字节，当BIU从内存中取出一个指令字节后，IP就自动加(取出该字节的长度，如：BIU从内存中取出的是1个字节，IP就会自动加1，如果BIU从内存中取出的字节数长度为3，IP就自动加3)，指向下一个指令字节。注意，IP指向的是指令地址的段内地址偏移量，又称偏移地址(Offset Address)或有效地址(EA，Effective Address)。 BIU是80x86微处理系统芯片中的两个基本功能部件之一，中文为总线接口部件（Bus Interface Unit）。BIU是和总线打交道的接口部件，它根据执行单元(Execution Unit)的请求，执行8086 CPU 对存储器或 I/O 接口的总线操作，完成数据传送，BIU由指令队列缓冲器，16位指令指针寄存器CS、DS、SS和ES，地址产生器和段寄存器，总线控制逻辑等部分构成。 标志寄存器（1个）： 标志寄存器（Flags Register，FR）又称程序状态字(Program Status Word,PSW)。这是一个存放条件标志、控制标志寄存器，主要用于反映处理器的状态和运算结果的某些特征及控制指令的执行。 在FR中有意义的有9位，其中6位是状态位，3位是控制位。 标志 描述 CF(carry flag) 进位标志，主要用来反映无符号数运算是否产生进位或借位。如果运算结果的最高位产生了一个进位或借位，那么，其值为1，否则其值为0。 PF(parity flag) 奇偶标志，用于反映运算结果中“1”的个数的奇偶性。如果“1”的个数为偶数，则PF的值为1，否则其值为0。 AF(adjust flag) 辅助进位标志，存在下列情况值被置为1，否则其值为0：在字操作时，发生低字节向高字节进位或借位时；在字节操作时，发生低4位向高4位进位或借位时。 ZF(zero flag) 零标志，用来反映运算结果是否为0。如果运算结果为0，则其值为1，否则其值为0。在判断运算结果是否为0时，可使用此标志位。 SF(sign flag) 符号标志，用来反映运算结果的符号位，它与运算结果的最高位相同。在微机系统中，有符号数采用补码表示法，所以，SF也就反映运算结果的正负号。运算结果为非负数时，SF的值为0，否则其值为1。当运算结果没有产生溢出时，运算结果等于逻辑结果（即应该得到的正确的结果），此时SF表示的是逻辑结果的正负，当运算结果产生溢出时，运算结果不等于逻辑结果，此时的SF值所表示的正负情况与逻辑结果相反，即：SF=0时，逻辑结果为负，SF=1时，逻辑结果为非负。 TF(trap flag) 跟踪标志，可用于程序调试。TF标志没有专门的指令来设置或清除。如果TF=1，则CPU处于单步执行指令的工作方式，此时每执行完一条指令，就显示CPU内各个寄存器的当前值及CPU将要执行的下一条指令。如果TF=0，则处于连续工作模式。 IF(interrupt enable flag) 中断允许标志，用来决定CPU是否响应CPU外部的可屏蔽中断发出的中断请求。但不管该标志为何值，CPU都必须响应CPU外部的不可屏蔽中断所发出的中断请求，以及CPU内部产生的中断请求。具体规定如下：当IF=1时，CPU可以响应CPU外部的可屏蔽中断发出的中断请求；当IF=0时，CPU不响应CPU外部的可屏蔽中断发出的中断请求。 DF(direction flag) 方向标志，用来决定在串操作指令执行时有关指针寄存器发生调整的方向。 OF(overflow flag) 溢出标志，用于反映有符号数加减运算所得结果是否溢出。如果运算结果超过当前运算位数所能表示的范围，则称为溢出，OF的值被置为1，否则，OF的值被清为0。 段寄存器（4个）： CS（Code Segment）：代码段寄存器 DS（Data Segment）：数据段寄存器 SS（Stack Segment）：堆栈段寄存器 ES（Extra Segment）：附加段寄存器 当一个程序要执行时，就要决定程序代码、数据和堆栈各要用到内存的哪些位置，通过设定段寄存器CS，DS，SS来指向这些起始位置。通常是将DS固定，而根据需要修改CS。所以，程序可以在可寻 址空间小于64K的情况下被写成任意大小。所以，程序和其数据组合起来的大小，限制在DS所指的64K内，这就是COM文件不得大于64K的原因。8086以内存作为战场，用寄存器做为军事基地，以加速工作。 备注：由于所讲的是16位cpu，IP寄存器的位数为16，即：偏移地址为16位，2的16次幂就是64K，所以16位段地址不能超过64K，超过64K会造成64K以上的地址找不到。 32位寄存器32位CPU 寄存器及数据结构图： 通用寄存器（8个）：EAX、EBX、ECX、EDX、ESP、EBP、ESI、EDI 尽管这8个通用寄存器大多时候是通用的，可以用作任何用途，但是在某些情况下，他们也有隐含的用法。比如ECX、ESI和EDI在串循环操作中分别用作计数器、源和目标。EBP和ESP主要用来维护栈，ESP通常指向栈的顶部，EBP指向当前栈帧的起始地址。 EAX, EBX, ECX, EDX都可以作为32位寄存器、16位寄存器或者8位寄存器使用。用法类似16位数据寄存器。 EBP和ESP是32位寄存器，也可作为16位寄存器BP, SP使用，常用于椎栈操作。 EDI和ESI常用于串操作，EDI用于寻址目标数据串，ESI用于寻址源数据串。 标志寄存器（1个）：EFLAGS 控制任务状态和模式切换、中断处理、指令追踪和访问权限控制。寄存器中的标志位需要特权指令代码才可以修改(特权指令:运行在内核态下的代码)。 在16位标志寄存器的基础上增加了几个标志： 标志 描述 IOPL(I/O privilege level field) 指示当前运行任务的I/O特权级(I/O privilege level)，正在运行任务的当前特权级(CPL)必须小于或等于I/O特权级才能允许访问I/O地址空间。这个域只能在CPL为0时才能通过POPF以及IRET指令修改。 NT(Nested task flag) 控制中断链和被调用任务。若当前任务与前一个执行任务相关则置1，反之则清零。 RF(Resume flag) 控制处理器对调试异常的响应。 VM(Virtual-8086 mode flag) 置1以允许虚拟8086模式，清除则返回保护模式。 AC(Alignment check flag) 该标志以及在CR0寄存器中的AM位置1时将允许内存引用的对齐检查，以上两个标志中至少有一个被清零则禁用对齐检查。 VIF(Virtual interrupt flag) 该标志是IF标志的虚拟镜像(Virtual image)，与VIP标志结合起来使用。使用这个标志以及VIP标志，并设置CR4控制寄存器中的VME标志就可以允许虚拟模式扩展(virtual mode extensions) VIP(Virtual interrupt pending flag) 该位置1以指示一个中断正在被挂起，当没有中断挂起时该位清零。(Software sets and clears this flag; the processor only reads it)与VIF标志结合使用。 ID(Identification flag) 程序能够设置或清除这个标志指示了处理器对CPUID指令的支持。 控制寄存器（Control Register）（5个）：CR0-CR4 控制寄存器决定处理器的操作模式和当前执行任务的一些特征。 CR0：控制系统的工作模式和处理器的状态；x86_32的CR0为32bit。X86_64下为64bit，其中低32bit与x86_32的CR0保持一致，高32bit没有定义，作保留使用，除了bit4其他所有位都是可读可写的。 标志 描述 PE(Protected-Mode Enable) PE=0，表示CPU处于实模式；PE=1表CPU处于保护模式，并使用分段机制。 MP 协处理器监视标志位 EM 该位表明是否需要仿真协处理器的功能 TS 每当任务切换时就设置该位，并且在解释协处理器指令之前测试该位 ET 扩展类型。设置有效位时，支持 ntel 387 DX 数学协处理器指令 NE 数字错误标志位(与浮点协处理器共同使用) WP 写保护 AM 对齐功能屏蔽(与EFLAGS寄存器中 AC标志位一同使用) NW 直写无效(直写：高速缓存中的数据始终保持与主存储器中数据匹配，也叫做通写 ) CD cache 缺失设置位 PG(Paging Enable) 控制分页机制，PG=1，启动分页机制；PG=0,不使用分页机制。 ​ CR1：保留 ​ CR2：存放发生页错误时的虚拟地址 CR3：用来存放最高级页目录地址(物理地址），各级页表项中存放的也是物理地址。 标志 描述 PWT(Page-Level Writethrough) 控制cache采取直写还是回写的策略。当设置清空时，回写有效。当置位时，直写有效 PCD(Page-Level Cache Disable) PCD=1，表示最高目录表不可缓存，PCD=0，相反 Figure 3-4中，不使用PAE技术，有两层页表。最高层为页目录有1024项，占用4KB。page_directory_table base address为物理地址，指向4KB对齐的页目录地址。 Figure 3-5中，使用PAE技术，三层页表寻址。最高层为页目录指针，4项，占用32B空间。所以 page_directory_table base address为27位，指向32B对齐的页目录指针表。 ​ CR4：一些结构的扩展。表明对于特定的处理器和操作系统执行支持。 标志 描述 VME(virtual 8086 mode extension) 虚拟8086模式扩展位。VME=1，允许虚拟8086扩展，即允许8086模式和虚拟8086中断。VME=0，禁止虚拟8086模式扩展。 PVI(protected mode virtual interrupts) 保护模式虚拟中断位。PVI=1，允许保护模式虚拟中断。PVI=0，禁止保护模式虚拟中断。 TSD(time stamp disable) 禁止RDTSC指令位。TSD=0，则允许RDTSC（读时间标志计算器）指令在任何特权级上执行。TSD=1，仅允许RDTSC指令在0级特权级上执行，否则将发生一般保护模式异常。 DE(debugging extensions) 调试扩展位。DE=1，允许输入/输出断点。DE=0，不支持输入/输出断点。 PSE(page size extensions) 允许页容量大小扩展位。PSE=1，允许每页容量为4MB。PSE=0，只允许每页容量为4KB。 PAE(physical address extension) 允许物理地址扩展位。PAE=1，允许采用32位以上的物理地址（包括32位和64位地址）。PAE=0，只允许采用32位物理地址。 MCE(machine check exception) 允许机器检查异常位。MCE=1，允许机器检查异常。MCE=0，不允许机器检查异常。 PGE(Page-Global Enable) 将PGE设置为1可启用全局页面机制。将该位清除为0将禁用该机制。当启用PGE时，系统软件可以将页面转换层级的最低级别的全局页面（G）位设置为1，表示页面翻译是全局的。当页面翻译表基地址（CR3）更新时，标记为全局的页面翻译在TLB中不会失效。 PCE(Performance-Monitoring Counter Enable) 将PCE设置为1允许在任何权限级别运行的软件使用RDPMC指令。软件使用RDPMC指令读取性能监视MSRs PerfCtrn。 将PCE清除为0仅允许最特权的软件（CPL = 0）使用RDPMC指令。 OSFXSR(FXSAVE/FXRSTOR Support) 设置为1，以使能256位和128位媒体指令。当该位设置为1时，它还指示系统软件使用FXSAVE和FXRSTOR指令来保存和恢复x87,64位介质和128位介质指令的处理器状态。 OSXMMEXCPT(Unmasked Exception Support) 当系统软件支持SIMD浮点异常（#XF）来处理未屏蔽的256位和128位媒体浮点错误时，系统软件必须将OSXMMEXCPT位设置为1。将OSXMMEXCPT位清除为0表示不支持#XF处理程序。 当OSXMMEXCPT = 0时，未屏蔽的128位媒体浮点异常会导致无效操作码异常 OSXSAVE(XSAVE and Extended States) 设置为1，则操作系统支持XGETBV，XSETBV，XSAVE和XRSTOR指令。处理器也将能够执行XGETBV和XSETBV指令，以读写XCR0。 调试寄存器（Debug Register）（8个）：DR0-DR7 ​ 调试寄存器主要作用是调试应用代码、系统代码、开发多任务操作系统.来监视代码的运行和处理器的性能。 ​ DR0-DR3：保留32位断点的线性地址。 ​ DR4-DR5：保留。 ​ DR6： 标志 描述 B0-B3 断点状态的监测 BD 调试寄存器访问监测。置位，表明在指令流中，下一条指令将访问其中的一个调试寄存器 BS 单步执行标志位 BT 任务转换标志位 ​ DR7： 标志 描述 L0-L3 局部断点使能标志位 G0-G3 全局断点使能标志位 LE GE 置位，表明处理器可以监测导致数据断点的指令。推荐置位为1 GD 通用监测使能标志位。表明是否开启调试寄存器保护 LEN0 - LEN3 用来表明相应断点地址寄存器内存位置的大小 R/W0 - R/W3 相应断点的状态 系统地址寄存器（4个）：GDTR、IDTR、LDTR和TR GDTR：全局描述符表寄存器，是一个48位寄存器，用来存放全局描述符表GDT的32位线性基地址和16位的界限值。在全局描述符表中不仅包括有操作系统使用的描述符，而且还有所有任务使用的公用描述符。 IDTR：中断描述符表寄存器，是一个48位寄存器，用来存放中断描述符表IDT的32位线性基地址和16位的界限值。 LDTR：局部描述符表寄存器，是一个16位寄存器，用来存放局部描述符表LDT的16位选择符。另外还有一个隐含的描述符高速缓冲寄存器，用来存放LDT表描述符。 TR：任务状态寄存器，是一个16位寄存器，用来存放任务状态段TSS的16位选择符。与之相应，也有一个隐含的描述符高速缓冲寄存器，用来存放任务状态段TSS的描述符。 16位段寄存器（6个）：CS、DS、ES、FS、GS、SS 段寄存器有两部分，一部分是编程可见的选择器寄存器，为6个16位寄存器，对应在另一部分有6个64位的描述符寄存器，后一部分是编程不可见的。 在实地址方式或虚拟8086方式，描述符寄存器不起作用，选择器寄存器退化成16位CPU的段寄存器功能，存放内存段的段基址——段首地址的高16位，其中CS对应于代码段、SS对应于堆栈段，DS对应于数据段，ES对应于附加数据段，在串操作时，DS和ES分别对应于源数据段和目的数据段。FS和GS没有定义。 其他寄存器：EIP、TSC等 指令指针指示器–EIP 32位寄存器，低16位称为IP，用于兼容16位CPU，其内容是下一条要取入CPU的指令在内存中的偏移地址。当一个程序开始运行时，系统把EIP清零，每取入一条指令，EPI自动增加取入CPU的字节数目。所以称EIP为指令指针。 时间戳寄存器–TSC 每个时钟周期时其值加1，重启时清零。通过RDTSC指令读取TSC寄存器，只有当CR4寄存器的TSD位为0时，才可以在任何优先级下执行该指令，否则只能在特权级下执行该指令。 浮点寄存器 由于在80486微处理器内部设有浮点运算器，因此在其内部有相应的寄存器，其中包括8个80位通用数据寄存器、1个48位指令指针寄存器、1个48位数据指针寄存器、1个16位控制字寄存器、1个16位状态字寄存器和1个16位标记字寄存器。 主要技术重命名技术：寄存器重命名，是CPU在解码过程中对寄存器进行重命名，解码器把“其它”的寄存器名字变为“通用”的寄存器名字，本质上是通过一个表格把x86寄存器重新映射到其它寄存器，这样可以让实际使用到的寄存器远大于8个。这样做的好处除了便于前面指令发生意外或分支预测出错时取消外，还避免了由于两条指令写同一个寄存器时的等待。 乱序执行技术：采用乱序执行技术使CPU内部电路满负荷运转并相应提高了CPU运行程序的速度。类似多个CPU同步执行。 特点寄存器又分为内部寄存器与外部寄存器，所谓内部寄存器，其实也是一些小的存储单元，也能存储数据。但同存储器相比，寄存器又有自己独有的特点： 寄存器位于CPU内部，数量很少，仅十四个 寄存器所能存储的数据位数根据处理器类型不同而不同（8bit、16bit及32bit） 每个内部寄存器都有一个名字，而没有类似存储器的地址编号 参考资料80X86寄存器详解 寄存器（百度百科） Intel X86 CPU系列的寄存器 CR0-4寄存器介绍EFLAGS]]></content>
      <categories>
        <category>Linux内核</category>
      </categories>
      <tags>
        <tag>CPU</tag>
        <tag>寄存器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intel CPU 发展简史]]></title>
    <url>%2FIntel-CPU-%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2.html</url>
    <content type="text"><![CDATA[1971年11月15日：40041971年11月15日，Intel公司的工程师霍夫发明了世界上第一个商用微处理器—4004。这款4位微处理器集成了2250个晶体管，晶体管之间的距离是10微米，能够处理4bit的数据，每秒运算6万次，频率为108KHz，前端总线为0.74MHz （4bit）。原为日本Busicom公司的计数器开发。 编号为4004，第一个“4”代表此芯片是客户订购的产品编号，后一个“4”代表此芯片是英特尔公司制作的第四个订制芯片，之前还有4001（动态内存DRAM）、4002（只读存储器ROM）、4003（Register），这种数字代号却延用至今。采用4004芯片后，再配用一块程序存储器，数据存储器，移位寄存器，再加上键盘和数码管，就构成了一台完整的微型计算机。 4004的最高频率有740kHz，能执行4位运算，支持8位指令集及12位地址集。 4004只能称为世界上第一款商用处理器，而不是世界上第一款微处理器。第一款微处理器应该是美国军方研制，用于F-14雄猫战机中由6颗晶片组成的中央空气数据计算机：CADC（CenterAir Data Computer），虽然它的构造比4004还要简单，速度只有9.15KHz。 1972年：80088008频率为200KHz，晶体管的总数已经达到了3500个，能处理8比特的数据，性能是4004的两倍，速度为200KHz。更为重要的是，英特尔还首次获得了处理器的指令技术。 8008芯片原本是为德克萨斯州的Datapoint公司设计的，但是这家公司最终却没有足够的财力支付这笔费用。于是双方达成协议，英特尔拥有这款芯片所有的知识产权，而且还获得了由Datapoint公司开发的指令集。这套指令集奠定了今天英特尔公司X86系列微处理器指令集的基础。 执行8位运算，支持16位地址总线和16位数据总线。 1974年：8080在微处理器发展初期，具有革新意义的芯片非Intel8080莫属了。由于采用了复杂的指令集以及40管脚封装，8080的处理能力大为提高，其功能是8008的10倍，每秒能执行29万条指令，集成晶体管数目6000，运行速度2MHz。 与此同时，微处理器的优势已经被业内人士所认同，于是更多的公司开始接入这一领域，竞争开始变得日益激烈。当时与英特尔同台竞技的有RCA(美国无线电公司)、Honeywell、Fairchild、美国国家半导体公司、AMD、摩托罗拉以及Zilog公司。值得一提的是Zilog，世界上第一块4004芯片的设计者Faggin就加盟了该公司。由该公司推出的Z80微处理器比Intel8080功能更为强大，而且直到今天这款处理器仍然被尊为经典。 8080有幸成为了第一款个人计算机Altair的大脑。 8位元处理器，拥有16位地址总线和8位数据总线，包含7个八位寄存器，支持16位寻址，同时也包含一些输入输出端口，有效解决了外部设备在内存寻址能力不足的问题。 1978年：8086-80881978年，英特尔推出了首枚16位微处理器8086，同时生产出与之配合的数学协处理器8087，这两种芯片使用相同的指令集，以后英特尔生产的处理器，均对其兼容。趁着市场销售正好的时机，以及市场需求的提升，Intel在同一年推出了性能更出色的8088处理器。三款处理器都拥有29000只晶体管，速度可分为5MHz、8MHz、10MHz，内部数据总线(处理器内部传输数据的总线)、外部数据总线(处理器外部传输数据的总线)均为16位，地址总线为20位，可寻址1MB内存。首次在商业市场给消费者提供了更自由选择。 1982年：8028680286(也称286)是处理器进入全新技术的标准产品，具备16位字长，集成了14.3万只晶体管，具有6MHz、8MHz、10MHz、12.5MHz四个主频的产品。286是Intel第一款具有完全兼容性的处理器，即可以运行所有针对其前代处理器编写的软件，这一软件兼容性也成为了Intel处理器家族一个恒久不变的特点。286是英特尔的最后一块16位处理器 最大主频为20MHz，采用16位数据总线和24位地址总线。 80286有两种工作模式：实模式和保护模式。在实模式下，80286直接访问内存的空间被限制在1M字节，更多内存需要通过EMS或XMS内存机制进行映射。而在保护模式下，80286可以直接访问16M字节的内存，并具有异常处理机制。 1985年：80386英特尔第一款32位处理器，集成了27万5千只晶体管，超过了4004芯片的一百倍，每秒可以处理500万条指令。同时也是第一款具有“多任务”功能的处理器，所谓“多任务”就是说它可以同时处理多个程序程序的指令，这对微软的操作系统发展有着重要的影响。 重要特点： 首次在x86处理器中实现了32位系统， 可配合使用80387数字辅助处理器增强浮点运算能力 首次采用高速缓存（外置）解决内存速度瓶颈问题。 由于这些设计，80386的运算速度达到了前代产品80286的数倍。80386DX的内部和外部数据总线（或资料汇流排）是32位，地址总线（或位址汇流排）也是32位，可以定址到4GB内存，并可以管理64TB的虚拟存储空间（虚拟存储空间：通过硬件和软件的综合来扩大用户可存储空间，它在内存储器和外存储器（磁盘、光盘）之间增加一定的硬件和软件支持，使两者形成一个有机整体，支持运行比实际配置的内存容量大的多的大任务程序）。 80386有三种工作模式：实模式、保护模式、虚拟86模式。真实模式为DOS系统的常用模式，直接内存访问空间被限制在1M字节（也叫做位元组）；保护模式下80386-DX可以直接访问4G位元组的内存，并具有异常处理机制；虚拟86模式可以同时模拟多个8086处理器来加强多工处理能力。 Intel RapidCAD 被遗忘的微处理器RapidCAD是英特尔有史以来第一款为旧款个人计算机所提供的升级套件(也就是OverDrive的始祖)。原386的使用者不需要更换主机板，只要把RapidCAD买回来将主机板上旧有的中央处理器芯片(CPU)替换掉，就可以享受接近486的运算能力。RapidCAD其实就是把486 DX芯片去掉内部高速缓存然后装入386的封装里面，RapidCAD也不支持486增加的新指令。不过由于386封装的频宽限制，RapidCAD对整体的效能提升比不上直接升级到486 DX。相同频率下，486 DX可以有比386/387快上两倍的速度，而RapidCAD在整数运算方面最多只能提升35%，在浮点运算方面，则可以提升将近70%。 Intel RapidCAD特殊的地方在于，它是由两颗芯片组成，缺一不可。这归咎于486 DX内建浮点运算器(FPU)，而386则是将浮点运算器分开(就是387)。由于RapidCAD-1本身就含有浮点运算器(因为它就是486 DX阉割版)，根本不需要387，所以RapidCAD-2就是用来替代原来主机板上的387芯片。RapidCAD-1负责所有的运算，而RapidCAD-2则是负责模拟浮点运算器，以防止旧有主机板以为没有安装浮点运算功能(尤其在执行286/287的程序时)。市面上有时候把RapidCAD-1与RapidCAD-2分开卖，这就是不了解RapidCAD运作方式的结果。 1989年：8048680486处理器集成了125万个晶体管，时钟频率由25MHz逐步提升到33MHz、40MHz、50MHz及后来的100MHz。486家族的指令集与386非常相似，只有增加少量的指令。 486处理器的应用意味着用户从此摆脱了命令形式的计算机，进入“选中并点击(point-and-click)”的计算时代。英特尔486处理器首次采用内建的数学协处理器，将负载的数学运算功能从中央处理器中分离出来，从而显著加快了计算速度。 386和486推向市场后，均大获成功，英特尔在芯片领域的霸主地位日益凸现。此后，英特尔开始告别微处理器数字编号时代，进入到了Pentium时代。 1994年3月10日：Intel Pentium中央处理器芯片1993年，英特尔发布了Pentium(俗称586)中央处理器芯片(CPU)。本来按照惯常的命名规律是80586，但是在486发展末期，就已经有公司将486等级的产品标识成586来销售了。因此英特尔决定使用自创的品牌来作为新产品的商标—Pentium。 英特尔奔腾处理器采用了0.60微米工艺技术制造，核心由320万个晶体管组成。支持计算机更轻松的集成“现实世界”数据，如语音、声音、手写体和图片等。 Pentium是x86系列一大革新。其中晶体管数大幅提高、增强了浮点运算功能、并把十年未变的工作电压降至3.3V。Pentium刚推出的时候拥有浮点数除法不正确的错误(FDIV Bug)，导致英特尔大量回收第一代产品(1994年12月之前的产品)，所以有FDIV Bug的微处理器所剩不多。Pentium 50MHz也有这个FDIV错误，不过A80501-50只是业界样本，从来没有在市场上出现过。 1995年3月27日，英特尔发布Pentium 120MHz处理器，采用了0.60微米/0.35两种工艺技术，不过核心依旧由320万个晶体管组成。 1995年6月，英特尔发布Pentium 133MHz处理器，采用0.35工艺技术制造,核心提升到由330万个晶体管组成。 1995年11月1日，英特尔发布Pentium 150MHz、Pentium 166MHz、Pentium 180MHz、Pentium 200MHz四款处理器，并且采用了0.60微米/0.35两种工艺技术,核心提升到由550万个晶体管组成。此时INTEL在以前设计基础上增加了L2 cache为256K和512K两种版本。 1996年1月4日，英特尔又发布Pentium 150MHz、Pentium 166MHz两款处理器，采用了0.35微米工艺技术,不过核心由330万个晶体管组成。 1996年6月10日，英特尔发布Pentium 200MHz处理器，采用了0.35微米工艺技术，不过核心还是由330万个晶体管组成。 1996年：Intel Pentium ProPentimuPro的内部含有高达550万个的晶体管，内部时钟频率为133MHz，处理速度几乎是100MHz的Pentium的2倍。PentimuPro的一级(片内)缓存为8KB指令和8KB数据。值得注意的是在PentimuPro的一个封装中除PentimuPro芯片外还包括有一个256KB的二级缓存芯片，两个芯片之间用高频宽的内部通讯总线互连，处理器与高速缓存的连接线路也被安置在该封装中，这样就使高速缓存能更容易地运行在更高的频率上。PentiumPro 200MHz CPU的L2 Cache就是运行在200MHz，也就是工作在与处理器相同的频率上。这样的设计领PentiumPro达到了最高的性能。而PentimuPro最引人注目的地方是它具有一项称为“动态执行”的创新技术，这是继Pentium在超标量体系结构上实现实破之后的又一次飞跃。PentimuPro系列的工作频率是150/166/180/200，一级缓存都是16KB，而前三者都有256KB的二级缓存，至于频率为200的CPU还分为三种版本，不同就在于他们的内置的缓存分别是256KB，512KB，1MB。 1997年1月：Intel Pentium MMXPentium MMX芯片在X86指令集的基础上加入了57条多媒体指令。这些指令专门用来处理视频、音频和图象数据，使CPU在多媒体操作上具有更强大的处理能力，Pentium MMX还使用了许多新技术。单指令多数据流SIMD技术能够用一个指令并行处理多个数据，缩短了CPU在处理视频、音频、图形和动画时用于运算的时间；流水线从5级增加到6级，一级高速缓存扩充为16K，一个用于数据高速缓存，另一个用于指令高速缓存，因而速度大大加快；Pentium MMX还吸收了其他CPU的优秀处理技术，如分支预测技术和返回堆栈技术。 Pentium MMX等于是Pentium的加强版中央处理器芯片(CPU)，除了增加67个MMX(Multi-Media eXtension)指令以及64位数据型态之外之外，也将内建指令及数据暂存(Cache)从之前的8KB增加到16KB，内部工作电压降到2.8V。而英特尔之后的桌上型中央处理器皆包含了MMX指令。 1997年：Intel Pentium OverdriveIntel Pentium OverDrive中央处理器芯片(CPU)，又是一项英特尔造福旧计算机使用者的升级选择。Pentium OverDrive有两种，一种(不含MMX，5V)是给80486升级用的，另一种(含MMX，3.3V)是给Pentium早期产品(Socket6, 50-66MHz)升级的。他们都有含散热器及风扇。 1997-1998年：Pentium II1997年5月7日，英特尔发布Pentium II 233MHz、Pentium II 266MHz、Pentium II 300MHz三款PII处理器，采用了0.35微米工艺技术，核心提升到750万个晶体管组成。采用SLOT1架构，通过单边插接卡(SEC)与主板相连，SEC卡盒将CPU内核和二级高速缓存封装在一起，二级高速缓存的工作速度是处理器内核工作速度的一半；处理器采用了与Pentium PRO相同的动态执行技术，可以加速软件的执行；通过双重独立总线与系统总线相连，可进行多重数据交换，提高系统性能；PentiumII也包含MMX指令集。Intel此举希望用SLOT1构架的专利将AMD等一棍打死，可没想到Socket 7平台在以AMD的K6-2为首的处理器的支持下，走入了另一个春天。而从此开始，Intel也开始走上了一条前途不明的道路，开始频繁的强行制定自己的标准，企图借此达到迅速挤垮竞争对手的目的，但市场与用户的需要使得Intel开始不断的陷入被动和不利的局面。 在这个时期100MHz频率的SDR内存已经出现在市场上，但是Intel却惊人地宣布他们将放弃并行内存而主推一种名为Rambus的内存，而一时间众多大公司如西门子、HP和DELL等都投入了Rambus的门下，不过后来DDR内存的流行也证明了Intel的失败。 1997年6月2日，英特尔发布MMX指令技术的Pentium II 233MHz处理器，采用了0.35微米工艺技术,核心由450万个晶体管组成。 1997年8月18日，英特尔发布L2 cache为1M的Pentium II 200MHz处理器，采用了0.35微米工艺技术,核心由550万个晶体管组成。 1998年1月26日，英特尔发布Pentium II 333MHz处理器，采用了0.35微米工艺技术，核心由750万个晶体管组成。 1998年4月15日，英特尔发布Pentium II 350MHz、Pentium II 400MHz和第一款Celeron 266MHz处理器，此三款CPU都采用了最新0.25微米工艺技术,核心由750万个晶体管组成。 1998年8月24日，英特尔发布Pentium II 450MHz处理器，采用了0.25微米工艺技术,核心由750万个晶体管组成。 CPU发展到这个时期，就不能不说说Intel Pentium II Cerelon处理器。英特尔将Celeron处理器的L2 Cache设定为只有Pentium II的一半(也就是128KB)，这样既有合理的效能，又有相对低廉的售价(有A字尾的)；这样的策略一直延续到今天。不过很快有人发现，使用双Celeron的系统与双Pentium II的系统差距不大，而价格却便宜很多，结果造成了Celeron冲击高阶市场的局面。后来英特尔决定取消Celeron处理器的SMP功能，才解决了这个问题。 Pentium II Celeron处理器Celeron（赛扬）300A，是一个让多少人闻之动容的产品，又陪伴了多少曾经年少的读者度过悠长的学生时代。赛扬300A，从某种意义上已经是Intel的第二代赛扬处理器。第一代的赛扬处理器仅仅拥有266MHz、300MHz两种版本，第一代的Celeron处理器由于不拥有任何的二级缓存，虽然有效的降低了成本，但是性能也无法让人满意。为了弥补性能上的不足，Intel终于首次推出带有二级缓存的赛扬处理器——采用Mendocino核心的Celeron300A、333、366。经典，从此诞生。 1999年：Intel Pentium III1999年2月26日，英特尔发布Pentium III 450MHz、Pentium III 500MHz处理器，同时采用了0.25微米工艺技术,核心由950万个晶体管组成，从此Intel开始踏上了PIII旅程。 Pentium III是给桌上型计算机的中央处理器芯片(CPU)，等于是Pentium II的加强版，新增七十条新指令(SIMD，SSE)。Pentium III与Pentium II一样有Mobile、Xeon以及Cerelon等不同的版本。Celeron系列与Pentium III最大的差距在于二级缓存，100MHz外频的Tualatin Celeron 1GHz可以轻松地跃上133MHz外频。更为重要的是，Tualatin Celeron还有很好的向下兼容性，甚至440BX主板在使用转接卡之后也有望采用该CPU，因此也成为很多升级用户的首选。 特别指出的是，Pentium III光是桌上型就拥有Katmai Slot 1 、Coppermine Slot 1以及Coppermine Socket 370等三种不同的系列。到后期，英特尔放弃插卡式界面而又回归到插槽界面(Socket 370)。socket370封装开始推出的时候，有一部分消费者舍弃了slot1平台而选择了新的处理器。新的PGA封装分为PPGA和FC-PGA两种，前者较为廉价，因而被赛扬处理器所采用，而更为昂贵的后者则被奔腾III处理器所采用。例外的是：采用Mendocino核心的赛扬处理器同时有这两种不同封装的版本。采用PPGA封装的赛扬处理器可以通过转接卡在slot1主板上使用，而采用FC-PGA封装的奔三处理器则无能为力了。 2000年：Intel Pentium IVPentium 4处理器集成了4200万个晶体管，到了改进版的Pentium 4(Northwood)更是集成了5千5百万个晶体管；并且开始采用0.18微米进行制造，初始速度就达到了1.5GHz。 Pentium 4还提供的SSE2指令集，这套指令集增加144个全新的指令，在128bit压缩的数据，在SSE时，仅能以4个单精度浮点值的形式来处理，而在SSE2指令集，该资料能采用多种数据结构来处理： 4个单精度浮点数(SSE)对应2个双精度浮点数(SSE2)，对应16字节数(SSE2)，对应8个字数(word)，对应4个双字数(SSE2)，对应2个四字数(SSE2)，对应1个128位长的整数(SSE2) 。 2002-2004年：超线程P4处理器2002年11月14日，英特尔在全新英特尔奔腾4处理器3.06GHz上推出其创新超线程(Hyper-Threading，HT)技术。超线程(HT)技术支持全新级别的高性能台式机，同时快速运行多个计算应用，或为采用多线程的单独软件程序提供更多性能。超线程(HT)技术可将电脑性能提升达25%。除了为台式机用户引入超线程(HT)技术外，英特尔在推出英特尔奔腾4处理器3.06GHZ时达到了一个电脑里程碑。这是第一款商用微处理器，运行速率为每秒30亿周期，并且采用当时业界最先进的0.13微米制程制作。 P4处理器3.06GHz2003年，英特尔发布前端总线为533MHz的Pentium 4 3.06GHz处理器，采用了0.13微米工艺技术,提供512K的二级缓存，核心由5500万个晶体管组成。 P4处理器至尊版3.20GHz2004年初发布，该处理器可兼容现有的英特尔865和英特尔875芯片组家族产品以及标准系统内存。2MB三级高速缓存可以预先加载图形帧缓冲区或视频帧，以满足处理器随后的要求，使在访问内存和I/O设备时实现更高的吞吐率和更快的帧带率。最终，这可带来更逼真的游戏效果和改进的视频编辑性能。增强的CPU性能还可支持软件厂商创建完善的软件物理引擎，从而带来栩栩如生的人物动作和人工智能，使电脑控制的人物更加形象、逼真。 半年之后，2004年6月，英特尔发布了P4 3.4GHz处理器，该处理器支持超线程(HT)技术，采用0.13微米制程，具备512KB二级高速缓存、2MB三级高速缓存和800MHz系统前端总线速度。 Northwood是第二代产品，采用0.13微米制程，具有电压低、体积小、温度低的优点。接着就是Prescott(0.09微米)，虽然这技术很新，不过由于效能提升并不明显，而且有过热的问题。后来英特尔又推出Hyper Threading技术，大大增加工作效率，让P4又成为市场宠儿。英特尔之后又推出Extreme Edition、含有Prestonia(原本给服务器用的Xeon核心)以及Gallatin(0.13微米Northwood外频提升改良版)核心的CPU。现在市场上的高阶Pentium 4则是Socket LGA 775的Prescott为主。 2005-2006年：双核处理器2005年4月，英特尔的第一款双核处理器平台包括采用英特尔955X高速芯片组、主频为3.2GHz的英特尔奔腾处理器至尊版840，此款产品的问世标志着一个新时代来临了。双核和多核处理器设计用于在一枚处理器中集成两个或多个完整执行内核，以支持同时管理多项活动。英特尔超线程(HT)技术能够使一个执行内核发挥两枚逻辑处理器的作用，因此与该技术结合使用时，英特尔奔腾处理器至尊版840能够充分利用以前可能被闲置的资源，同时处理四个软件线程。 英特尔奔腾D处理器5月，带有两个处理内核的英特尔奔腾D处理器随英特尔945高速芯片组家族一同推出，可带来某些消费电子产品的特性，例如：环绕立体声音频、高清晰度视频和增强图形功能。2006年1月，英特尔发布了Pentium D 9xx系列处理器，包括了支持VT虚拟化技术的Pentium D 960(3.60GHz)、950(3.40GHz)和不支持VT的Pentium D 945(3.4 GHz)、925(3GHz)(注：925不支持VT虚拟化技术)和915(2.80GHz)。 英特尔酷睿2双核处理器2006年7月，英特尔公司面向家用和商用个人电脑与笔记本电脑，发布了十款全新英特尔酷睿2双核处理器和英特尔酷睿至尊处理器。英特尔酷睿2双核处理器家族包括五款专门针对企业、家庭、工作站和玩家(如高端游戏玩家)而定制的台式机处理器，以及五款专门针对移动生活而定制的处理器。英特尔酷睿2双核处理器设计用于提供出色的能效表现，并更快速地运行多种复杂应用，支持用户改进各种任务的处理，例如：更流畅地观看和播放高清晰度视频；在电子商务交易过程中更好地保护电脑及其资产；以及提供更耐久的电池使用时间和更加纤巧时尚的笔记本电脑外形。 全新处理器可实现高达40%的性能提升，其能效比最出色的英特尔奔腾处理器高出40%。英特尔酷睿2双核处理器包含2.91亿个晶体管。不过，Pentium D谈不上是一套完美的双核架构，Intel只是将两个完全独立的CPU核心做在同一枚芯片上，通过同一条前端总线与芯片组相连。两个核心缺乏必要的协同和资源共享能力，而且还必须频繁地对二级缓存作同步化刷新动作，以避免两个核心的工作步调出问题。从这个意义上说，Pentium D带来的进步并没有人们预想得那么大！ 2011年：重新确定处理器产品架构2011年3月，使用32nm工艺全新桌面级和移动端处理器采用了i3、i5和i7的产品分级架构。其中i3主攻低端市场，采用双核处理器架构，约2MB二级缓存；i5处理器主攻主流市场，采用四核处理器架构，4MB二级缓存；i7主攻高端市场，采用四核八线程或六核十二线程架构，二级缓存不少于8MB。 2012年：发布22纳米工艺和第三代处理器使用22nm工艺的处理器热功耗普遍小于77W，使得处理器的散热需求大幅下降，提升了大规模数据运算的可靠性，并降低了散热功耗。 2014年：首发桌面级8核心16线程处理器2014年9月上市的i7-5960X处理器是第一款基于22nm工艺的八核心桌面级处理器，拥有高达20MB的三级缓存，主频达到3.5GHz，热功耗140W。此处理器的处理能力可谓超群，浮点数计算能力是普通办公电脑的10倍以上。随着这一“怪兽”处理器的问世，Intel公司在处理器领域与AMD的差距越拉越大，已经完全形成了一家独大的局面。 寻址空间发展表摘自 寻访x86处理器“实模式”和“保护模式”的前世今生 参考资料：Intel CPU发展简史]]></content>
      <categories>
        <category>CPU发展史</category>
      </categories>
      <tags>
        <tag>CPU</tag>
        <tag>处理器</tag>
      </tags>
  </entry>
</search>
